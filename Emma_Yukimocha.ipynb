{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b4DrfPQD8bu",
        "outputId": "d1a548cb-d27c-42a2-8895-b0b47cac7581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.11)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.2.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.2.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.12.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas matplotlib seaborn gitpython PyGithub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Emt10RyeEP8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Callable, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# For Git\n",
        "import git\n",
        "from github import Github"
      ],
      "metadata": {
        "id": "uXFR2EPoEQ7R"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitManager"
      ],
      "metadata": {
        "id": "yNsf56ZvBRyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Ice-Citron\"\n",
        "!git config --global user.email \"shng2025@gmail.com\""
      ],
      "metadata": {
        "id": "qYjjupbjF3I2"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "#        GIT MANAGER       #\n",
        "############################\n",
        "\n",
        "class GitManager:\n",
        "    \"\"\"Handles GitHub operations to push results to a random-named branch.\"\"\"\n",
        "\n",
        "    def __init__(self, username: str, repo_name: str):\n",
        "        self.username = username\n",
        "        self.repo_name = repo_name\n",
        "        self.token = os.environ.get('github')  # read from environment\n",
        "        self.repo_url = f\"https://x-access-token:{self.token}@github.com/{username}/{repo_name}.git\"\n",
        "        # random branch name like \"optimization_1234\"\n",
        "        self.branch_id = f\"optimization_{random.randint(1000, 9999)}\"\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.repo = None  # will hold a git.Repo object\n",
        "\n",
        "    def setup_repo(self) -> str:\n",
        "        \"\"\"\n",
        "        Clone or init the repo inside 'optimization_results',\n",
        "        create + checkout a new branch,\n",
        "        then create a subfolder \"experiment_{timestamp}\"\n",
        "        and return the path to that folder.\n",
        "        \"\"\"\n",
        "        import git\n",
        "\n",
        "        try:\n",
        "            root_dir = \"optimization_results\"\n",
        "            os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "            with git.Git().custom_environment(GIT_SSL_NO_VERIFY='true'):\n",
        "                try:\n",
        "                    # If there's already a repo in root_dir, just link it\n",
        "                    self.repo = git.Repo(root_dir)\n",
        "                    origin = self.repo.remote('origin')\n",
        "                    origin.set_url(self.repo_url)\n",
        "                except git.exc.InvalidGitRepositoryError:\n",
        "                    # Otherwise, init a fresh repo and set origin\n",
        "                    self.repo = git.Repo.init(root_dir)\n",
        "                    origin = self.repo.create_remote('origin', self.repo_url)\n",
        "\n",
        "                    config_writer = self.repo.config_writer()\n",
        "                    config_writer.set_value(\"http\", \"sslVerify\", \"false\")\n",
        "                    config_writer.release()\n",
        "\n",
        "                # fetch & checkout new branch\n",
        "                origin.fetch()\n",
        "                origin.pull('main')\n",
        "                new_branch = self.repo.create_head(self.branch_id, origin.refs.main)\n",
        "                new_branch.checkout()\n",
        "\n",
        "                # create a unique experiment folder\n",
        "                experiment_dir = os.path.join(root_dir, f\"experiment_{self.timestamp}\")\n",
        "                os.makedirs(experiment_dir, exist_ok=True)\n",
        "                print(f\"Successfully created/checked out branch: {self.branch_id}\")\n",
        "                return experiment_dir\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up repository: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def push_results(self):\n",
        "        \"\"\"\n",
        "        Add all changes, commit, push to the branch, and open a PR on GitHub.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            env = {\n",
        "                'GIT_SSL_NO_VERIFY': 'true',\n",
        "                'GIT_TERMINAL_PROMPT': '0',\n",
        "                'GIT_USERNAME': 'x-access-token',\n",
        "                'GIT_PASSWORD': self.token\n",
        "            }\n",
        "\n",
        "            with self.repo.git.custom_environment(**env):\n",
        "                print(f\"\\nPushing results from experiment_{self.timestamp}...\")\n",
        "                self.repo.index.add('*')\n",
        "                self.repo.index.commit(f\"Results from experiment_{self.timestamp}\")\n",
        "                self.repo.remotes.origin.push(self.branch_id)\n",
        "\n",
        "                # open a PR\n",
        "                g = Github(self.token)\n",
        "                repo = g.get_repo(f\"{self.username}/{self.repo_name}\")\n",
        "                pr = repo.create_pull(\n",
        "                    title=f\"Results from experiment_{self.timestamp}\",\n",
        "                    body=f\"Automated results from {self.branch_id}\",\n",
        "                    head=self.branch_id,\n",
        "                    base=\"main\"\n",
        "                )\n",
        "                print(f\"Created PR at: {pr.html_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error pushing results: {str(e)}\")"
      ],
      "metadata": {
        "id": "iIW0RE-DBTlo"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Metrics"
      ],
      "metadata": {
        "id": "ftJAaFd0OHee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "#     HELPER METRICS      #\n",
        "###########################\n",
        "\n",
        "def distance_to_minimum(x: np.ndarray, x_min: Optional[np.ndarray]) -> float:\n",
        "    if x_min is None:\n",
        "        return np.nan\n",
        "    return float(np.linalg.norm(x - x_min))\n",
        "\n",
        "def step_size(x_curr: np.ndarray, x_prev: np.ndarray) -> float:\n",
        "    return float(np.linalg.norm(x_curr - x_prev))\n",
        "\n",
        "def grad_cosine_similarity(g_curr: np.ndarray, g_prev: np.ndarray) -> float:\n",
        "    norm_curr = np.linalg.norm(g_curr)\n",
        "    norm_prev = np.linalg.norm(g_prev)\n",
        "    if norm_curr < 1e-15 or norm_prev < 1e-15:\n",
        "        return np.nan\n",
        "    return float(np.dot(g_curr, g_prev)/(norm_curr*norm_prev))\n",
        "\n",
        "def relative_improvement(current_val: float, prev_val: float) -> float:\n",
        "    if abs(prev_val) < 1e-15:\n",
        "        return 0.0\n",
        "    return float((prev_val - current_val)/abs(prev_val))"
      ],
      "metadata": {
        "id": "BrU_Pc0uOJ4W"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizers and Functions"
      ],
      "metadata": {
        "id": "F7ANmo1gESeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#    OPTIMIZERS & OPTIMIZERCONFIG      #\n",
        "########################################\n",
        "\n",
        "class GradientDescent:\n",
        "    \"\"\"Gradient Descent optimizer with iteration-level logging.\"\"\"\n",
        "    def __init__(self, learning_rate=0.001, max_iter=1000, tol=1e-6):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def optimize(self, func: Callable, grad: Callable, x0: np.ndarray,\n",
        "                 x_min: Optional[np.ndarray] = None):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "         - x_final, f_final, n_iter, success\n",
        "         - path, f_path\n",
        "         - iter_df (DataFrame with iteration-level logs)\n",
        "        \"\"\"\n",
        "        import pandas as pd\n",
        "        x_curr = x0.copy()\n",
        "        path = []\n",
        "        fvals = []\n",
        "\n",
        "        rows = []\n",
        "\n",
        "        g_curr = grad(x_curr)\n",
        "        f_curr = func(x_curr)\n",
        "        dist_curr = distance_to_minimum(x_curr, x_min)\n",
        "\n",
        "        step_sz = np.nan\n",
        "        grad_cos_sim = np.nan\n",
        "        rel_imp_f = np.nan\n",
        "        rel_imp_d = np.nan\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            path.append(x_curr.copy())\n",
        "            fvals.append(f_curr)\n",
        "\n",
        "            rows.append({\n",
        "                'method': 'GradientDescent',\n",
        "                'dimension': len(x0),\n",
        "                'iteration': i,\n",
        "                'x': x_curr.tolist(),\n",
        "                'f_val': f_curr,\n",
        "                'dist_to_min': dist_curr,\n",
        "                'step_size': step_sz,\n",
        "                'grad_norm': float(np.linalg.norm(g_curr)),\n",
        "                'grad_cosine_sim': grad_cos_sim,\n",
        "                'rel_improvement_f': rel_imp_f,\n",
        "                'rel_improvement_dist': rel_imp_d\n",
        "            })\n",
        "\n",
        "            if np.linalg.norm(g_curr) < self.tol:\n",
        "                break\n",
        "\n",
        "            x_prev = x_curr.copy()\n",
        "            f_prev = f_curr\n",
        "            dist_prev = dist_curr\n",
        "            g_prev = g_curr.copy()\n",
        "\n",
        "            x_curr = x_curr - self.learning_rate*g_prev\n",
        "            g_curr = grad(x_curr)\n",
        "            f_curr = func(x_curr)\n",
        "            dist_curr = distance_to_minimum(x_curr, x_min)\n",
        "\n",
        "            step_sz = step_size(x_curr, x_prev)\n",
        "            grad_cos_sim = grad_cosine_similarity(g_curr, g_prev)\n",
        "            rel_imp_f = relative_improvement(f_curr, f_prev)\n",
        "            rel_imp_d = relative_improvement(dist_curr, dist_prev)\n",
        "\n",
        "        iter_df = pd.DataFrame(rows)\n",
        "\n",
        "        return {\n",
        "            'x_final': x_curr,\n",
        "            'f_final': f_curr,\n",
        "            'n_iter': i+1,\n",
        "            'success': (np.linalg.norm(g_curr) < self.tol),\n",
        "            'path': path,\n",
        "            'f_path': fvals,\n",
        "            'iter_df': iter_df\n",
        "        }\n",
        "\n",
        "\n",
        "class NewtonRaphson:\n",
        "    \"\"\"Newton–Raphson with approximate Hessian + iteration-level logging.\"\"\"\n",
        "    def __init__(self, max_iter=200, tol=1e-6):\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def optimize(self, func: Callable, grad: Callable, hess_approx: Callable,\n",
        "                 x0: np.ndarray, x_min: Optional[np.ndarray] = None):\n",
        "        import pandas as pd\n",
        "        x_curr = x0.copy()\n",
        "        path = []\n",
        "        fvals = []\n",
        "\n",
        "        rows = []\n",
        "\n",
        "        g_curr = grad(x_curr)\n",
        "        f_curr = func(x_curr)\n",
        "        dist_curr = distance_to_minimum(x_curr, x_min)\n",
        "\n",
        "        step_sz = np.nan\n",
        "        grad_cos_sim = np.nan\n",
        "        rel_imp_f = np.nan\n",
        "        rel_imp_d = np.nan\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            path.append(x_curr.copy())\n",
        "            fvals.append(f_curr)\n",
        "\n",
        "            rows.append({\n",
        "                'method': 'NewtonRaphson',\n",
        "                'dimension': len(x0),\n",
        "                'iteration': i,\n",
        "                'x': x_curr.tolist(),\n",
        "                'f_val': f_curr,\n",
        "                'dist_to_min': dist_curr,\n",
        "                'step_size': step_sz,\n",
        "                'grad_norm': float(np.linalg.norm(g_curr)),\n",
        "                'grad_cosine_sim': grad_cos_sim,\n",
        "                'rel_improvement_f': rel_imp_f,\n",
        "                'rel_improvement_dist': rel_imp_d\n",
        "            })\n",
        "\n",
        "            if np.linalg.norm(g_curr) < self.tol:\n",
        "                break\n",
        "\n",
        "            H = hess_approx(x_curr)\n",
        "            try:\n",
        "                p = np.linalg.solve(H, g_curr)\n",
        "            except np.linalg.LinAlgError:\n",
        "                break\n",
        "\n",
        "            x_prev = x_curr.copy()\n",
        "            f_prev = f_curr\n",
        "            dist_prev = dist_curr\n",
        "            g_prev = g_curr.copy()\n",
        "\n",
        "            x_curr = x_curr - p\n",
        "            g_curr = grad(x_curr)\n",
        "            f_curr = func(x_curr)\n",
        "            dist_curr = distance_to_minimum(x_curr, x_min)\n",
        "\n",
        "            step_sz = step_size(x_curr, x_prev)\n",
        "            grad_cos_sim = grad_cosine_similarity(g_curr, g_prev)\n",
        "            rel_imp_f = relative_improvement(f_curr, f_prev)\n",
        "            rel_imp_d = relative_improvement(dist_curr, dist_prev)\n",
        "\n",
        "        iter_df = pd.DataFrame(rows)\n",
        "\n",
        "        return {\n",
        "            'x_final': x_curr,\n",
        "            'f_final': f_curr,\n",
        "            'n_iter': i+1,\n",
        "            'success': (np.linalg.norm(g_curr) < self.tol),\n",
        "            'path': path,\n",
        "            'f_path': fvals,\n",
        "            'iter_df': iter_df\n",
        "        }\n",
        "\n",
        "\n",
        "class OptimizerConfig:\n",
        "    \"\"\"\n",
        "    Allows you to control which optimizers are enabled for each dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.enabled_optimizers = {\n",
        "            2:   ['GradientDescent', 'NewtonRaphson'],\n",
        "            8:   ['GradientDescent', 'NewtonRaphson'],\n",
        "            32:  ['GradientDescent', 'NewtonRaphson'],\n",
        "            128: ['GradientDescent'],\n",
        "            512: ['GradientDescent'],\n",
        "            2048:['GradientDescent']\n",
        "        }\n",
        "\n",
        "    def get_enabled(self, dimension: int) -> List[str]:\n",
        "        return self.enabled_optimizers.get(dimension, [])\n"
      ],
      "metadata": {
        "id": "h7gdKhQqEVPK"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "#     EXTENDED FUNCTIONS  #\n",
        "###########################\n",
        "\n",
        "class TestFunctions:\n",
        "    \"\"\"Extended test functions that work with any dimension.\"\"\"\n",
        "    @staticmethod\n",
        "    def get_global_minimum(func_name: str, dimension: int = 2) -> tuple:\n",
        "        \"\"\"Get global minimum for a given function and dimension\"\"\"\n",
        "        # If dimension affects the global min (like in Michealwicz), you can handle it dynamically\n",
        "        global_minima = {\n",
        "            'ackley': (np.zeros(dimension), 0.0),\n",
        "            'rastrigin': (np.zeros(dimension), 0.0),\n",
        "            'rosenbrock': (np.ones(dimension), 0.0),\n",
        "            'sphere': (np.zeros(dimension), 0.0),\n",
        "            'schwefel': (420.9687 * np.ones(dimension), 0.0),\n",
        "            'sum_squares': (np.zeros(dimension), 0.0),\n",
        "            'michalewicz': (None, None),  # Varies with dimension\n",
        "        }\n",
        "        return global_minima.get(func_name.lower(), (None, None))\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley(x: np.ndarray) -> float:\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "        return (-20.0 * np.exp(-0.2*np.sqrt(sum_sq/n))\n",
        "                - np.exp(sum_cos/n)\n",
        "                + 20.0 + np.e)\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "\n",
        "        # Handle potential /0 if sum_sq=0\n",
        "        if sum_sq < 1e-15:\n",
        "            term1 = 0.0 * x\n",
        "        else:\n",
        "            term1 = (20.0 * 0.2 / np.sqrt(n*sum_sq)) * np.exp(-0.2 * np.sqrt(sum_sq/n)) * x\n",
        "\n",
        "        term2 = (2.0 * np.pi / n)*np.exp(sum_cos/n)*np.sin(2.0 * np.pi * x)\n",
        "        return term1 + term2\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        f_grad = TestFunctions.ackley_gradient\n",
        "\n",
        "        base_grad = f_grad(x)\n",
        "        for j in range(n):\n",
        "            x_jp = x.copy()\n",
        "            x_jp[j] += eps\n",
        "            grad_p = f_grad(x_jp)\n",
        "            H[:, j] = (grad_p - base_grad)/eps\n",
        "        return 0.5*(H + H.T)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin(x: np.ndarray) -> float:\n",
        "        n = len(x)\n",
        "        return 10.0*n + np.sum(x**2 - 10.0*np.cos(2.0*np.pi*x))\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        return 2.0*x + 20.0*np.pi*np.sin(2.0*np.pi*x)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        diag_cos = np.cos(2.0*np.pi*x)\n",
        "        return 2.0*np.eye(n) + 40.0*(np.pi**2)*np.diag(diag_cos)\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel(x: np.ndarray) -> float:\n",
        "        n = len(x)\n",
        "        return 418.9829*n - np.sum(x*np.sin(np.sqrt(np.abs(x))))\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        # sqrt_abs_x might be zero if x=0. We do small eps\n",
        "        eps = 1e-15\n",
        "        sqrt_abs_x = np.sqrt(np.abs(x)+eps)\n",
        "        term1 = np.sin(sqrt_abs_x)\n",
        "        term2 = x*np.cos(sqrt_abs_x)/(2.0*sqrt_abs_x)\n",
        "        return -(term1 + term2)\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad_fn = TestFunctions.schwefel_gradient\n",
        "\n",
        "        base_grad = grad_fn(x)\n",
        "        for j in range(n):\n",
        "            x_jp = x.copy()\n",
        "            x_jp[j] += eps\n",
        "            grad_p = grad_fn(x_jp)\n",
        "            H[:, j] = (grad_p - base_grad)/eps\n",
        "        return 0.5*(H + H.T)\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares(x: np.ndarray) -> float:\n",
        "        # sum_{i=1}^n i*(x_i^2)\n",
        "        i_idx = np.arange(1, len(x)+1)\n",
        "        return np.sum(i_idx*(x**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        i_idx = np.arange(1, len(x)+1)\n",
        "        return 2.0*i_idx*x\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        i_idx = np.arange(1, n+1)\n",
        "        return 2.0*np.diag(i_idx)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere(x: np.ndarray) -> float:\n",
        "        \"\"\"Simple Sphere function, global min at x=0 => f=0\"\"\"\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        return 2.0*x\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        return 2.0*np.eye(n)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock(x: np.ndarray) -> float:\n",
        "        return np.sum(100.0*(x[1:]-x[:-1]**2)**2 + (1.0 - x[:-1])**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        grad = np.zeros(n)\n",
        "        grad[0] = -400.0*x[0]*(x[1]-x[0]**2) - 2.0*(1.0-x[0])\n",
        "        grad[-1] = 200.0*(x[-1]-x[-2]**2)\n",
        "        if n>2:\n",
        "            grad[1:-1] = (200.0*(x[1:-1]-x[:-2]**2)\n",
        "                          - 400.0*x[1:-1]*(x[2:]-x[1:-1]**2)\n",
        "                          - 2.0*(1.0-x[1:-1]))\n",
        "        return grad\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        def grad_rb(xx):\n",
        "            return TestFunctions.rosenbrock_gradient(xx)\n",
        "\n",
        "        base_grad = grad_rb(x)\n",
        "        for j in range(n):\n",
        "            x_jp = x.copy()\n",
        "            x_jp[j] += eps\n",
        "            grad_p = grad_rb(x_jp)\n",
        "            H[:, j] = (grad_p - base_grad)/eps\n",
        "        return 0.5*(H + H.T)\n"
      ],
      "metadata": {
        "id": "DtVTGV6DEXqq"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Experiments"
      ],
      "metadata": {
        "id": "Br-zPeN_EZWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ RUN_EXPERIMENTS.PY ============\n",
        "\n",
        "def pick_function_components(func_name: str):\n",
        "    \"\"\"\n",
        "    Return (f, grad, hess) from TestFunctions, matching the name.\n",
        "    \"\"\"\n",
        "    func_name = func_name.lower()\n",
        "    if func_name == 'ackley':\n",
        "        return TestFunctions.ackley, TestFunctions.ackley_gradient, TestFunctions.ackley_hessian\n",
        "    elif func_name == 'rastrigin':\n",
        "        return TestFunctions.rastrigin, TestFunctions.rastrigin_gradient, TestFunctions.rastrigin_hessian\n",
        "    elif func_name == 'rosenbrock':\n",
        "        return TestFunctions.rosenbrock, TestFunctions.rosenbrock_gradient, TestFunctions.rosenbrock_hessian\n",
        "    elif func_name == 'sphere':\n",
        "        return TestFunctions.sphere, TestFunctions.sphere_gradient, TestFunctions.sphere_hessian\n",
        "    elif func_name == 'schwefel':\n",
        "        return TestFunctions.schwefel, TestFunctions.schwefel_gradient, TestFunctions.schwefel_hessian\n",
        "    elif func_name == 'sum_squares':\n",
        "        return TestFunctions.sum_squares, TestFunctions.sum_squares_gradient, TestFunctions.sum_squares_hessian\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported function: {func_name}\")\n",
        "\n",
        "\n",
        "def generate_starting_points(\n",
        "    func_name: str,\n",
        "    dimension: int,\n",
        "    n_points: int,\n",
        "    min_dist: float,\n",
        "    max_dist: float,\n",
        "    seed: int = 42\n",
        "):\n",
        "    np.random.seed(seed)\n",
        "    starts = []\n",
        "\n",
        "    x_min, _ = TestFunctions.get_global_minimum(func_name, dimension)\n",
        "    if x_min is None:\n",
        "        # fallback in [-2,2]^dim\n",
        "        for _ in range(n_points):\n",
        "            x0 = np.random.uniform(-2, 2, size=dimension)\n",
        "            starts.append(x0)\n",
        "        return starts\n",
        "\n",
        "    for _ in range(n_points):\n",
        "        direction = np.random.randn(dimension)\n",
        "        direction /= (np.linalg.norm(direction) + 1e-12)\n",
        "        dist = np.random.uniform(min_dist, max_dist)\n",
        "        x0 = x_min + dist*direction\n",
        "        starts.append(x0)\n",
        "    return starts\n",
        "\n",
        "\n",
        "def run_experiment_once(\n",
        "    run_id: int,\n",
        "    func_name: str,\n",
        "    dimension: int,\n",
        "    x0: np.ndarray,\n",
        "    optimizer_name: str,\n",
        "    all_optimizers: dict,\n",
        "    save_dir: str = \"results_csv\"\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Pick (f, grad, hess) from pick_function_components(func_name)\n",
        "    2) Run the chosen optimizer\n",
        "    3) Save iteration-level CSV =>\n",
        "       <save_dir>/<dimension>D/<mapped_method>/<func_name>/run_<mapped_method>_<run_id>.csv\n",
        "    4) Return final row dict for summary\n",
        "    \"\"\"\n",
        "    f, grad, hess_approx = pick_function_components(func_name)\n",
        "\n",
        "    # Map internal method name to short folder name\n",
        "    method_subdir_map = {\n",
        "        \"NewtonRaphson\": \"newton\",\n",
        "        \"GradientDescent\": \"gradient\"\n",
        "        # Add more if needed\n",
        "    }\n",
        "    method_subdir = method_subdir_map.get(optimizer_name, optimizer_name.lower())\n",
        "\n",
        "    x_min, _ = TestFunctions.get_global_minimum(func_name, dimension)\n",
        "    optimizer = all_optimizers[optimizer_name]\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Actually run\n",
        "    if optimizer_name == \"GradientDescent\":\n",
        "        result = optimizer.optimize(f, grad, x0, x_min=x_min)\n",
        "    elif optimizer_name == \"NewtonRaphson\":\n",
        "        result = optimizer.optimize(f, grad, hess_approx, x0, x_min=x_min)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer '{optimizer_name}'\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed = end_time - start_time\n",
        "\n",
        "    path = result.get('path', [])\n",
        "    f_path = result.get('f_path', [])\n",
        "    iter_df = result.get('iter_df')\n",
        "\n",
        "    if len(path) > 0:\n",
        "        x_initial = path[0]\n",
        "        f_initial = f_path[0]\n",
        "        x_final = path[-1]\n",
        "        f_final = f_path[-1]\n",
        "    else:\n",
        "        x_initial = x0\n",
        "        x_final = result.get('x_final', x0)\n",
        "        f_initial = f(x_initial)\n",
        "        f_final = f(x_final)\n",
        "\n",
        "    if x_min is not None:\n",
        "        dist_init = float(np.linalg.norm(x_initial - x_min))\n",
        "        dist_final = float(np.linalg.norm(x_final - x_min))\n",
        "    else:\n",
        "        dist_init = float('nan')\n",
        "        dist_final = float('nan')\n",
        "\n",
        "    # build local dir\n",
        "    dim_dir = os.path.join(save_dir, f\"{dimension}D\")\n",
        "    method_dir = os.path.join(dim_dir, method_subdir)\n",
        "    func_dir = os.path.join(method_dir, func_name.lower())\n",
        "    os.makedirs(func_dir, exist_ok=True)\n",
        "\n",
        "    # e.g. run_gradient_1.csv\n",
        "    csv_name = f\"run_{method_subdir}_{run_id}.csv\"\n",
        "    csv_path = os.path.join(func_dir, csv_name)\n",
        "\n",
        "    if iter_df is not None:\n",
        "        iter_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # build row for summary\n",
        "    row = {\n",
        "        'experiment_num': run_id,  # local run ID\n",
        "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
        "        'function': func_name,\n",
        "        'dimension': dimension,\n",
        "        'method': optimizer_name,\n",
        "        'success': result['success'],\n",
        "        'iterations': result['n_iter'],\n",
        "        'runtime': elapsed,\n",
        "        'f_initial': f_initial,\n",
        "        'f_final': f_final,\n",
        "        'x_initial': x_initial.tolist(),\n",
        "        'x_final': x_final.tolist(),\n",
        "        'initial_distance_to_minimum': dist_init,\n",
        "        'final_distance_to_minimum': dist_final\n",
        "    }\n",
        "    return row\n",
        "\n",
        "\n",
        "def run_experiments(\n",
        "    func_names: List[str],\n",
        "    dimensions: List[int],\n",
        "    n_experiments: int,\n",
        "    all_optimizers: dict,\n",
        "    optimizer_config,\n",
        "    save_dir: str = \"results_csv\",\n",
        "    min_dist: float = 1.0,\n",
        "    max_dist: float = 5.0\n",
        "):\n",
        "    \"\"\"\n",
        "    For each function, dimension, method:\n",
        "      - generate n_experiments starting points\n",
        "      - for run_id in 1..n_experiments:\n",
        "          x0 = starts[run_id-1]\n",
        "          produce run_<method>_<run_id>.csv in subdir\n",
        "      - also append row to summary.csv\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    summary_path = os.path.join(save_dir, \"summary.csv\")\n",
        "\n",
        "    # create summary.csv with header if needed\n",
        "    if not os.path.exists(summary_path):\n",
        "        with open(summary_path, 'w') as f:\n",
        "            f.write(\"experiment_num,timestamp,function,dimension,method,success,\"\n",
        "                    \"iterations,runtime,f_initial,f_final,x_initial,x_final,\"\n",
        "                    \"initial_distance_to_minimum,final_distance_to_minimum\\n\")\n",
        "\n",
        "    for func_name in func_names:\n",
        "        for dim in dimensions:\n",
        "            # figure out which methods\n",
        "            enabled = optimizer_config.get_enabled(dim)\n",
        "            # create the random starts\n",
        "            starts = generate_starting_points(func_name, dim, n_experiments, min_dist, max_dist)\n",
        "\n",
        "            for method in enabled:\n",
        "                for run_id in range(1, n_experiments+1):\n",
        "                    # x0 = the run_id-th start\n",
        "                    x0 = starts[run_id-1]\n",
        "\n",
        "                    row = run_experiment_once(\n",
        "                        run_id=run_id,\n",
        "                        func_name=func_name,\n",
        "                        dimension=dim,\n",
        "                        x0=x0,\n",
        "                        optimizer_name=method,\n",
        "                        all_optimizers=all_optimizers,\n",
        "                        save_dir=save_dir\n",
        "                    )\n",
        "\n",
        "                    # append row to summary\n",
        "                    with open(summary_path, 'a') as f:\n",
        "                        f.write(\n",
        "                            f\"{row['experiment_num']},{row['timestamp']},{row['function']},\"\n",
        "                            f\"{row['dimension']},{row['method']},{row['success']},\"\n",
        "                            f\"{row['iterations']},{row['runtime']},{row['f_initial']},\"\n",
        "                            f\"{row['f_final']},\\\"{row['x_initial']}\\\",\\\"{row['x_final']}\\\",\"\n",
        "                            f\"{row['initial_distance_to_minimum']},{row['final_distance_to_minimum']}\\n\"\n",
        "                        )\n",
        "\n",
        "    print(f\"Experiments complete. Summary => {summary_path}\")\n"
      ],
      "metadata": {
        "id": "_o7maUnTEbrZ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "#     RUN EXPERIMENTS    #\n",
        "##########################\n",
        "\n",
        "def run_experiments_in_dir(\n",
        "    base_dir: str,\n",
        "    func_names: List[str],\n",
        "    dimensions: List[int],\n",
        "    n_experiments: int,\n",
        "    all_optimizers: dict,\n",
        "    optimizer_config,\n",
        "    min_dist: float = 1.0,\n",
        "    max_dist: float = 5.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Run all experiments but place results in <base_dir>/results_csv/...\n",
        "    \"\"\"\n",
        "    # local \"results_csv\" inside base_dir\n",
        "    save_dir = os.path.join(base_dir, \"results_csv\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    summary_path = os.path.join(save_dir, \"summary.csv\")\n",
        "    if not os.path.exists(summary_path):\n",
        "        with open(summary_path, 'w') as f:\n",
        "            f.write(\"experiment_num,timestamp,function,dimension,method,success,iterations,\"\n",
        "                    \"runtime,f_initial,f_final,x_initial,x_final,initial_distance_to_minimum,\"\n",
        "                    \"final_distance_to_minimum\\n\")\n",
        "\n",
        "    for func_name in func_names:\n",
        "        for dim in dimensions:\n",
        "            # which methods\n",
        "            enabled_methods = optimizer_config.get_enabled(dim)\n",
        "            # generate starts\n",
        "            starts = generate_starting_points(func_name, dim, n_experiments, min_dist, max_dist)\n",
        "\n",
        "            for method in enabled_methods:\n",
        "                for run_id in range(1, n_experiments+1):\n",
        "                    x0 = starts[run_id-1]\n",
        "\n",
        "                    row = run_experiment_once(\n",
        "                        run_id=run_id,\n",
        "                        func_name=func_name,\n",
        "                        dimension=dim,\n",
        "                        x0=x0,\n",
        "                        optimizer_name=method,\n",
        "                        all_optimizers=all_optimizers,\n",
        "                        save_dir=save_dir  # pass the local \"results_csv\" path\n",
        "                    )\n",
        "\n",
        "                    # append to summary\n",
        "                    with open(summary_path, 'a') as f:\n",
        "                        f.write(\n",
        "                            f\"{row['experiment_num']},{row['timestamp']},{row['function']},\"\n",
        "                            f\"{row['dimension']},{row['method']},{row['success']},\"\n",
        "                            f\"{row['iterations']},{row['runtime']},{row['f_initial']},\"\n",
        "                            f\"{row['f_final']},\\\"{row['x_initial']}\\\",\\\"{row['x_final']}\\\",\"\n",
        "                            f\"{row['initial_distance_to_minimum']},{row['final_distance_to_minimum']}\\n\"\n",
        "                        )\n",
        "\n",
        "    print(f\"\\nAll experiments done. Summary => {summary_path}\")"
      ],
      "metadata": {
        "id": "i_5L7TVuBsWR"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "NCvwm12cEdJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "#          MAIN          #\n",
        "##########################\n",
        "\n",
        "def main():\n",
        "    # 1) Setup your Git manager\n",
        "    username = \"Ice-Citron\"\n",
        "    repo_name = \"AAH-IA\"\n",
        "    git_manager = GitManager(username, repo_name)\n",
        "\n",
        "    # 2) Clone & create a new branch => returns e.g. \"optimization_results/experiment_YYYYMMDD_HHMMSS\"\n",
        "    local_repo_dir = git_manager.setup_repo()\n",
        "    print(f\"Local repo directory: {local_repo_dir}\")\n",
        "\n",
        "    # 3) Define your experiment parameters\n",
        "    func_names = ['ackley', 'rastrigin', 'rosenbrock', 'sphere', 'schwefel', 'sum_squares']\n",
        "    dims = [2, 8, 32, 128, 512, 2048]\n",
        "    n_experiments = 5\n",
        "\n",
        "    # 4) Create your optimizers\n",
        "    gd = GradientDescent(learning_rate=0.001, max_iter=500, tol=1e-8)\n",
        "    nr = NewtonRaphson(max_iter=200, tol=1e-8)\n",
        "\n",
        "    all_optimizers = {\n",
        "        'GradientDescent': gd,\n",
        "        'NewtonRaphson': nr\n",
        "    }\n",
        "\n",
        "    # 5) Config\n",
        "    optimizer_cfg = OptimizerConfig()\n",
        "\n",
        "    # 6) Actually run experiments & store in local_repo_dir\n",
        "    run_experiments_in_dir(\n",
        "        base_dir=local_repo_dir,\n",
        "        func_names=func_names,\n",
        "        dimensions=dims,\n",
        "        n_experiments=n_experiments,\n",
        "        all_optimizers=all_optimizers,\n",
        "        optimizer_config=optimizer_cfg,\n",
        "        min_dist=2.0,\n",
        "        max_dist=5.0\n",
        "    )\n",
        "\n",
        "    # 7) After finishing, push results to GitHub\n",
        "    try:\n",
        "        git_manager.push_results()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to push: {e}\")\n",
        "\n",
        "    print(\"Done with main().\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYg1FSSmEmVQ",
        "outputId": "05bc0ea8-f017-4845-e3af-d14b9af51a50"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created/checked out branch: optimization_1943\n",
            "Local repo directory: optimization_results/experiment_20250115_035846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-37ca142519a9>:144: RuntimeWarning: overflow encountered in scalar multiply\n",
            "  grad[0] = -400.0*x[0]*(x[1]-x[0]**2) - 2.0*(1.0-x[0])\n",
            "<ipython-input-110-37ca142519a9>:138: RuntimeWarning: overflow encountered in square\n",
            "  return np.sum(100.0*(x[1:]-x[:-1]**2)**2 + (1.0 - x[:-1])**2)\n",
            "<ipython-input-108-b5538cbaae27>:23: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  return float((prev_val - current_val)/abs(prev_val))\n",
            "<ipython-input-109-64afc0a9ed50>:62: RuntimeWarning: invalid value encountered in subtract\n",
            "  x_curr = x_curr - self.learning_rate*g_prev\n",
            "<ipython-input-108-b5538cbaae27>:18: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return float(np.dot(g_curr, g_prev)/(norm_curr*norm_prev))\n",
            "<ipython-input-110-37ca142519a9>:144: RuntimeWarning: overflow encountered in scalar power\n",
            "  grad[0] = -400.0*x[0]*(x[1]-x[0]**2) - 2.0*(1.0-x[0])\n",
            "<ipython-input-110-37ca142519a9>:145: RuntimeWarning: overflow encountered in scalar power\n",
            "  grad[-1] = 200.0*(x[-1]-x[-2]**2)\n",
            "<ipython-input-110-37ca142519a9>:144: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  grad[0] = -400.0*x[0]*(x[1]-x[0]**2) - 2.0*(1.0-x[0])\n",
            "<ipython-input-110-37ca142519a9>:145: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  grad[-1] = 200.0*(x[-1]-x[-2]**2)\n",
            "<ipython-input-110-37ca142519a9>:138: RuntimeWarning: invalid value encountered in subtract\n",
            "  return np.sum(100.0*(x[1:]-x[:-1]**2)**2 + (1.0 - x[:-1])**2)\n",
            "<ipython-input-110-37ca142519a9>:147: RuntimeWarning: overflow encountered in square\n",
            "  grad[1:-1] = (200.0*(x[1:-1]-x[:-2]**2)\n",
            "<ipython-input-110-37ca142519a9>:148: RuntimeWarning: overflow encountered in square\n",
            "  - 400.0*x[1:-1]*(x[2:]-x[1:-1]**2)\n",
            "<ipython-input-110-37ca142519a9>:148: RuntimeWarning: overflow encountered in multiply\n",
            "  - 400.0*x[1:-1]*(x[2:]-x[1:-1]**2)\n",
            "<ipython-input-110-37ca142519a9>:147: RuntimeWarning: invalid value encountered in subtract\n",
            "  grad[1:-1] = (200.0*(x[1:-1]-x[:-2]**2)\n",
            "<ipython-input-110-37ca142519a9>:148: RuntimeWarning: invalid value encountered in subtract\n",
            "  - 400.0*x[1:-1]*(x[2:]-x[1:-1]**2)\n",
            "<ipython-input-110-37ca142519a9>:109: RuntimeWarning: overflow encountered in multiply\n",
            "  return np.sum(i_idx*(x**2))\n",
            "<ipython-input-110-37ca142519a9>:109: RuntimeWarning: overflow encountered in square\n",
            "  return np.sum(i_idx*(x**2))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All experiments done. Summary => optimization_results/experiment_20250115_035846/results_csv/summary.csv\n",
            "\n",
            "Pushing results from experiment_20250115_035846...\n",
            "Created PR at: https://github.com/Ice-Citron/AAH-IA/pull/22\n",
            "Done with main().\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fhkq1D9RB7oT"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lspyrDkICyqW"
      },
      "execution_count": 113,
      "outputs": []
    }
  ]
}