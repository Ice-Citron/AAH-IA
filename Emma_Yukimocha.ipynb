{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b4DrfPQD8bu",
        "outputId": "cf7bbd64-5c3b-468f-f5ad-1f15e173b7d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (3.1.44)\n",
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython) (4.0.12)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from PyGithub) (2.3.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from PyGithub) (1.2.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.2)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.14.0->PyGithub) (2024.12.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->PyGithub) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas matplotlib seaborn gitpython PyGithub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emt10RyeEP8n"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "uXFR2EPoEQ7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2457390f-5ee5-456e-e366-c4fda79fc3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub token set from colab secrets.\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "# 1) Imports & Basic Setup\n",
        "##################################################\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Callable, Optional\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "# For Git\n",
        "import git\n",
        "from github import Github\n",
        "\n",
        "\n",
        "##################################################\n",
        "# 1.1) Retrieve GitHub Token from Colab Secrets\n",
        "##################################################\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    GH_TOKEN = userdata.get(\"github\")  # 'github' must match your secret's name exactly\n",
        "    if GH_TOKEN:\n",
        "        import os\n",
        "        os.environ[\"github\"] = GH_TOKEN\n",
        "        print(\"GitHub token set from colab secrets.\")\n",
        "    else:\n",
        "        print(\"No GitHub token found in colab secrets.\")\n",
        "except ImportError:\n",
        "    print(\"Not in Colab or no userdata available.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading colab secret: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNsf56ZvBRyG"
      },
      "source": [
        "# GitManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "qYjjupbjF3I2"
      },
      "outputs": [],
      "source": [
        "!git config --global user.name \"Ice-Citron\"\n",
        "!git config --global user.email \"shng2025@gmail.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "iIW0RE-DBTlo"
      },
      "outputs": [],
      "source": [
        "############################\n",
        "#        GIT MANAGER       #\n",
        "############################\n",
        "\n",
        "class GitManager:\n",
        "    \"\"\"Handles GitHub operations to push results to a random-named branch.\"\"\"\n",
        "    def __init__(self, username: str, repo_name: str):\n",
        "        self.username = username\n",
        "        self.repo_name = repo_name\n",
        "        # after your above block sets os.environ[\"github\"], read it here:\n",
        "        self.token = os.environ.get('github')\n",
        "        self.repo_url = f\"https://x-access-token:{self.token}@github.com/{username}/{repo_name}.git\"\n",
        "        self.branch_id = f\"optimization_{random.randint(1000, 9999)}\"\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.repo = None\n",
        "\n",
        "    def setup_repo(self) -> str:\n",
        "        import git\n",
        "\n",
        "        try:\n",
        "            root_dir = \"optimization_results\"\n",
        "            os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "            with git.Git().custom_environment(GIT_SSL_NO_VERIFY='true'):\n",
        "                try:\n",
        "                    self.repo = git.Repo(root_dir)\n",
        "                    origin = self.repo.remote('origin')\n",
        "                    origin.set_url(self.repo_url)\n",
        "                except git.exc.InvalidGitRepositoryError:\n",
        "                    self.repo = git.Repo.init(root_dir)\n",
        "                    origin = self.repo.create_remote('origin', self.repo_url)\n",
        "\n",
        "                    config_writer = self.repo.config_writer()\n",
        "                    config_writer.set_value(\"http\", \"sslVerify\", \"false\")\n",
        "                    config_writer.release()\n",
        "\n",
        "                origin.fetch()\n",
        "                origin.pull('main')\n",
        "                new_branch = self.repo.create_head(self.branch_id, origin.refs.main)\n",
        "                new_branch.checkout()\n",
        "\n",
        "                experiment_dir = os.path.join(root_dir, f\"experiment_{self.timestamp}\")\n",
        "                os.makedirs(experiment_dir, exist_ok=True)\n",
        "                print(f\"Successfully created/checked out branch: {self.branch_id}\")\n",
        "                return experiment_dir\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up repository: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def push_results(self):\n",
        "        try:\n",
        "            env = {\n",
        "                'GIT_SSL_NO_VERIFY': 'true',\n",
        "                'GIT_TERMINAL_PROMPT': '0',\n",
        "                'GIT_USERNAME': 'x-access-token',\n",
        "                'GIT_PASSWORD': self.token\n",
        "            }\n",
        "\n",
        "            with self.repo.git.custom_environment(**env):\n",
        "                print(f\"\\nPushing results from experiment_{self.timestamp}...\")\n",
        "                self.repo.index.add('*')\n",
        "                self.repo.index.commit(f\"Results from experiment_{self.timestamp}\")\n",
        "                self.repo.remotes.origin.push(self.branch_id)\n",
        "\n",
        "                g = Github(self.token)\n",
        "                repo = g.get_repo(f\"{self.username}/{self.repo_name}\")\n",
        "                pr = repo.create_pull(\n",
        "                    title=f\"Results from experiment_{self.timestamp}\",\n",
        "                    body=f\"Automated results from {self.branch_id}\",\n",
        "                    head=self.branch_id,\n",
        "                    base=\"main\"\n",
        "                )\n",
        "                print(f\"Created PR at: {pr.html_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error pushing results: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftJAaFd0OHee"
      },
      "source": [
        "# Helper Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "BrU_Pc0uOJ4W"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "#     HELPER METRICS      #\n",
        "###########################\n",
        "\n",
        "def distance_to_minimum(x: np.ndarray, x_min: Optional[np.ndarray]) -> float:\n",
        "    if x_min is None:\n",
        "        return float('nan')\n",
        "    return float(np.linalg.norm(x - x_min))\n",
        "\n",
        "def step_size(x_curr: np.ndarray, x_prev: np.ndarray) -> float:\n",
        "    return float(np.linalg.norm(x_curr - x_prev))\n",
        "\n",
        "def grad_cosine_similarity(g_curr: np.ndarray, g_prev: np.ndarray) -> float:\n",
        "    norm_curr = np.linalg.norm(g_curr)\n",
        "    norm_prev = np.linalg.norm(g_prev)\n",
        "    if norm_curr < 1e-15 or norm_prev < 1e-15:\n",
        "        return float('nan')\n",
        "    return float(np.dot(g_curr, g_prev)/(norm_curr*norm_prev))\n",
        "\n",
        "def relative_improvement(current_val: float, prev_val: float) -> float:\n",
        "    if abs(prev_val) < 1e-15:\n",
        "        return 0.0\n",
        "    return float((prev_val - current_val)/abs(prev_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ANmo1gESeO"
      },
      "source": [
        "# Optimizers and Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "h7gdKhQqEVPK"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "#     OPTIMIZERS          #\n",
        "###########################\n",
        "\n",
        "class GradientDescent:\n",
        "    \"\"\"Gradient Descent optimizer with iteration-level logging.\"\"\"\n",
        "    def __init__(self, learning_rate=0.001, max_iter=1000, tol=1e-6):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def optimize(self, func: Callable, grad: Callable, x0: np.ndarray,\n",
        "                 x_min: Optional[np.ndarray] = None):\n",
        "        import pandas as pd\n",
        "        x_curr = x0.copy()\n",
        "        path = []\n",
        "        fvals = []\n",
        "        rows = []\n",
        "\n",
        "        g_curr = grad(x_curr)\n",
        "        f_curr = func(x_curr)\n",
        "        dist_curr = distance_to_minimum(x_curr, x_min)\n",
        "\n",
        "        step_sz = np.nan\n",
        "        grad_cos_sim = np.nan\n",
        "        rel_imp_f = np.nan\n",
        "        rel_imp_d = np.nan\n",
        "        rel_imp_g = np.nan  # new\n",
        "\n",
        "        dim = len(x0)\n",
        "        for i in range(self.max_iter):\n",
        "            # decide if we store x or not\n",
        "            store_x = x_curr.tolist() if dim < 128 else None\n",
        "\n",
        "            path.append(x_curr.copy())\n",
        "            fvals.append(f_curr)\n",
        "\n",
        "            rows.append({\n",
        "                'method': 'GradientDescent',\n",
        "                'dimension': dim,\n",
        "                'iteration': i,\n",
        "                'x': store_x,\n",
        "                'f_val': f_curr,\n",
        "                'dist_to_min': dist_curr,\n",
        "                'step_size': step_sz,\n",
        "                'grad_norm': float(np.linalg.norm(g_curr)),\n",
        "                'grad_cosine_sim': grad_cos_sim,\n",
        "                'rel_improvement_f': rel_imp_f,\n",
        "                'rel_improvement_dist': rel_imp_d,\n",
        "                'rel_improvement_grad': rel_imp_g  # newly added column\n",
        "            })\n",
        "\n",
        "            if np.linalg.norm(g_curr) < self.tol:\n",
        "                break\n",
        "\n",
        "            # save old states for next iteration\n",
        "            x_prev = x_curr.copy()\n",
        "            f_prev = f_curr\n",
        "            dist_prev = dist_curr\n",
        "            g_prev = g_curr.copy()\n",
        "            grad_norm_prev = float(np.linalg.norm(g_prev))\n",
        "\n",
        "            # do update\n",
        "            x_curr = x_curr - self.learning_rate * g_prev\n",
        "            g_curr = grad(x_curr)\n",
        "            f_curr = func(x_curr)\n",
        "            dist_curr = distance_to_minimum(x_curr, x_min)\n",
        "\n",
        "            step_sz = step_size(x_curr, x_prev)\n",
        "            grad_cos_sim = grad_cosine_similarity(g_curr, g_prev)\n",
        "            rel_imp_f = relative_improvement(f_curr, f_prev)\n",
        "            rel_imp_d = relative_improvement(dist_curr, dist_prev)\n",
        "\n",
        "            grad_norm_curr = float(np.linalg.norm(g_curr))\n",
        "            rel_imp_g = relative_improvement(grad_norm_curr, grad_norm_prev)\n",
        "\n",
        "        success = (np.linalg.norm(g_curr) < self.tol)\n",
        "\n",
        "        iter_df = pd.DataFrame(rows)\n",
        "        return {\n",
        "            'x_final': x_curr,\n",
        "            'f_final': f_curr,\n",
        "            'n_iter': i+1,\n",
        "            'success': success,\n",
        "            'path': path,\n",
        "            'f_path': fvals,\n",
        "            'iter_df': iter_df\n",
        "        }\n",
        "\n",
        "\n",
        "class NewtonRaphson:\n",
        "    \"\"\"Newton–Raphson with approximate Hessian + iteration-level logging.\"\"\"\n",
        "    def __init__(self, max_iter=200, tol=1e-6):\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "    def optimize(self, func: Callable, grad: Callable, hess_approx: Callable,\n",
        "                 x0: np.ndarray, x_min: Optional[np.ndarray] = None):\n",
        "        import pandas as pd\n",
        "        x_curr = x0.copy()\n",
        "        path = []\n",
        "        fvals = []\n",
        "        rows = []\n",
        "\n",
        "        g_curr = grad(x_curr)\n",
        "        f_curr = func(x_curr)\n",
        "        dist_curr = distance_to_minimum(x_curr, x_min)\n",
        "\n",
        "        step_sz = np.nan\n",
        "        grad_cos_sim = np.nan\n",
        "        rel_imp_f = np.nan\n",
        "        rel_imp_d = np.nan\n",
        "        rel_imp_g = np.nan  # new\n",
        "\n",
        "        dim = len(x0)\n",
        "        for i in range(self.max_iter):\n",
        "            store_x = x_curr.tolist() if dim < 128 else None\n",
        "\n",
        "            path.append(x_curr.copy())\n",
        "            fvals.append(f_curr)\n",
        "\n",
        "            rows.append({\n",
        "                'method': 'NewtonRaphson',\n",
        "                'dimension': dim,\n",
        "                'iteration': i,\n",
        "                'x': store_x,\n",
        "                'f_val': f_curr,\n",
        "                'dist_to_min': dist_curr,\n",
        "                'step_size': step_sz,\n",
        "                'grad_norm': float(np.linalg.norm(g_curr)),\n",
        "                'grad_cosine_sim': grad_cos_sim,\n",
        "                'rel_improvement_f': rel_imp_f,\n",
        "                'rel_improvement_dist': rel_imp_d,\n",
        "                'rel_improvement_grad': rel_imp_g  # newly added\n",
        "            })\n",
        "\n",
        "            if np.linalg.norm(g_curr) < self.tol:\n",
        "                break\n",
        "\n",
        "            H = hess_approx(x_curr)\n",
        "            try:\n",
        "                p = np.linalg.solve(H, g_curr)\n",
        "            except np.linalg.LinAlgError:\n",
        "                break\n",
        "\n",
        "            x_prev = x_curr.copy()\n",
        "            f_prev = f_curr\n",
        "            dist_prev = dist_curr\n",
        "            g_prev = g_curr.copy()\n",
        "            grad_norm_prev = float(np.linalg.norm(g_prev))\n",
        "\n",
        "            x_curr = x_curr - p\n",
        "            g_curr = grad(x_curr)\n",
        "            f_curr = func(x_curr)\n",
        "            dist_curr = distance_to_minimum(x_curr, x_min)\n",
        "\n",
        "            step_sz = step_size(x_curr, x_prev)\n",
        "            grad_cos_sim = grad_cosine_similarity(g_curr, g_prev)\n",
        "            rel_imp_f = relative_improvement(f_curr, f_prev)\n",
        "            rel_imp_d = relative_improvement(dist_curr, dist_prev)\n",
        "\n",
        "            grad_norm_curr = float(np.linalg.norm(g_curr))\n",
        "            rel_imp_g = relative_improvement(grad_norm_curr, grad_norm_prev)\n",
        "\n",
        "        success = (np.linalg.norm(g_curr) < self.tol)\n",
        "\n",
        "        iter_df = pd.DataFrame(rows)\n",
        "        return {\n",
        "            'x_final': x_curr,\n",
        "            'f_final': f_curr,\n",
        "            'n_iter': i+1,\n",
        "            'success': success,\n",
        "            'path': path,\n",
        "            'f_path': fvals,\n",
        "            'iter_df': iter_df\n",
        "        }\n",
        "\n",
        "# dims = [2, 8, 128, 512, 2048, 8192, 32768, 131072]  # you can skip big dims if files are too huge\n",
        "class OptimizerConfig:\n",
        "    \"\"\"Allows you to control which optimizers are enabled for each dimension.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.enabled_optimizers = {\n",
        "            2:   ['GradientDescent', 'NewtonRaphson'],\n",
        "            8:   ['GradientDescent', 'NewtonRaphson'],\n",
        "            32:  ['GradientDescent', 'NewtonRaphson'],\n",
        "            128: ['GradientDescent', \"NewtonRaphson\"],\n",
        "            512: ['GradientDescent', \"NewtonRaphson\"], # 20 seconds\n",
        "            2048:['GradientDescent'], # 206 seconds\n",
        "            8192: ['GradientDescent'], # 299 seconds\n",
        "            32768: ['GradientDescent'], # 2731-5000+ seconds\n",
        "            131072: ['GradientDescent'], # too long\n",
        "        }\n",
        "\n",
        "    def get_enabled(self, dimension: int) -> List[str]:\n",
        "        return self.enabled_optimizers.get(dimension, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "DtVTGV6DEXqq"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "# EXTENDED TEST FUNCTIONS\n",
        "###########################\n",
        "\n",
        "class TestFunctions:\n",
        "    \"\"\"Extended test functions that work with any dimension.\"\"\"\n",
        "    @staticmethod\n",
        "    def get_global_minimum(func_name: str, dimension: int = 2) -> tuple:\n",
        "        global_minima = {\n",
        "            'ackley': (np.zeros(dimension), 0.0),\n",
        "            'rastrigin': (np.zeros(dimension), 0.0),\n",
        "            'rosenbrock': (np.ones(dimension), 0.0),\n",
        "            'sphere': (np.zeros(dimension), 0.0),\n",
        "            'schwefel': (420.9687 * np.ones(dimension), 0.0),\n",
        "            'sum_squares': (np.zeros(dimension), 0.0),\n",
        "            'michalewicz': (None, None),\n",
        "        }\n",
        "        return global_minima.get(func_name.lower(), (None, None))\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley(x: np.ndarray) -> float:\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2*np.pi*x))\n",
        "        return (\n",
        "            -20.0*np.exp(-0.2*np.sqrt(sum_sq/n))\n",
        "            - np.exp(sum_cos/n)\n",
        "            + 20.0 + np.e\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2.0*np.pi*x))\n",
        "        if sum_sq < 1e-15:\n",
        "            term1 = 0.0*x\n",
        "        else:\n",
        "            term1 = (20.0*0.2/np.sqrt(n*sum_sq))*np.exp(-0.2*np.sqrt(sum_sq/n))*x\n",
        "        term2 = (2.0*np.pi/n)*np.exp(sum_cos/n)*np.sin(2.0*np.pi*x)\n",
        "        return term1 + term2\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n,n))\n",
        "        base_grad = TestFunctions.ackley_gradient(x)\n",
        "        for j in range(n):\n",
        "            x_jp = x.copy()\n",
        "            x_jp[j] += eps\n",
        "            grad_p = TestFunctions.ackley_gradient(x_jp)\n",
        "            H[:, j] = (grad_p - base_grad)/eps\n",
        "        return 0.5*(H + H.T)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin(x: np.ndarray) -> float:\n",
        "        n = len(x)\n",
        "        return 10.0*n + np.sum(x**2 - 10.0*np.cos(2.0*np.pi*x))\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        return 2.0*x + 20.0*np.pi*np.sin(2.0*np.pi*x)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        diag_cos = np.cos(2.0*np.pi*x)\n",
        "        return 2.0*np.eye(n) + 40.0*(np.pi**2)*np.diag(diag_cos)\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel(x: np.ndarray) -> float:\n",
        "        n = len(x)\n",
        "        return 418.9829*n - np.sum(x*np.sin(np.sqrt(np.abs(x))))\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        eps = 1e-15\n",
        "        sqrt_abs_x = np.sqrt(np.abs(x)+eps)\n",
        "        term1 = np.sin(sqrt_abs_x)\n",
        "        term2 = x*np.cos(sqrt_abs_x)/(2.0*sqrt_abs_x)\n",
        "        return -(term1 + term2)\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n,n))\n",
        "        grad_fn = TestFunctions.schwefel_gradient\n",
        "        base_grad = grad_fn(x)\n",
        "        for j in range(n):\n",
        "            x_ij = x.copy()\n",
        "            x_ij[j] += eps\n",
        "            H[:, j] = (grad_fn(x_ij) - base_grad)/eps\n",
        "        return 0.5*(H + H.T)\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares(x: np.ndarray) -> float:\n",
        "        i_idx = np.arange(1, len(x)+1)\n",
        "        return np.sum(i_idx*(x**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        i_idx = np.arange(1, len(x)+1)\n",
        "        return 2.0*i_idx*x\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        i_idx = np.arange(1, n+1)\n",
        "        return 2.0*np.diag(i_idx)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere(x: np.ndarray) -> float:\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        return 2.0*x\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        return 2.0*np.eye(n)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock(x: np.ndarray) -> float:\n",
        "        return np.sum(100.0*(x[1:]-x[:-1]**2)**2 + (1.0-x[:-1])**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        n = len(x)\n",
        "        grad = np.zeros(n)\n",
        "        grad[0] = -400.0*x[0]*(x[1]-x[0]**2) - 2.0*(1.0-x[0])\n",
        "        grad[-1] = 200.0*(x[-1]-x[-2]**2)\n",
        "        if n>2:\n",
        "            grad[1:-1] = (\n",
        "                200.0*(x[1:-1]-x[:-2]**2)\n",
        "                -400.0*x[1:-1]*(x[2:]-x[1:-1]**2)\n",
        "                -2.0*(1.0-x[1:-1])\n",
        "            )\n",
        "        return grad\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n,n))\n",
        "        def grad_rb(xx):\n",
        "            return TestFunctions.rosenbrock_gradient(xx)\n",
        "        base_grad = grad_rb(x)\n",
        "        for j in range(n):\n",
        "            x_jp = x.copy()\n",
        "            x_jp[j] += eps\n",
        "            grad_p = grad_rb(x_jp)\n",
        "            H[:, j] = (grad_p - base_grad)/eps\n",
        "        return 0.5*(H + H.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-zPeN_EZWf"
      },
      "source": [
        "# Run Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "_o7maUnTEbrZ"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "#   RUN EXPERIMENTS\n",
        "##########################\n",
        "\n",
        "def pick_function_components(func_name: str):\n",
        "    func_name = func_name.lower()\n",
        "    if func_name == 'ackley':\n",
        "        return (TestFunctions.ackley,\n",
        "                TestFunctions.ackley_gradient,\n",
        "                TestFunctions.ackley_hessian)\n",
        "    elif func_name == 'rastrigin':\n",
        "        return (TestFunctions.rastrigin,\n",
        "                TestFunctions.rastrigin_gradient,\n",
        "                TestFunctions.rastrigin_hessian)\n",
        "    elif func_name == 'rosenbrock':\n",
        "        return (TestFunctions.rosenbrock,\n",
        "                TestFunctions.rosenbrock_gradient,\n",
        "                TestFunctions.rosenbrock_hessian)\n",
        "    elif func_name == 'sphere':\n",
        "        return (TestFunctions.sphere,\n",
        "                TestFunctions.sphere_gradient,\n",
        "                TestFunctions.sphere_hessian)\n",
        "    elif func_name == 'schwefel':\n",
        "        return (TestFunctions.schwefel,\n",
        "                TestFunctions.schwefel_gradient,\n",
        "                TestFunctions.schwefel_hessian)\n",
        "    elif func_name == 'sum_squares':\n",
        "        return (TestFunctions.sum_squares,\n",
        "                TestFunctions.sum_squares_gradient,\n",
        "                TestFunctions.sum_squares_hessian)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported function: {func_name}\")\n",
        "\n",
        "\n",
        "def generate_starting_points(\n",
        "    func_name: str,\n",
        "    dimension: int,\n",
        "    n_points: int,\n",
        "    min_dist: float,\n",
        "    max_dist: float,\n",
        "    seed: int = 42\n",
        "):\n",
        "    np.random.seed(seed)\n",
        "    starts = []\n",
        "\n",
        "    x_min, _ = TestFunctions.get_global_minimum(func_name, dimension)\n",
        "    if x_min is None:\n",
        "        for _ in range(n_points):\n",
        "            x0 = np.random.uniform(-2, 2, size=dimension)\n",
        "            starts.append(x0)\n",
        "        return starts\n",
        "\n",
        "    for _ in range(n_points):\n",
        "        direction = np.random.randn(dimension)\n",
        "        direction /= (np.linalg.norm(direction) + 1e-12)\n",
        "        dist = np.random.uniform(min_dist, max_dist)\n",
        "        x0 = x_min + dist*direction\n",
        "        starts.append(x0)\n",
        "    return starts\n",
        "\n",
        "\n",
        "def run_experiment_once(\n",
        "    run_id: int,\n",
        "    func_name: str,\n",
        "    dimension: int,\n",
        "    x0: np.ndarray,\n",
        "    optimizer_name: str,\n",
        "    all_optimizers: dict,\n",
        "    save_dir: str = \"results_csv\"\n",
        "):\n",
        "    # pick function\n",
        "    f, grad, hess_approx = pick_function_components(func_name)\n",
        "    method_subdir_map = {\n",
        "        \"NewtonRaphson\": \"newton\",\n",
        "        \"GradientDescent\": \"gradient\"\n",
        "    }\n",
        "    method_subdir = method_subdir_map.get(optimizer_name, optimizer_name.lower())\n",
        "\n",
        "    x_min, f_min = TestFunctions.get_global_minimum(func_name, dimension)\n",
        "    # compute initial grad norm\n",
        "    init_grad_norm = float(np.linalg.norm(grad(x0)))\n",
        "\n",
        "    optimizer = all_optimizers[optimizer_name]\n",
        "\n",
        "    start_time = time.time()\n",
        "    if optimizer_name == \"GradientDescent\":\n",
        "        result = optimizer.optimize(f, grad, x0, x_min=x_min)\n",
        "    elif optimizer_name == \"NewtonRaphson\":\n",
        "        result = optimizer.optimize(f, grad, hess_approx, x0, x_min=x_min)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer '{optimizer_name}'\")\n",
        "    end_time = time.time()\n",
        "    elapsed = end_time - start_time\n",
        "\n",
        "    path = result.get('path', [])\n",
        "    f_path = result.get('f_path', [])\n",
        "    iter_df = result.get('iter_df')\n",
        "\n",
        "    if len(path) > 0:\n",
        "        x_initial = path[0]\n",
        "        f_initial = f_path[0]\n",
        "        x_final = path[-1]\n",
        "        f_final = f_path[-1]\n",
        "    else:\n",
        "        x_initial = x0\n",
        "        x_final = result.get('x_final', x0)\n",
        "        f_initial = f(x_initial)\n",
        "        f_final = f(x_final)\n",
        "\n",
        "    # final grad norm\n",
        "    final_grad_norm = float(np.linalg.norm(grad(x_final)))\n",
        "\n",
        "    if x_min is not None:\n",
        "        dist_init = float(np.linalg.norm(x_initial - x_min))\n",
        "        dist_final = float(np.linalg.norm(x_final - x_min))\n",
        "    else:\n",
        "        dist_init = float('nan')\n",
        "        dist_final = float('nan')\n",
        "\n",
        "    # Overwrite success if final solution is worse than initial\n",
        "    success = result['success']\n",
        "    if f_final > f_initial + 1e-12:\n",
        "        success = False\n",
        "    if dist_final > dist_init + 1e-12:\n",
        "        success = False\n",
        "\n",
        "    # 4) relative improvements\n",
        "    rel_imp_f = relative_improvement(f_final, f_initial)\n",
        "    rel_imp_d = relative_improvement(dist_final, dist_init)\n",
        "    rel_imp_g = relative_improvement(final_grad_norm, init_grad_norm)\n",
        "\n",
        "    # Suppose we've already computed:\n",
        "    # success = ...\n",
        "    # f_initial, f_final\n",
        "    # dist_init, dist_final\n",
        "\n",
        "    # 1) Compute improvement fraction for function and distance\n",
        "    f_improvement_fraction = 0.0\n",
        "    dist_improvement_fraction = 0.0\n",
        "\n",
        "    if abs(f_initial) > 1e-15:\n",
        "        f_improvement_fraction = 1.0 - (f_final / f_initial)\n",
        "    if abs(dist_init) > 1e-15:\n",
        "        dist_improvement_fraction = 1.0 - (dist_final / dist_init)\n",
        "\n",
        "    # 2) Define thresholds in multiples of 15% up to 90% => [0.15, 0.30, 0.45, 0.60, 0.75, 0.90]\n",
        "    thresholds = [0.15, 0.30, 0.45, 0.60, 0.75, 0.90]\n",
        "\n",
        "    # 3) For each threshold, define success_f_<THRESH>, success_dist_<THRESH>\n",
        "    #    e.g. success_f_15 => (f_improvement_fraction >= 0.15)\n",
        "    success_f_15  = (f_improvement_fraction  >= 0.15)\n",
        "    success_f_30  = (f_improvement_fraction  >= 0.30)\n",
        "    success_f_45  = (f_improvement_fraction  >= 0.45)\n",
        "    success_f_60  = (f_improvement_fraction  >= 0.60)\n",
        "    success_f_75  = (f_improvement_fraction  >= 0.75)\n",
        "    success_f_90  = (f_improvement_fraction  >= 0.90)\n",
        "\n",
        "    success_dist_15  = (dist_improvement_fraction >= 0.15)\n",
        "    success_dist_30  = (dist_improvement_fraction >= 0.30)\n",
        "    success_dist_45  = (dist_improvement_fraction >= 0.45)\n",
        "    success_dist_60  = (dist_improvement_fraction >= 0.60)\n",
        "    success_dist_75  = (dist_improvement_fraction >= 0.75)\n",
        "    success_dist_90  = (dist_improvement_fraction >= 0.90)\n",
        "\n",
        "\n",
        "    # build local dir\n",
        "    dim_dir = os.path.join(save_dir, f\"{dimension}D\")\n",
        "    os.makedirs(dim_dir, exist_ok=True)\n",
        "    method_dir = os.path.join(dim_dir, method_subdir)\n",
        "    os.makedirs(method_dir, exist_ok=True)\n",
        "    func_dir = os.path.join(method_dir, func_name.lower())\n",
        "    os.makedirs(func_dir, exist_ok=True)\n",
        "\n",
        "    csv_name = f\"run_{method_subdir}_{run_id}.csv\"\n",
        "    csv_path = os.path.join(func_dir, csv_name)\n",
        "\n",
        "    if iter_df is not None:\n",
        "        iter_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # Reorder summary columns =>\n",
        "    # experiment_num, timestamp, function, dimension, method, success,\n",
        "    # iterations, runtime, f_initial, f_final, initial_distance_to_minimum,\n",
        "    # final_distance_to_minimum, initial_grad_norm, final_grad_norm,\n",
        "    # x_initial, x_final\n",
        "    row = {\n",
        "        # existing fields\n",
        "        'experiment_num': run_id,\n",
        "        'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
        "        'function': func_name,\n",
        "        'dimension': dimension,\n",
        "        'method': optimizer_name,\n",
        "        'success': success,  # gradient-based\n",
        "        'iterations': result['n_iter'],\n",
        "        'runtime': elapsed,\n",
        "        'f_initial': f_initial,\n",
        "        'f_final': f_final,\n",
        "        'initial_distance_to_minimum': dist_init,\n",
        "        'final_distance_to_minimum': dist_final,\n",
        "        'initial_grad_norm': init_grad_norm,\n",
        "        'final_grad_norm': final_grad_norm,\n",
        "        'relative_improvement_f': rel_imp_f,\n",
        "        'relative_improvement_dist': rel_imp_d,\n",
        "        'relative_improvement_grad': rel_imp_g,\n",
        "\n",
        "        # Now add new success flags\n",
        "        'success_f_15': success_f_15,\n",
        "        'success_f_30': success_f_30,\n",
        "        'success_f_45': success_f_45,\n",
        "        'success_f_60': success_f_60,\n",
        "        'success_f_75': success_f_75,\n",
        "        'success_f_90': success_f_90,\n",
        "\n",
        "        'success_dist_15': success_dist_15,\n",
        "        'success_dist_30': success_dist_30,\n",
        "        'success_dist_45': success_dist_45,\n",
        "        'success_dist_60': success_dist_60,\n",
        "        'success_dist_75': success_dist_75,\n",
        "        'success_dist_90': success_dist_90,\n",
        "\n",
        "        # keep x_initial, x_final last\n",
        "        'x_initial': x_initial.tolist(),\n",
        "        'x_final': x_final.tolist()\n",
        "    }\n",
        "\n",
        "    return row\n",
        "\n",
        "\n",
        "def run_experiments(\n",
        "    func_names: List[str],\n",
        "    dimensions: List[int],\n",
        "    n_experiments: int,\n",
        "    all_optimizers: dict,\n",
        "    optimizer_config,\n",
        "    save_dir: str = \"results_csv\",\n",
        "    min_dist: float = 1.0,\n",
        "    max_dist: float = 5.0\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    summary_path = os.path.join(save_dir, \"summary.csv\")\n",
        "\n",
        "    # create summary with new col order:\n",
        "    # experiment_num, timestamp, function, dimension, method, success,\n",
        "    # iterations, runtime, f_initial, f_final, initial_distance_to_minimum,\n",
        "    # final_distance_to_minimum, initial_grad_norm, final_grad_norm,\n",
        "    # x_initial, x_final\n",
        "    if not os.path.exists(summary_path):\n",
        "        with open(summary_path, 'w') as f:\n",
        "            f.write(\n",
        "                \"experiment_num,timestamp,function,dimension,method,success,\"\n",
        "                \"iterations,runtime,\"\n",
        "                \"f_initial,f_final,\"\n",
        "                \"initial_distance_to_minimum,final_distance_to_minimum,\"\n",
        "                \"initial_grad_norm,final_grad_norm,\"\n",
        "                \"relative_improvement_f,relative_improvement_dist,relative_improvement_grad,\"\n",
        "                # now add your new success columns\n",
        "                \"success_f_15,success_f_30,success_f_45,success_f_60,success_f_75,success_f_90,\"\n",
        "                \"success_dist_15,success_dist_30,success_dist_45,success_dist_60,success_dist_75,success_dist_90,\"\n",
        "                \"x_initial,x_final\\n\"\n",
        "            )\n",
        "\n",
        "    for func_name in func_names:\n",
        "        for dim in dimensions:\n",
        "            enabled = optimizer_config.get_enabled(dim)\n",
        "            starts = generate_starting_points(func_name, dim, n_experiments, min_dist, max_dist)\n",
        "\n",
        "            for method in enabled:\n",
        "                for run_id in range(1, n_experiments+1):\n",
        "                    x0 = starts[run_id-1]\n",
        "                    row = run_experiment_once(\n",
        "                        run_id=run_id,\n",
        "                        func_name=func_name,\n",
        "                        dimension=dim,\n",
        "                        x0=x0,\n",
        "                        optimizer_name=method,\n",
        "                        all_optimizers=all_optimizers,\n",
        "                        save_dir=save_dir\n",
        "                    )\n",
        "\n",
        "                    # 2) updated .write(...) line:\n",
        "                    with open(summary_path, 'a') as f:\n",
        "                        f.write(\n",
        "                            f\"{row['experiment_num']},{row['timestamp']},{row['function']},\"\n",
        "                            f\"{row['dimension']},{row['method']},{row['success']},\"\n",
        "                            f\"{row['iterations']},{row['runtime']},\"\n",
        "                            f\"{row['f_initial']},{row['f_final']},\"\n",
        "                            f\"{row['initial_distance_to_minimum']},{row['final_distance_to_minimum']},\"\n",
        "                            f\"{row['initial_grad_norm']},{row['final_grad_norm']},\"\n",
        "                            f\"{row['relative_improvement_f']},{row['relative_improvement_dist']},{row['relative_improvement_grad']},\"\n",
        "                            # New success columns for function\n",
        "                            f\"{row['success_f_15']},{row['success_f_30']},{row['success_f_45']},\"\n",
        "                            f\"{row['success_f_60']},{row['success_f_75']},{row['success_f_90']},\"\n",
        "                            # New success columns for distance\n",
        "                            f\"{row['success_dist_15']},{row['success_dist_30']},{row['success_dist_45']},\"\n",
        "                            f\"{row['success_dist_60']},{row['success_dist_75']},{row['success_dist_90']},\"\n",
        "                            # Keep x_initial, x_final last\n",
        "                            f\"\\\"{row['x_initial']}\\\",\\\"{row['x_final']}\\\"\\n\"\n",
        "                        )\n",
        "    print(f\"Experiments complete. Summary => {summary_path}\")\n",
        "\n",
        "\n",
        "def run_experiments_in_dir(\n",
        "    base_dir: str,\n",
        "    func_names: List[str],\n",
        "    dimensions: List[int],\n",
        "    n_experiments: int,\n",
        "    all_optimizers: dict,\n",
        "    optimizer_config,\n",
        "    min_dist: float = 1.0,\n",
        "    max_dist: float = 5.0\n",
        "):\n",
        "    save_dir = os.path.join(base_dir, \"results_csv\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    summary_path = os.path.join(save_dir, \"summary.csv\")\n",
        "    # Where you do:\n",
        "    if not os.path.exists(summary_path):\n",
        "        with open(summary_path, 'w') as f:\n",
        "            f.write(\n",
        "                \"experiment_num,timestamp,function,dimension,method,success,\"\n",
        "                \"iterations,runtime,\"\n",
        "                \"f_initial,f_final,\"\n",
        "                \"initial_distance_to_minimum,final_distance_to_minimum,\"\n",
        "                \"initial_grad_norm,final_grad_norm,\"\n",
        "                \"relative_improvement_f,relative_improvement_dist,relative_improvement_grad,\"\n",
        "                # now add your new success columns\n",
        "                \"success_f_15,success_f_30,success_f_45,success_f_60,success_f_75,success_f_90,\"\n",
        "                \"success_dist_15,success_dist_30,success_dist_45,success_dist_60,success_dist_75,success_dist_90,\"\n",
        "                \"x_initial,x_final\\n\"\n",
        "            )\n",
        "\n",
        "\n",
        "    for func_name in func_names:\n",
        "        for dim in dimensions:\n",
        "            enabled_methods = optimizer_config.get_enabled(dim)\n",
        "            starts = generate_starting_points(func_name, dim, n_experiments, min_dist, max_dist)\n",
        "            for method in enabled_methods:\n",
        "                for run_id in range(1, n_experiments+1):\n",
        "                    x0 = starts[run_id-1]\n",
        "                    row = run_experiment_once(\n",
        "                        run_id=run_id,\n",
        "                        func_name=func_name,\n",
        "                        dimension=dim,\n",
        "                        x0=x0,\n",
        "                        optimizer_name=method,\n",
        "                        all_optimizers=all_optimizers,\n",
        "                        save_dir=save_dir\n",
        "                    )\n",
        "                    with open(summary_path, 'a') as f:\n",
        "                        f.write(\n",
        "                            f\"{row['experiment_num']},{row['timestamp']},{row['function']},\"\n",
        "                            f\"{row['dimension']},{row['method']},{row['success']},\"\n",
        "                            f\"{row['iterations']},{row['runtime']},\"\n",
        "                            f\"{row['f_initial']},{row['f_final']},\"\n",
        "                            f\"{row['initial_distance_to_minimum']},{row['final_distance_to_minimum']},\"\n",
        "                            f\"{row['initial_grad_norm']},{row['final_grad_norm']},\"\n",
        "                            f\"{row['relative_improvement_f']},{row['relative_improvement_dist']},{row['relative_improvement_grad']},\"\n",
        "                            # New success columns for function\n",
        "                            f\"{row['success_f_15']},{row['success_f_30']},{row['success_f_45']},\"\n",
        "                            f\"{row['success_f_60']},{row['success_f_75']},{row['success_f_90']},\"\n",
        "                            # New success columns for distance\n",
        "                            f\"{row['success_dist_15']},{row['success_dist_30']},{row['success_dist_45']},\"\n",
        "                            f\"{row['success_dist_60']},{row['success_dist_75']},{row['success_dist_90']},\"\n",
        "                            # Keep x_initial, x_final last\n",
        "                            f\"\\\"{row['x_initial']}\\\",\\\"{row['x_final']}\\\"\\n\"\n",
        "                        )\n",
        "    print(f\"\\nAll experiments done. Summary => {summary_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCvwm12cEdJ8"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYg1FSSmEmVQ",
        "outputId": "f9db1309-6483-430d-a030-913bc0516a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created/checked out branch: optimization_7526\n",
            "Local repo directory: optimization_results/experiment_20250116_032431\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "#                MAIN FUNCTION\n",
        "##################################################\n",
        "\n",
        "def main():\n",
        "    from google.colab import userdata\n",
        "    username = \"Ice-Citron\"\n",
        "    repo_name = \"AAH-IA\"\n",
        "    git_manager = GitManager(username, repo_name)\n",
        "\n",
        "    local_repo_dir = git_manager.setup_repo()\n",
        "    print(f\"Local repo directory: {local_repo_dir}\")\n",
        "\n",
        "    # example\n",
        "    func_names = ['ackley', 'rastrigin', 'rosenbrock', 'schwefel', 'sum_squares']\n",
        "    dims = [2, 8, 128, 512, 2048, 8192, 32768, 131072]  # you can skip big dims if files are too huge\n",
        "    n_experiments = 15\n",
        "\n",
        "    gd = GradientDescent(learning_rate=0.001, max_iter=2000, tol=1e-4)\n",
        "    nr = NewtonRaphson(max_iter=200, tol=1e-4)\n",
        "\n",
        "    all_optimizers = {\n",
        "        'GradientDescent': gd,\n",
        "        'NewtonRaphson': nr\n",
        "    }\n",
        "\n",
        "    optimizer_cfg = OptimizerConfig()\n",
        "\n",
        "    run_experiments_in_dir(\n",
        "        base_dir=local_repo_dir,\n",
        "        func_names=func_names,\n",
        "        dimensions=dims,\n",
        "        n_experiments=n_experiments,\n",
        "        all_optimizers=all_optimizers,\n",
        "        optimizer_config=optimizer_cfg,\n",
        "        min_dist=2.0,\n",
        "        max_dist=5.0\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        git_manager.push_results()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to push: {e}\")\n",
        "\n",
        "    print(\"Done with main().\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "import wandb\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8cYBQUopm6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ILoO6mDApmyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import getpass\n",
        "\n",
        "token = getpass.getpass('Enter your GitHub Personal Access Token: ')\n",
        "repo_url = \"github.com/Ice-Citron/AAH-IA.git\"\n",
        "\n",
        "!git remote remove origin\n",
        "!git remote add origin https://{git PAT}@github.com/Ice-Citron/AAH-IA.git"
      ],
      "metadata": {
        "id": "lspyrDkICyqW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}