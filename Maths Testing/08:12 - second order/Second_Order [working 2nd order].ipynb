{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INITIAL TEST"
      ],
      "metadata": {
        "id": "eiRTKba4bQw1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jwVvDUwtaQgE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, List, Dict, Optional, Tuple\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import psutil\n",
        "\n",
        "\n",
        "\n",
        "class OptimizationResult:\n",
        "    \"\"\"Enhanced optimization result storage\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        self.x_final = kwargs.get('x_final')\n",
        "        self.f_final = kwargs.get('f_final')\n",
        "        self.success = kwargs.get('success')\n",
        "        self.iterations = kwargs.get('iterations')\n",
        "        self.runtime = kwargs.get('runtime')\n",
        "        self.path = kwargs.get('path', [])\n",
        "        self.f_path = kwargs.get('f_path', [])\n",
        "        self.grad_norm_path = kwargs.get('grad_norm_path', [])\n",
        "        self.timestamps = kwargs.get('timestamps', [])\n",
        "        self.memory_usage = kwargs.get('memory_usage', [])\n",
        "        self.flops_per_step = kwargs.get('flops_per_step', [])\n",
        "        self.method = kwargs.get('method')\n",
        "        self.dimension = kwargs.get('dimension')\n",
        "        self.function_name = kwargs.get('function_name')\n",
        "        self.x_initial = kwargs.get('x_initial')\n",
        "        self.f_initial = kwargs.get('f_initial')\n",
        "        self.grad_initial = kwargs.get('grad_initial')\n",
        "        self.grad_final = kwargs.get('grad_final')\n",
        "\n",
        "        # Calculate distance from global minimum\n",
        "        x_min, f_min = TestFunctions.get_global_minimum(self.function_name, self.dimension)\n",
        "        if x_min is not None and f_min is not None:\n",
        "            self.distance_to_minimum = np.linalg.norm(self.x_final - x_min)\n",
        "            self.f_error = abs(self.f_final - f_min)\n",
        "        else:\n",
        "            self.distance_to_minimum = None\n",
        "            self.f_error = None\n",
        "\n",
        "\n",
        "class FLOPCounter:\n",
        "    \"\"\"Tracks floating point operations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.flops = 0\n",
        "        self.operation_counts = {\n",
        "            'add': 0,\n",
        "            'multiply': 0,\n",
        "            'divide': 0,\n",
        "            'sqrt': 0,\n",
        "            'exp': 0,\n",
        "            'log': 0,\n",
        "            'trig': 0\n",
        "        }\n",
        "\n",
        "    def add_flops(self, operation: str, count: int = 1):\n",
        "        self.operation_counts[operation] += count\n",
        "        # Update total FLOPS based on operation weight\n",
        "        weights = {\n",
        "            'add': 1,\n",
        "            'multiply': 1,\n",
        "            'divide': 4,\n",
        "            'sqrt': 8,\n",
        "            'exp': 10,\n",
        "            'log': 10,\n",
        "            'trig': 15\n",
        "        }\n",
        "        self.flops += weights[operation] * count\n",
        "\n",
        "    def get_summary(self) -> dict:\n",
        "        return {\n",
        "            'total_flops': self.flops,\n",
        "            'operations': self.operation_counts\n",
        "        }\n",
        "\n",
        "class TestFunctions:\n",
        "    \"\"\"Test functions that work with any dimension\"\"\"\n",
        "    @staticmethod\n",
        "    def get_global_minimum(func_name: str, dimension: int = 2) -> tuple:\n",
        "        \"\"\"Get global minimum for a given function and dimension\"\"\"\n",
        "        global_minima = {\n",
        "            'ackley': (np.zeros(dimension), 0.0),\n",
        "            'rastrigin': (np.zeros(dimension), 0.0),\n",
        "            'rosenbrock': (np.ones(dimension), 0.0),\n",
        "            'sphere': (np.zeros(dimension), 0.0),\n",
        "            'michalewicz': (None, None),  # Varies with dimension\n",
        "        }\n",
        "        return global_minima.get(func_name, (None, None))\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley(x: np.ndarray) -> float:\n",
        "        \"\"\"Ackley function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "        return (-20 * np.exp(-0.2 * np.sqrt(sum_sq / n))\n",
        "                - np.exp(sum_cos / n)\n",
        "                + 20 + np.e)\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Ackley function\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "\n",
        "        term1 = (20 * 0.2 / np.sqrt(n * sum_sq)) * np.exp(-0.2 * np.sqrt(sum_sq / n)) * x\n",
        "        term2 = (2 * np.pi / n) * np.exp(sum_cos / n) * np.sin(2 * np.pi * x)\n",
        "        return term1 + term2\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Ackley Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.ackley_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin(x: np.ndarray) -> float:\n",
        "        \"\"\"Rastrigin function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        return 10 * n + np.sum(x**2 - 10 * np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rastrigin function\"\"\"\n",
        "        return 2 * x + 20 * np.pi * np.sin(2 * np.pi * x)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Rastrigin function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n) + 40 * np.pi**2 * np.diag(np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere(x: np.ndarray) -> float:\n",
        "        \"\"\"Sphere function for n dimensions\"\"\"\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Sphere function\"\"\"\n",
        "        return 2 * x\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Sphere function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock(x: np.ndarray) -> float:\n",
        "        \"\"\"Rosenbrock function for n dimensions\"\"\"\n",
        "        return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rosenbrock function\"\"\"\n",
        "        n = len(x)\n",
        "        grad = np.zeros(n)\n",
        "        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "        grad[-1] = 200 * (x[-1] - x[-2]**2)\n",
        "        if n > 2:\n",
        "            grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
        "        return grad\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Rosenbrock Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.rosenbrock_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "class OptimizationLogger:\n",
        "    \"\"\"Handles logging of optimization progress\"\"\"\n",
        "    def __init__(self, method: str, function_name: str, dimension: int):\n",
        "        self.method = method\n",
        "        self.function_name = function_name\n",
        "        self.dimension = dimension\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.path = []\n",
        "        self.f_path = []\n",
        "        self.grad_norm_path = []\n",
        "        self.step_sizes = []\n",
        "        self.memory_usage = []\n",
        "        self.timestamps = []\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log_iteration(self, x: np.ndarray, f: float, grad_norm: float, step_size: float):\n",
        "        self.path.append(x.copy())\n",
        "        self.f_path.append(f)\n",
        "        self.grad_norm_path.append(grad_norm)\n",
        "        self.step_sizes.append(step_size)\n",
        "        self.memory_usage.append(self.get_memory_usage())\n",
        "        self.timestamps.append(time.time() - self.start_time)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_memory_usage() -> float:\n",
        "        \"\"\"Get current memory usage in MB\"\"\"\n",
        "        import psutil\n",
        "        process = psutil.Process()\n",
        "        return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    def save_logs(self, base_dir: str):\n",
        "        \"\"\"Save optimization logs to CSV\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        log_dir = os.path.join(base_dir, self.function_name, str(self.dimension) + \"D\", self.method)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        log_data = {\n",
        "            'iteration': range(len(self.path)),\n",
        "            'function_value': self.f_path,\n",
        "            'gradient_norm': self.grad_norm_path,\n",
        "            'step_size': self.step_sizes,\n",
        "            'memory_mb': self.memory_usage,\n",
        "            'runtime_seconds': self.timestamps\n",
        "        }\n",
        "\n",
        "        # Add parameter values\n",
        "        for i in range(self.dimension):\n",
        "            log_data[f'x{i+1}'] = [p[i] for p in self.path]\n",
        "\n",
        "        df = pd.DataFrame(log_data)\n",
        "        df.to_csv(os.path.join(log_dir, f'optimization_log_{timestamp}.csv'), index=False)\n",
        "\n",
        "class Visualizer:\n",
        "    \"\"\"Enhanced visualization capabilities\"\"\"\n",
        "    @staticmethod\n",
        "    def plot_optimization_summary(results: Dict[str, OptimizationResult], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot summary comparing initial and final states\"\"\"\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        methods = list(results.keys())\n",
        "        x = np.arange(len(methods))\n",
        "\n",
        "        # Fix the ticks warning by setting them explicitly\n",
        "        for ax in [ax1, ax2, ax3, ax4]:\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(methods, rotation=45)\n",
        "\n",
        "        # Function Values Plot\n",
        "        initial_values = [result.f_initial for result in results.values()]\n",
        "        final_values = [result.f_final for result in results.values()]\n",
        "        width = 0.35\n",
        "\n",
        "        ax1.bar(x - width/2, initial_values, width, label='Initial', color='lightcoral')\n",
        "        ax1.bar(x + width/2, final_values, width, label='Final', color='lightgreen')\n",
        "        ax1.set_ylabel('Function Value')\n",
        "        ax1.set_title('Initial vs Final Function Values')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Add global minimum line if available\n",
        "        _, f_min = TestFunctions.get_global_minimum(function_name, results[methods[0]].dimension)\n",
        "        if f_min is not None:\n",
        "            ax1.axhline(y=f_min, color='r', linestyle='--', label=f'Global Min ({f_min})')\n",
        "            ax1.legend()\n",
        "\n",
        "        # Gradient Norms Plot\n",
        "        initial_grads = [np.linalg.norm(result.grad_initial) for result in results.values()]\n",
        "        final_grads = [np.linalg.norm(result.grad_final) for result in results.values()]\n",
        "\n",
        "        ax2.bar(x - width/2, initial_grads, width, label='Initial', color='lightcoral')\n",
        "        ax2.bar(x + width/2, final_grads, width, label='Final', color='lightgreen')\n",
        "        ax2.set_ylabel('Gradient Norm')\n",
        "        ax2.set_title('Initial vs Final Gradient Norms')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # Runtime Comparison\n",
        "        runtimes = [result.runtime for result in results.values()]\n",
        "        ax3.bar(methods, runtimes, color='skyblue')\n",
        "        ax3.set_ylabel('Runtime (seconds)')\n",
        "        ax3.set_title('Total Runtime by Method')\n",
        "        ax3.grid(True)\n",
        "\n",
        "        # Iterations Comparison\n",
        "        iterations = [result.iterations for result in results.values()]\n",
        "        ax4.bar(methods, iterations, color='lightgreen')\n",
        "        ax4.set_ylabel('Number of Iterations')\n",
        "        ax4.set_title('Total Iterations by Method')\n",
        "        ax4.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, f'optimization_summary_{function_name}.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_2d_trajectory(f: Callable, result: OptimizationResult, save_dir: str, experiment_num: int = None):\n",
        "        \"\"\"Plot optimization trajectory for 2D problems with multi-scale contours\"\"\"\n",
        "        if result.dimension != 2:\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # Get path bounds and include (0,0)\n",
        "        path = np.array(result.path)\n",
        "        x_min_traj = min(float(path[:, 0].min()), 0)\n",
        "        x_max_traj = max(float(path[:, 0].max()), 0)\n",
        "        y_min_traj = min(float(path[:, 1].min()), 0)\n",
        "        y_max_traj = max(float(path[:, 1].max()), 0)\n",
        "\n",
        "        # Add margin to bounds\n",
        "        margin = 0.1\n",
        "        x_range = x_max_traj - x_min_traj\n",
        "        y_range = y_max_traj - y_min_traj\n",
        "        plot_x_min = x_min_traj - margin * x_range\n",
        "        plot_x_max = x_max_traj + margin * x_range\n",
        "        plot_y_min = y_min_traj - margin * y_range\n",
        "        plot_y_max = y_max_traj + margin * y_range\n",
        "\n",
        "        # Create contour plot covering the full trajectory\n",
        "        x = np.linspace(plot_x_min, plot_x_max, 200)\n",
        "        y = np.linspace(plot_y_min, plot_y_max, 200)\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "        Z = np.array([[f(np.array([xi, yi])) for xi in x] for yi in y])\n",
        "\n",
        "        # Create contour levels starting from 0\n",
        "        global_max = float(Z.max())\n",
        "        global_levels = np.linspace(0, global_max, 50)\n",
        "\n",
        "        # Plot contours\n",
        "        plt.contour(X, Y, Z, levels=global_levels, cmap='viridis', alpha=0.7)\n",
        "        plt.colorbar(label='Function Value', ticks=np.linspace(0, global_max, 10))\n",
        "\n",
        "        # Plot trajectory\n",
        "        plt.plot(path[:, 0], path[:, 1], 'r.-', label='Optimization Path', linewidth=1, markersize=2)\n",
        "        plt.plot(path[0, 0], path[0, 1], 'go', label='Start', markersize=8)\n",
        "        plt.plot(path[-1, 0], path[-1, 1], 'ro', label='End', markersize=8)\n",
        "\n",
        "        # Get and plot global minimum\n",
        "        x_min, f_min = TestFunctions.get_global_minimum(result.function_name)\n",
        "        if x_min is not None:\n",
        "            plt.plot(x_min[0], x_min[1], 'k*', label='Global Minimum', markersize=10)\n",
        "\n",
        "        # Set axis limits to show full trajectory (using scalar values)\n",
        "        plt.xlim(float(plot_x_min), float(plot_x_max))\n",
        "        plt.ylim(float(plot_y_min), float(plot_y_max))\n",
        "\n",
        "        # Add gridlines\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Labels and title\n",
        "        plt.title(f'{result.function_name} - {result.method}\\n'\n",
        "                f'Final value: {result.f_final:.6f}\\n'\n",
        "                f'Iterations: {result.iterations}')\n",
        "        plt.xlabel('x₁')\n",
        "        plt.ylabel('x₂')\n",
        "        plt.legend()\n",
        "\n",
        "        # Include experiment number in filename if provided\n",
        "        experiment_suffix = f'_exp{experiment_num}' if experiment_num is not None else ''\n",
        "        filename = f'trajectory_{result.function_name}_{result.method}{experiment_suffix}.png'\n",
        "\n",
        "        plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_convergence(results: Dict[str, OptimizationResult], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot convergence with enhanced information\"\"\"\n",
        "        if not results:  # Skip if no results\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Get global minimum if available\n",
        "        _, f_min = TestFunctions.get_global_minimum(function_name)\n",
        "        f_min_text = f\"(Global min: {f_min})\" if f_min is not None else \"\"\n",
        "\n",
        "        # Function value convergence\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'f_path') and result.f_path:  # Check if result and f_path exist\n",
        "                ax1.semilogy(result.f_path, label=f\"{method}\")\n",
        "        ax1.set_xlabel('Iteration')\n",
        "        ax1.set_ylabel('Function Value (log scale)')\n",
        "        ax1.set_title(f'Function Value Convergence {f_min_text}')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Gradient norm convergence\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'grad_norm_path') and result.grad_norm_path:  # Check if result and grad_norm_path exist\n",
        "                ax2.semilogy(result.grad_norm_path, label=f\"{method}\")\n",
        "        ax2.set_xlabel('Iteration')\n",
        "        ax2.set_ylabel('Gradient Norm (log scale)')\n",
        "        ax2.set_title('Gradient Norm Convergence')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, f'convergence_{function_name}.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_computational_metrics(results: Dict[str, OptimizationResult], save_dir: str):\n",
        "        \"\"\"Plot computational metrics over time\"\"\"\n",
        "        if not results:  # Skip if no results\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Memory usage over time\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'timestamps') and hasattr(result, 'memory_usage'):\n",
        "                if result.timestamps and result.memory_usage:  # Check if data exists\n",
        "                    ax1.plot(result.timestamps, result.memory_usage, label=method)\n",
        "        ax1.set_xlabel('Time (seconds)')\n",
        "        ax1.set_ylabel('Memory Usage (MB)')\n",
        "        ax1.set_title('Memory Usage Over Time')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # FLOPS over time\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'timestamps') and hasattr(result, 'flops_per_step'):\n",
        "                if result.timestamps and result.flops_per_step:  # Check if data exists\n",
        "                    cumulative_flops = np.cumsum(result.flops_per_step)\n",
        "                    ax2.plot(result.timestamps, cumulative_flops, label=method)\n",
        "        ax2.set_xlabel('Time (seconds)')\n",
        "        ax2.set_ylabel('Cumulative FLOPS')\n",
        "        ax2.set_title('Computational Cost Over Time')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'computational_metrics.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "def get_memory_usage() -> float:\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def run_optimization(f: Callable,\n",
        "                    grad: Callable,\n",
        "                    hess: Callable,\n",
        "                    x0: np.ndarray,\n",
        "                    method: str,\n",
        "                    function_name: str) -> OptimizationResult:\n",
        "    \"\"\"Enhanced optimization runner with detailed metrics\"\"\"\n",
        "    start_time = time.time()\n",
        "    flop_counter = FLOPCounter()\n",
        "\n",
        "    # Calculate initial metrics\n",
        "    f_initial = f(x0)\n",
        "    grad_initial = grad(x0)\n",
        "\n",
        "    # Storage for metrics\n",
        "    path = [x0.copy()]  # Start with initial point\n",
        "    f_path = [f_initial]\n",
        "    grad_norm_path = [np.linalg.norm(grad_initial)]\n",
        "    timestamps = [0.0]\n",
        "    memory_usage = [get_memory_usage()]\n",
        "    flops_per_step = [0]\n",
        "\n",
        "    def callback(xk):\n",
        "        current_time = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        f_val = f(xk)\n",
        "        grad_val = grad(xk)\n",
        "        grad_norm = np.linalg.norm(grad_val)\n",
        "\n",
        "        # Store metrics\n",
        "        path.append(xk.copy())\n",
        "        f_path.append(f_val)\n",
        "        grad_norm_path.append(grad_norm)\n",
        "        timestamps.append(current_time)\n",
        "        memory_usage.append(get_memory_usage())\n",
        "        flops_per_step.append(flop_counter.flops)\n",
        "\n",
        "    try:\n",
        "        # Run optimization with method-specific settings\n",
        "        if method == 'BFGS':\n",
        "            result = minimize(f, x0, method=method, jac=grad, callback=callback)\n",
        "        elif method == 'newton-cg':\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        elif method in ['trust-exact', 'trust-krylov']:\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n",
        "        # Calculate final gradient\n",
        "        grad_final = grad(result.x)\n",
        "\n",
        "        return OptimizationResult(\n",
        "            x_final=result.x,\n",
        "            f_final=result.fun,\n",
        "            success=result.success,\n",
        "            iterations=result.nit,\n",
        "            runtime=time.time() - start_time,\n",
        "            path=path,\n",
        "            f_path=f_path,\n",
        "            grad_norm_path=grad_norm_path,\n",
        "            timestamps=timestamps,\n",
        "            memory_usage=memory_usage,\n",
        "            flops_per_step=flops_per_step,\n",
        "            method=method,\n",
        "            dimension=len(x0),\n",
        "            function_name=function_name,\n",
        "            x_initial=x0,\n",
        "            f_initial=f_initial,\n",
        "            grad_initial=grad_initial,\n",
        "            grad_final=grad_final\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Optimization failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentManager:\n",
        "    \"\"\"Manages multiple optimization experiments\"\"\"\n",
        "    def __init__(self,\n",
        "                 test_functions: Dict[str, Tuple[Callable, Callable, Callable]],\n",
        "                 methods: List[str],\n",
        "                 dimensions: List[int],\n",
        "                 n_experiments: int = 50,\n",
        "                 min_dist: float = 100,\n",
        "                 max_dist: float = 1000):\n",
        "        self.test_functions = test_functions\n",
        "        self.methods = methods\n",
        "        self.dimensions = dimensions\n",
        "        self.n_experiments = n_experiments\n",
        "        self.min_dist = min_dist\n",
        "        self.max_dist = max_dist\n",
        "\n",
        "    def generate_starting_points(self, dimension: int, seed: int = None) -> np.ndarray:\n",
        "        \"\"\"Generate random starting points with specified distance from origin\"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        starting_points = []\n",
        "\n",
        "        for _ in range(self.n_experiments):\n",
        "            # Generate random direction vector\n",
        "            direction = np.random.randn(dimension)\n",
        "            direction = direction / np.linalg.norm(direction)\n",
        "\n",
        "            # Generate random distance within specified range\n",
        "            distance = np.random.uniform(self.min_dist, self.max_dist)\n",
        "\n",
        "            # Create starting point\n",
        "            point = direction * distance\n",
        "            starting_points.append(point)\n",
        "\n",
        "        return np.array(starting_points)\n",
        "\n",
        "    def run_experiments(self, base_dir: str = \"optimization_results\"):\n",
        "        \"\"\"Run all experiments with proper directory structure\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        experiment_dir = os.path.join(base_dir, f\"experiment_{timestamp}\")\n",
        "\n",
        "        # Store all results for final analysis\n",
        "        all_results = []\n",
        "\n",
        "        for func_name, (f, grad, hess) in self.test_functions.items():\n",
        "            print(f\"\\nTesting {func_name} function:\")\n",
        "\n",
        "            for dim in self.dimensions:\n",
        "                print(f\"\\nDimension: {dim}\")\n",
        "\n",
        "                # Generate starting points for this dimension\n",
        "                starting_points = self.generate_starting_points(dim, seed=42)\n",
        "\n",
        "                # Create directories\n",
        "                dim_dir = os.path.join(experiment_dir, func_name, f\"{dim}D\")\n",
        "                results_dir = os.path.join(dim_dir, \"results\")\n",
        "                trajectory_dir = os.path.join(dim_dir, \"trajectories\")\n",
        "                os.makedirs(results_dir, exist_ok=True)\n",
        "                if dim == 2:\n",
        "                    os.makedirs(trajectory_dir, exist_ok=True)\n",
        "\n",
        "                # Save starting points\n",
        "                pd.DataFrame(starting_points).to_csv(\n",
        "                    os.path.join(dim_dir, \"starting_points.csv\"),\n",
        "                    index=False\n",
        "                )\n",
        "\n",
        "                # Run optimization for each starting point\n",
        "                for i, x0 in enumerate(starting_points):\n",
        "                    print(f\"\\nExperiment {i+1}/{self.n_experiments}\")\n",
        "                    experiment_results = {}\n",
        "\n",
        "                    for method in self.methods:\n",
        "                        print(f\"Running {method}...\")\n",
        "                        try:\n",
        "                            result = run_optimization(f, grad, hess, x0, method, func_name)\n",
        "                            if result is not None:\n",
        "                                experiment_results[method] = result\n",
        "\n",
        "                                # Add to results list\n",
        "                                all_results.append({\n",
        "                                    'function': func_name,\n",
        "                                    'dimension': dim,\n",
        "                                    'experiment': i,\n",
        "                                    'method': method,\n",
        "                                    'start_distance': np.linalg.norm(x0),\n",
        "                                    'final_value': result.f_final,\n",
        "                                    'iterations': result.iterations,\n",
        "                                    'runtime': result.runtime,\n",
        "                                    'success': result.success,\n",
        "                                    'distance_to_minimum': result.distance_to_minimum,\n",
        "                                    'f_error': result.f_error,\n",
        "                                    'initial_gradient_norm': np.linalg.norm(result.grad_initial),\n",
        "                                    'final_gradient_norm': np.linalg.norm(result.grad_final)\n",
        "                                })\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error with {method}: {e}\")\n",
        "                            continue\n",
        "\n",
        "                    # Generate plots for this experiment\n",
        "                    if experiment_results:\n",
        "                        Visualizer.plot_convergence(experiment_results, results_dir,\n",
        "                                                 f\"{func_name}_exp{i}\")\n",
        "                        Visualizer.plot_computational_metrics(experiment_results, results_dir)\n",
        "                        Visualizer.plot_optimization_summary(experiment_results, results_dir,\n",
        "                                                         f\"{func_name}_exp{i}\")\n",
        "\n",
        "                        if dim == 2:\n",
        "                          for method, result in experiment_results.items():\n",
        "                              if result is not None:\n",
        "                                  Visualizer.plot_2d_trajectory(f, result, trajectory_dir, i)  # Pass experiment number i\n",
        "\n",
        "                # Generate statistical plots for this dimension\n",
        "                results_df = pd.DataFrame([r for r in all_results\n",
        "                                         if r['function'] == func_name and r['dimension'] == dim])\n",
        "                self._generate_statistics(results_df, dim_dir)\n",
        "\n",
        "        # Save complete results\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "        results_df.to_csv(os.path.join(experiment_dir, \"all_results.csv\"), index=False)\n",
        "        self._generate_summary_statistics(results_df, experiment_dir)\n",
        "\n",
        "    def _generate_statistics(self, results_df: pd.DataFrame, save_dir: str):\n",
        "        \"\"\"Generate statistical visualizations for a specific dimension\"\"\"\n",
        "        # Violin plots\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.violinplot(data=results_df, x='method', y='final_value')\n",
        "        plt.title('Distribution of Final Values')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'final_values_distribution.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Success rates\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        success_rates = results_df.groupby('method')['success'].mean()\n",
        "        success_rates.plot(kind='bar')\n",
        "        plt.title('Success Rates by Method')\n",
        "        plt.ylabel('Success Rate')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'success_rates.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def _generate_summary_statistics(self, results_df: pd.DataFrame, save_dir: str):\n",
        "        \"\"\"Generate overall summary statistics\"\"\"\n",
        "        summary = results_df.groupby(['function', 'dimension', 'method']).agg({\n",
        "            'final_value': ['mean', 'std', 'min', 'max'],\n",
        "            'runtime': ['mean', 'std'],\n",
        "            'iterations': ['mean', 'std'],\n",
        "            'success': 'mean',\n",
        "            'distance_to_minimum': ['mean', 'std'],\n",
        "            'f_error': ['mean', 'std']\n",
        "        }).reset_index()\n",
        "\n",
        "        summary.to_csv(os.path.join(save_dir, \"experiment_summary.csv\"))"
      ],
      "metadata": {
        "id": "nd91nH3NiT5D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    test_functions = {\n",
        "        'ackley': (\n",
        "            TestFunctions.ackley,\n",
        "            TestFunctions.ackley_gradient,\n",
        "            TestFunctions.ackley_hessian\n",
        "        ),\n",
        "        'rastrigin': (\n",
        "            TestFunctions.rastrigin,\n",
        "            TestFunctions.rastrigin_gradient,\n",
        "            TestFunctions.rastrigin_hessian\n",
        "        ),\n",
        "        'sphere': (\n",
        "            TestFunctions.sphere,\n",
        "            TestFunctions.sphere_gradient,\n",
        "            TestFunctions.sphere_hessian\n",
        "        ),\n",
        "        'rosenbrock': (\n",
        "            TestFunctions.rosenbrock,\n",
        "            TestFunctions.rosenbrock_gradient,\n",
        "            TestFunctions.rosenbrock_hessian\n",
        "        )\n",
        "    }\n",
        "\n",
        "    methods = ['BFGS', 'newton-cg', 'trust-exact', 'trust-krylov']\n",
        "    dimensions = [2, 4, 8, 12]\n",
        "\n",
        "    # Create experiment manager\n",
        "    experiment = ExperimentManager(\n",
        "        test_functions=test_functions,\n",
        "        methods=methods,\n",
        "        dimensions=dimensions,\n",
        "        n_experiments=50,\n",
        "        min_dist=100,\n",
        "        max_dist=1000\n",
        "    )\n",
        "\n",
        "    # Run experiments\n",
        "    experiment.run_experiments()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqoSGTWxadj1",
        "outputId": "caf0de13-ad8c-4c4d-8c2a-5c283060c1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing ackley function:\n",
            "\n",
            "Dimension: 2\n",
            "\n",
            "Experiment 1/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 2/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 3/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 4/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 5/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 6/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 7/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 8/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 9/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 10/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 11/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 12/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 13/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 14/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 15/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 16/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 17/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 18/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 19/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 20/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 21/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 22/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 23/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n",
            "\n",
            "Experiment 24/50\n",
            "Running BFGS...\n",
            "Running newton-cg...\n",
            "Running trust-exact...\n",
            "Running trust-krylov...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HGcBPbFubQFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}