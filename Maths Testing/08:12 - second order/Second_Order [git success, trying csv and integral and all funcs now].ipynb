{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INITIAL TEST"
      ],
      "metadata": {
        "id": "eiRTKba4bQw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn\n",
        "!pip install gitpython PyGithub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KkKYrlBwFZs",
        "outputId": "9f59b32a-1431-4d20-c7f8-e90c8b12b6d9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.11)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.2.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.2.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.8.30)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jwVvDUwtaQgE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, List, Dict, Optional, Tuple\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class OptimizationResult:\n",
        "    \"\"\"Enhanced optimization result storage\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        self.x_final = kwargs.get('x_final')\n",
        "        self.f_final = kwargs.get('f_final')\n",
        "        self.success = kwargs.get('success')\n",
        "        self.iterations = kwargs.get('iterations')\n",
        "        self.runtime = kwargs.get('runtime')\n",
        "        self.path = kwargs.get('path', [])\n",
        "        self.f_path = kwargs.get('f_path', [])\n",
        "        self.grad_norm_path = kwargs.get('grad_norm_path', [])\n",
        "        self.timestamps = kwargs.get('timestamps', [])\n",
        "        self.memory_usage = kwargs.get('memory_usage', [])\n",
        "        self.flops_per_step = kwargs.get('flops_per_step', [])\n",
        "        self.method = kwargs.get('method')\n",
        "        self.dimension = kwargs.get('dimension')\n",
        "        self.function_name = kwargs.get('function_name')\n",
        "        self.x_initial = kwargs.get('x_initial')\n",
        "        self.f_initial = kwargs.get('f_initial')\n",
        "        self.grad_initial = kwargs.get('grad_initial')\n",
        "        self.grad_final = kwargs.get('grad_final')\n",
        "\n",
        "        # Calculate distance from global minimum\n",
        "        x_min, f_min = TestFunctions.get_global_minimum(self.function_name, self.dimension)\n",
        "        if x_min is not None and f_min is not None:\n",
        "            self.distance_to_minimum = np.linalg.norm(self.x_final - x_min)\n",
        "            self.f_error = abs(self.f_final - f_min)\n",
        "        else:\n",
        "            self.distance_to_minimum = None\n",
        "            self.f_error = None\n",
        "\n",
        "\n",
        "class FLOPCounter:\n",
        "    \"\"\"Tracks floating point operations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.flops = 0\n",
        "        self.operation_counts = {\n",
        "            'add': 0,\n",
        "            'multiply': 0,\n",
        "            'divide': 0,\n",
        "            'sqrt': 0,\n",
        "            'exp': 0,\n",
        "            'log': 0,\n",
        "            'trig': 0\n",
        "        }\n",
        "\n",
        "    def add_flops(self, operation: str, count: int = 1):\n",
        "        self.operation_counts[operation] += count\n",
        "        # Update total FLOPS based on operation weight\n",
        "        weights = {\n",
        "            'add': 1,\n",
        "            'multiply': 1,\n",
        "            'divide': 4,\n",
        "            'sqrt': 8,\n",
        "            'exp': 10,\n",
        "            'log': 10,\n",
        "            'trig': 15\n",
        "        }\n",
        "        self.flops += weights[operation] * count\n",
        "\n",
        "    def get_summary(self) -> dict:\n",
        "        return {\n",
        "            'total_flops': self.flops,\n",
        "            'operations': self.operation_counts\n",
        "        }\n",
        "\n",
        "class TestFunctions:\n",
        "    \"\"\"Test functions that work with any dimension\"\"\"\n",
        "    @staticmethod\n",
        "    def get_global_minimum(func_name: str, dimension: int = 2) -> tuple:\n",
        "        \"\"\"Get global minimum for a given function and dimension\"\"\"\n",
        "        global_minima = {\n",
        "            'ackley': (np.zeros(dimension), 0.0),\n",
        "            'rastrigin': (np.zeros(dimension), 0.0),\n",
        "            'rosenbrock': (np.ones(dimension), 0.0),\n",
        "            'sphere': (np.zeros(dimension), 0.0),\n",
        "            'michalewicz': (None, None),  # Varies with dimension\n",
        "        }\n",
        "        return global_minima.get(func_name, (None, None))\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley(x: np.ndarray) -> float:\n",
        "        \"\"\"Ackley function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "        return (-20 * np.exp(-0.2 * np.sqrt(sum_sq / n))\n",
        "                - np.exp(sum_cos / n)\n",
        "                + 20 + np.e)\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Ackley function\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "\n",
        "        term1 = (20 * 0.2 / np.sqrt(n * sum_sq)) * np.exp(-0.2 * np.sqrt(sum_sq / n)) * x\n",
        "        term2 = (2 * np.pi / n) * np.exp(sum_cos / n) * np.sin(2 * np.pi * x)\n",
        "        return term1 + term2\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Ackley Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.ackley_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin(x: np.ndarray) -> float:\n",
        "        \"\"\"Rastrigin function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        return 10 * n + np.sum(x**2 - 10 * np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rastrigin function\"\"\"\n",
        "        return 2 * x + 20 * np.pi * np.sin(2 * np.pi * x)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Rastrigin function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n) + 40 * np.pi**2 * np.diag(np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere(x: np.ndarray) -> float:\n",
        "        \"\"\"Sphere function for n dimensions\"\"\"\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Sphere function\"\"\"\n",
        "        return 2 * x\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Sphere function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock(x: np.ndarray) -> float:\n",
        "        \"\"\"Rosenbrock function for n dimensions\"\"\"\n",
        "        return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rosenbrock function\"\"\"\n",
        "        n = len(x)\n",
        "        grad = np.zeros(n)\n",
        "        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "        grad[-1] = 200 * (x[-1] - x[-2]**2)\n",
        "        if n > 2:\n",
        "            grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
        "        return grad\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Rosenbrock Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.rosenbrock_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "class OptimizationLogger:\n",
        "    \"\"\"Handles logging of optimization progress\"\"\"\n",
        "    def __init__(self, method: str, function_name: str, dimension: int):\n",
        "        self.method = method\n",
        "        self.function_name = function_name\n",
        "        self.dimension = dimension\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.path = []\n",
        "        self.f_path = []\n",
        "        self.grad_norm_path = []\n",
        "        self.step_sizes = []\n",
        "        self.memory_usage = []\n",
        "        self.timestamps = []\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log_iteration(self, x: np.ndarray, f: float, grad_norm: float, step_size: float):\n",
        "        self.path.append(x.copy())\n",
        "        self.f_path.append(f)\n",
        "        self.grad_norm_path.append(grad_norm)\n",
        "        self.step_sizes.append(step_size)\n",
        "        self.memory_usage.append(self.get_memory_usage())\n",
        "        self.timestamps.append(time.time() - self.start_time)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_memory_usage() -> float:\n",
        "        \"\"\"Get current memory usage in MB\"\"\"\n",
        "        import psutil\n",
        "        process = psutil.Process()\n",
        "        return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    def save_logs(self, base_dir: str):\n",
        "        \"\"\"Save optimization logs to CSV\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        log_dir = os.path.join(base_dir, self.function_name, str(self.dimension) + \"D\", self.method)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        log_data = {\n",
        "            'iteration': range(len(self.path)),\n",
        "            'function_value': self.f_path,\n",
        "            'gradient_norm': self.grad_norm_path,\n",
        "            'step_size': self.step_sizes,\n",
        "            'memory_mb': self.memory_usage,\n",
        "            'runtime_seconds': self.timestamps\n",
        "        }\n",
        "\n",
        "        # Add parameter values\n",
        "        for i in range(self.dimension):\n",
        "            log_data[f'x{i+1}'] = [p[i] for p in self.path]\n",
        "\n",
        "        df = pd.DataFrame(log_data)\n",
        "        df.to_csv(os.path.join(log_dir, f'optimization_log_{timestamp}.csv'), index=False)\n",
        "\n",
        "class Visualizer:\n",
        "    \"\"\"Enhanced visualization capabilities\"\"\"\n",
        "    @staticmethod\n",
        "    def plot_optimization_summary(results: Dict[str, OptimizationResult], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot summary comparing initial and final states\"\"\"\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        methods = list(results.keys())\n",
        "        x = np.arange(len(methods))\n",
        "\n",
        "        # Fix the ticks warning by setting them explicitly\n",
        "        for ax in [ax1, ax2, ax3, ax4]:\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(methods, rotation=45)\n",
        "\n",
        "        # Function Values Plot\n",
        "        initial_values = [result.f_initial for result in results.values()]\n",
        "        final_values = [result.f_final for result in results.values()]\n",
        "        width = 0.35\n",
        "\n",
        "        ax1.bar(x - width/2, initial_values, width, label='Initial', color='lightcoral')\n",
        "        ax1.bar(x + width/2, final_values, width, label='Final', color='lightgreen')\n",
        "        ax1.set_ylabel('Function Value')\n",
        "        ax1.set_title('Initial vs Final Function Values')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Add global minimum line if available\n",
        "        _, f_min = TestFunctions.get_global_minimum(function_name, results[methods[0]].dimension)\n",
        "        if f_min is not None:\n",
        "            ax1.axhline(y=f_min, color='r', linestyle='--', label=f'Global Min ({f_min})')\n",
        "            ax1.legend()\n",
        "\n",
        "        # Gradient Norms Plot\n",
        "        initial_grads = [np.linalg.norm(result.grad_initial) for result in results.values()]\n",
        "        final_grads = [np.linalg.norm(result.grad_final) for result in results.values()]\n",
        "\n",
        "        ax2.bar(x - width/2, initial_grads, width, label='Initial', color='lightcoral')\n",
        "        ax2.bar(x + width/2, final_grads, width, label='Final', color='lightgreen')\n",
        "        ax2.set_ylabel('Gradient Norm')\n",
        "        ax2.set_title('Initial vs Final Gradient Norms')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # Runtime Comparison\n",
        "        runtimes = [result.runtime for result in results.values()]\n",
        "        ax3.bar(methods, runtimes, color='skyblue')\n",
        "        ax3.set_ylabel('Runtime (seconds)')\n",
        "        ax3.set_title('Total Runtime by Method')\n",
        "        ax3.grid(True)\n",
        "\n",
        "        # Iterations Comparison\n",
        "        iterations = [result.iterations for result in results.values()]\n",
        "        ax4.bar(methods, iterations, color='lightgreen')\n",
        "        ax4.set_ylabel('Number of Iterations')\n",
        "        ax4.set_title('Total Iterations by Method')\n",
        "        ax4.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, f'optimization_summary_{function_name}.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_convergence(results: Dict[str, OptimizationResult], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot convergence with enhanced information\"\"\"\n",
        "        if not results:  # Skip if no results\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        plotted_something = False\n",
        "\n",
        "        # Get global minimum if available\n",
        "        _, f_min = TestFunctions.get_global_minimum(function_name)\n",
        "        f_min_text = f\"(Global min: {f_min})\" if f_min is not None else \"\"\n",
        "\n",
        "        # Function value convergence\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'f_path') and result.f_path:  # Check if result and f_path exist\n",
        "                ax1.semilogy(result.f_path, label=f\"{method}\")\n",
        "                plotted_something = True\n",
        "\n",
        "        if plotted_something:\n",
        "            ax1.set_xlabel('Iteration')\n",
        "            ax1.set_ylabel('Function Value (log scale)')\n",
        "            ax1.set_title(f'Function Value Convergence {f_min_text}')\n",
        "            ax1.legend()\n",
        "            ax1.grid(True)\n",
        "\n",
        "        plotted_something = False\n",
        "        # Gradient norm convergence\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'grad_norm_path') and result.grad_norm_path:\n",
        "                ax2.semilogy(result.grad_norm_path, label=f\"{method}\")\n",
        "                plotted_something = True\n",
        "\n",
        "        if plotted_something:\n",
        "            ax2.set_xlabel('Iteration')\n",
        "            ax2.set_ylabel('Gradient Norm (log scale)')\n",
        "            ax2.set_title('Gradient Norm Convergence')\n",
        "            ax2.legend()\n",
        "            ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, f'convergence_{function_name}.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_2d_trajectory(f: Callable, result: OptimizationResult, save_dir: str,\n",
        "                          experiment_num: int = None, equal_aspect: bool = True):\n",
        "        \"\"\"Plot optimization trajectory for 2D problems with two variants - with and without global minimum\"\"\"\n",
        "        if result.dimension != 2:\n",
        "            return\n",
        "\n",
        "        # Even more reduced resolution for better performance\n",
        "        grid_size = 50  # Reduced from 100 to 50\n",
        "\n",
        "        for show_global_min in [True, False]:  # Create both variants\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            # Get path bounds and include (0,0)\n",
        "            path = np.array(result.path)\n",
        "            x_min_traj = min(float(path[:, 0].min()), 0)\n",
        "            x_max_traj = max(float(path[:, 0].max()), 0)\n",
        "            y_min_traj = min(float(path[:, 1].min()), 0)\n",
        "            y_max_traj = max(float(path[:, 1].max()), 0)\n",
        "\n",
        "            # Add margin to bounds\n",
        "            margin = 0.1\n",
        "            x_range = x_max_traj - x_min_traj\n",
        "            y_range = y_max_traj - y_min_traj\n",
        "\n",
        "            if equal_aspect:\n",
        "                # Make ranges equal by expanding the smaller one\n",
        "                max_range = max(x_range, y_range)\n",
        "                x_center = (x_max_traj + x_min_traj) / 2\n",
        "                y_center = (y_max_traj + y_min_traj) / 2\n",
        "                x_min_traj = x_center - max_range/2\n",
        "                x_max_traj = x_center + max_range/2\n",
        "                y_min_traj = y_center - max_range/2\n",
        "                y_max_traj = y_center + max_range/2\n",
        "                x_range = y_range = max_range\n",
        "\n",
        "            plot_x_min = x_min_traj - margin * x_range\n",
        "            plot_x_max = x_max_traj + margin * x_range\n",
        "            plot_y_min = y_min_traj - margin * y_range\n",
        "            plot_y_max = y_max_traj + margin * y_range\n",
        "\n",
        "            # Create contour plot with reduced resolution\n",
        "            x = np.linspace(plot_x_min, plot_x_max, grid_size)\n",
        "            y = np.linspace(plot_y_min, plot_y_max, grid_size)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "\n",
        "            # Vectorized computation of Z values\n",
        "            points = np.column_stack((X.ravel(), Y.ravel()))\n",
        "            Z = np.array([f(point) for point in points]).reshape(X.shape)\n",
        "\n",
        "            # Reduced number of contour levels\n",
        "            global_max = float(Z.max())\n",
        "            global_levels = np.linspace(0, global_max, 15)  # Further reduced from 20 to 15\n",
        "\n",
        "            # Plot contours with reduced detail\n",
        "            contour = plt.contour(X, Y, Z, levels=global_levels, cmap='viridis', alpha=0.7)\n",
        "            plt.colorbar(contour, label='Function Value')\n",
        "\n",
        "            # Plot trajectory\n",
        "            plt.plot(path[:, 0], path[:, 1], 'r.-', label='Optimization Path',\n",
        "                    linewidth=1, markersize=2, zorder=5)\n",
        "            plt.plot(path[0, 0], path[0, 1], 'go', label='Start',\n",
        "                    markersize=8, zorder=6)\n",
        "            plt.plot(path[-1, 0], path[-1, 1], 'ro', label='End',\n",
        "                    markersize=8, zorder=6)\n",
        "\n",
        "            # Only plot global minimum in the first variant\n",
        "            if show_global_min:\n",
        "                x_min, f_min = TestFunctions.get_global_minimum(result.function_name)\n",
        "                if x_min is not None:\n",
        "                    plt.plot(x_min[0], x_min[1], 'k*', label='Global Minimum',\n",
        "                            markersize=10, zorder=6)\n",
        "\n",
        "            plt.xlim(float(plot_x_min), float(plot_x_max))\n",
        "            plt.ylim(float(plot_y_min), float(plot_y_max))\n",
        "\n",
        "            if equal_aspect:\n",
        "                plt.gca().set_aspect('equal')\n",
        "\n",
        "            plt.grid(True)\n",
        "\n",
        "            plt.title(f'{result.function_name} - {result.method}\\n'\n",
        "                    f'Final value: {result.f_final:.6f}\\n'\n",
        "                    f'Iterations: {result.iterations}')\n",
        "            plt.xlabel('x₁')\n",
        "            plt.ylabel('x₂')\n",
        "            plt.legend()\n",
        "\n",
        "            # Include experiment number and variant in filename\n",
        "            experiment_suffix = f'_exp{experiment_num}' if experiment_num is not None else ''\n",
        "            variant_suffix = '_with_global_min' if show_global_min else '_path_only'\n",
        "            aspect_suffix = '_equal_aspect' if equal_aspect else ''\n",
        "            filename = f'trajectory_{result.function_name}_{result.method}{experiment_suffix}{variant_suffix}{aspect_suffix}.png'\n",
        "\n",
        "            # Save with reduced DPI\n",
        "            plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_computational_metrics(results: Dict[str, OptimizationResult], save_dir: str):\n",
        "        \"\"\"Plot computational metrics over time\"\"\"\n",
        "        if not results:  # Skip if no results\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        plotted_memory = False\n",
        "        plotted_flops = False\n",
        "\n",
        "        # Memory usage over time\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'timestamps') and hasattr(result, 'memory_usage'):\n",
        "                if result.timestamps and result.memory_usage:  # Check if data exists\n",
        "                    ax1.plot(result.timestamps, result.memory_usage, label=method)\n",
        "                    plotted_memory = True\n",
        "\n",
        "        ax1.set_xlabel('Time (seconds)')\n",
        "        ax1.set_ylabel('Memory Usage (MB)')\n",
        "        ax1.set_title('Memory Usage Over Time')\n",
        "        if plotted_memory:  # Only create legend if we plotted something\n",
        "            ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # FLOPS over time\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'timestamps') and hasattr(result, 'flops_per_step'):\n",
        "                if result.timestamps and result.flops_per_step:  # Check if data exists\n",
        "                    cumulative_flops = np.cumsum(result.flops_per_step)\n",
        "                    ax2.plot(result.timestamps, cumulative_flops, label=method)\n",
        "                    plotted_flops = True\n",
        "\n",
        "        ax2.set_xlabel('Time (seconds)')\n",
        "        ax2.set_ylabel('Cumulative FLOPS')\n",
        "        ax2.set_title('Computational Cost Over Time')\n",
        "        if plotted_flops:  # Only create legend if we plotted something\n",
        "            ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'computational_metrics.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "def get_memory_usage() -> float:\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def run_optimization(f: Callable,\n",
        "                    grad: Callable,\n",
        "                    hess: Callable,\n",
        "                    x0: np.ndarray,\n",
        "                    method: str,\n",
        "                    function_name: str) -> OptimizationResult:\n",
        "    \"\"\"Enhanced optimization runner with detailed metrics\"\"\"\n",
        "    start_time = time.time()\n",
        "    flop_counter = FLOPCounter()\n",
        "\n",
        "    # Calculate initial metrics\n",
        "    f_initial = f(x0)\n",
        "    grad_initial = grad(x0)\n",
        "\n",
        "    # Storage for metrics\n",
        "    path = [x0.copy()]  # Start with initial point\n",
        "    f_path = [f_initial]\n",
        "    grad_norm_path = [np.linalg.norm(grad_initial)]\n",
        "    timestamps = [0.0]\n",
        "    memory_usage = [get_memory_usage()]\n",
        "    flops_per_step = [0]\n",
        "\n",
        "    def callback(xk):\n",
        "        current_time = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        f_val = f(xk)\n",
        "        grad_val = grad(xk)\n",
        "        grad_norm = np.linalg.norm(grad_val)\n",
        "\n",
        "        # Store metrics\n",
        "        path.append(xk.copy())\n",
        "        f_path.append(f_val)\n",
        "        grad_norm_path.append(grad_norm)\n",
        "        timestamps.append(current_time)\n",
        "        memory_usage.append(get_memory_usage())\n",
        "        flops_per_step.append(flop_counter.flops)\n",
        "\n",
        "    try:\n",
        "        # Run optimization with method-specific settings\n",
        "        if method == 'BFGS':\n",
        "            result = minimize(f, x0, method=method, jac=grad, callback=callback)\n",
        "        elif method == 'newton-cg':\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        elif method in ['trust-exact', 'trust-krylov']:\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n",
        "        # Calculate final gradient\n",
        "        grad_final = grad(result.x)\n",
        "        runtime = time.time() - start_time\n",
        "\n",
        "        \"\"\"\n",
        "        # Print optimization results with better formatting\n",
        "        print(f\"\\n{method} Results:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"{'Initial function value:':<25} {f_initial:>12.6f}\")\n",
        "        print(f\"{'Final function value:':<25} {result.fun:>12.6f}\")\n",
        "        print(f\"{'Number of iterations:':<25} {result.nit:>12d}\")\n",
        "        print(f\"{'Runtime:':<25} {runtime:>12.4f} seconds\")\n",
        "        print(f\"{'Initial gradient norm:':<25} {np.linalg.norm(grad_initial):>12.6f}\")\n",
        "        print(f\"{'Final gradient norm:':<25} {np.linalg.norm(grad_final):>12.6f}\")\n",
        "        print(\"-\" * 50)\n",
        "        print()\n",
        "        \"\"\"\n",
        "\n",
        "        return OptimizationResult(\n",
        "            x_final=result.x,\n",
        "            f_final=result.fun,\n",
        "            success=result.success,\n",
        "            iterations=result.nit,\n",
        "            runtime=runtime,\n",
        "            path=path,\n",
        "            f_path=f_path,\n",
        "            grad_norm_path=grad_norm_path,\n",
        "            timestamps=timestamps,\n",
        "            memory_usage=memory_usage,\n",
        "            flops_per_step=flops_per_step,\n",
        "            method=method,\n",
        "            dimension=len(x0),\n",
        "            function_name=function_name,\n",
        "            x_initial=x0,\n",
        "            f_initial=f_initial,\n",
        "            grad_initial=grad_initial,\n",
        "            grad_final=grad_final\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Optimization failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "    \"\"\"Basic gradient descent optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            x = x - self.learning_rate * g\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            # Add small random noise to simulate stochasticity\n",
        "            noise = np.random.normal(0, 0.01, size=x.shape)\n",
        "            g = g + noise\n",
        "            x = x - self.learning_rate * g\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }\n",
        "\n",
        "class MomentumGD:\n",
        "    \"\"\"Gradient Descent with Momentum\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            v = self.momentum * v - self.learning_rate * g\n",
        "            x = x + v\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }\n",
        "\n",
        "class RMSprop:\n",
        "    \"\"\"RMSprop optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, decay_rate=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            v = self.decay_rate * v + (1 - self.decay_rate) * g**2\n",
        "            x = x - self.learning_rate * g / (np.sqrt(v) + self.epsilon)\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }\n",
        "\n",
        "class Adam:\n",
        "    \"\"\"Adam optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        m = np.zeros_like(x)\n",
        "        v = np.zeros_like(x)\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            m = self.beta1 * m + (1 - self.beta1) * g\n",
        "            v = self.beta2 * v + (1 - self.beta2) * g**2\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = m / (1 - self.beta1**(i + 1))\n",
        "            v_hat = v / (1 - self.beta2**(i + 1))\n",
        "\n",
        "            x = x - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }"
      ],
      "metadata": {
        "id": "sGlrjFy2pnuJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentManager:\n",
        "    \"\"\"Manages multiple optimization experiments\"\"\"\n",
        "    def __init__(self,\n",
        "                 test_functions: Dict[str, Tuple[Callable, Callable, Callable]],\n",
        "                 first_order_optimizers: Dict[str, object],\n",
        "                 second_order_methods: List[str],\n",
        "                 dimensions: List[int],\n",
        "                 n_experiments: int = 50,\n",
        "                 min_dist: float = 100,\n",
        "                 max_dist: float = 1000):\n",
        "        self.test_functions = test_functions\n",
        "        self.first_order_optimizers = first_order_optimizers\n",
        "        self.second_order_methods = second_order_methods\n",
        "        self.dimensions = dimensions\n",
        "        self.n_experiments = n_experiments\n",
        "        self.min_dist = min_dist\n",
        "        self.max_dist = max_dist\n",
        "\n",
        "    def generate_starting_points(self, dimension: int, seed: int = None) -> np.ndarray:\n",
        "        \"\"\"Generate random starting points with specified distance from origin\"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        starting_points = []\n",
        "\n",
        "        for _ in range(self.n_experiments):\n",
        "            direction = np.random.randn(dimension)\n",
        "            direction = direction / np.linalg.norm(direction)\n",
        "            distance = np.random.uniform(self.min_dist, self.max_dist)\n",
        "            point = direction * distance\n",
        "            starting_points.append(point)\n",
        "\n",
        "        return np.array(starting_points)\n",
        "\n",
        "    def run_experiments(self, base_dir: str = \"optimization_results\"):\n",
        "        \"\"\"Run all experiments with proper directory structure\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        experiment_dir = os.path.join(base_dir, f\"experiment_{timestamp}\")\n",
        "        all_results = []\n",
        "\n",
        "        for func_name, (f, grad, hess) in self.test_functions.items():\n",
        "            print(f\"\\nTesting {func_name} function:\")\n",
        "\n",
        "            for dim in self.dimensions:\n",
        "                print(f\"\\nDimension: {dim}\")\n",
        "\n",
        "                # Generate starting points for this dimension\n",
        "                starting_points = self.generate_starting_points(dim)\n",
        "\n",
        "                # Create directory structure\n",
        "                func_dir = os.path.join(experiment_dir, func_name, f\"{dim}D\")\n",
        "                first_order_dir = os.path.join(func_dir, \"first_order\")\n",
        "                second_order_dir = os.path.join(func_dir, \"second_order\")\n",
        "\n",
        "                for directory in [first_order_dir, second_order_dir]:\n",
        "                    os.makedirs(os.path.join(directory, \"results\"), exist_ok=True)\n",
        "                    os.makedirs(os.path.join(directory, \"trajectories\"), exist_ok=True)\n",
        "\n",
        "                # Run experiments for each starting point\n",
        "                for i, x0 in enumerate(starting_points):\n",
        "                    print(f\"\\nExperiment {i+1}/{self.n_experiments}\")\n",
        "\n",
        "                    # First order methods\n",
        "                    first_order_results = {}\n",
        "                    print(\"\\nFirst-order methods:\")\n",
        "                    print(\"-\" * 50)\n",
        "                    for name, optimizer in self.first_order_optimizers.items():\n",
        "                        print(f\"Running {name}...\")\n",
        "                        try:\n",
        "                            # Create logger for this optimization run\n",
        "                            logger = OptimizationLogger(name, func_name, dim)\n",
        "\n",
        "                            # Create wrapper function for callback\n",
        "                            def callback(xk):\n",
        "                                f_val = f(xk)\n",
        "                                grad_val = grad(xk)\n",
        "                                grad_norm = np.linalg.norm(grad_val)\n",
        "                                step_size = 0.0  # We don't track step size for these methods\n",
        "                                logger.log_iteration(xk, f_val, grad_norm, step_size)\n",
        "\n",
        "                            start_time = time.time()\n",
        "                            result = optimizer.optimize(f, grad, x0, callback=callback)\n",
        "                            runtime = time.time() - start_time\n",
        "\n",
        "                            print(f\"  Runtime: {runtime:.3f} seconds\")\n",
        "                            print(f\"  Iterations: {result['nit']}\")\n",
        "                            print(f\"  Final value: {result['fun']:.6f}\")\n",
        "                            print(f\"  Success: {result['success']}\")\n",
        "                            print(f\"  Final gradient norm: {np.linalg.norm(result['grad_final']):.6f}\")\n",
        "\n",
        "                            opt_result = OptimizationResult(\n",
        "                                x_final=result['x'],\n",
        "                                f_final=result['fun'],\n",
        "                                success=result['success'],\n",
        "                                iterations=result['nit'],\n",
        "                                runtime=time.time() - logger.start_time,\n",
        "                                path=result['path'],\n",
        "                                f_path=result['f_path'],\n",
        "                                grad_norm_path=result['grad_path'],\n",
        "                                timestamps=logger.timestamps,\n",
        "                                memory_usage=logger.memory_usage,\n",
        "                                method=name,\n",
        "                                dimension=dim,\n",
        "                                function_name=func_name,\n",
        "                                x_initial=x0,\n",
        "                                f_initial=f(x0),\n",
        "                                grad_initial=grad(x0),\n",
        "                                grad_final=result['grad_final']\n",
        "                            )\n",
        "\n",
        "                            first_order_results[name] = opt_result\n",
        "                            all_results.append(self._format_result(opt_result, \"first_order\", i))\n",
        "                            print(\"Done\")\n",
        "\n",
        "                            print(\"-\" * 20)\n",
        "                            print()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Failed: {str(e)}\")\n",
        "\n",
        "                    # Second order methods\n",
        "                    second_order_results = {}\n",
        "                    print(\"\\nSecond-order methods:\")\n",
        "                    print(\"-\" * 50)\n",
        "                    for method in self.second_order_methods:\n",
        "                        print(f\"Running {method}...\")\n",
        "                        try:\n",
        "                            result = run_optimization(f, grad, hess, x0, method, func_name)\n",
        "\n",
        "                            print(f\"  Runtime: {result.runtime:.3f} seconds\")\n",
        "                            print(f\"  Iterations: {result.iterations}\")\n",
        "                            print(f\"  Final value: {result.f_final:.6f}\")\n",
        "                            print(f\"  Success: {result.success}\")\n",
        "                            print(f\"  Final gradient norm: {np.linalg.norm(result.grad_final):.6f}\")\n",
        "\n",
        "                            if result is not None:\n",
        "                                second_order_results[method] = result\n",
        "                                all_results.append(self._format_result(result, \"second_order\", i))\n",
        "                                print(\"Done\")\n",
        "                            else:\n",
        "                                print(\"Failed\")\n",
        "\n",
        "                            print(\"-\" * 20)\n",
        "                            print()\n",
        "                        except Exception as e:\n",
        "                            print(f\"Failed: {str(e)}\")\n",
        "\n",
        "                    # Generate plots\n",
        "                    self._generate_plots(first_order_results, first_order_dir, f, i, dim)\n",
        "                    self._generate_plots(second_order_results, second_order_dir, f, i, dim)\n",
        "\n",
        "                # Generate statistical plots for this dimension\n",
        "                for results, dir_path in [(first_order_results, first_order_dir),\n",
        "                                        (second_order_results, second_order_dir)]:\n",
        "                    if results:\n",
        "                        results_df = pd.DataFrame([r for r in all_results\n",
        "                                                if r['function'] == func_name and\n",
        "                                                r['dimension'] == dim and\n",
        "                                                r['method'] in results.keys()])\n",
        "                        generate_statistics(results_df, dir_path)\n",
        "\n",
        "        # Save complete results and generate overall statistics\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "        results_df.to_csv(os.path.join(experiment_dir, \"all_results.csv\"), index=False)\n",
        "        generate_statistics(results_df, experiment_dir)\n",
        "\n",
        "    def _generate_plots(self, results: Dict[str, OptimizationResult],\n",
        "                   base_dir: str, f: Callable, exp_num: int, dim: int):\n",
        "        \"\"\"Generate all plots for a set of results\"\"\"\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        results_dir = os.path.join(base_dir, \"results\")\n",
        "        trajectory_dir = os.path.join(base_dir, \"trajectories\")\n",
        "\n",
        "        # Only generate plots if we have results\n",
        "        if results:\n",
        "            # Get function name from first result\n",
        "            func_name = next(iter(results.values())).function_name\n",
        "\n",
        "            # Generate convergence plots\n",
        "            if any(hasattr(r, 'f_path') and r.f_path for r in results.values()):\n",
        "                Visualizer.plot_convergence(results, results_dir, f\"{func_name}_exp{exp_num}\")\n",
        "\n",
        "            # Generate computational metrics\n",
        "            if any(hasattr(r, 'timestamps') and r.timestamps for r in results.values()):\n",
        "                Visualizer.plot_computational_metrics(results, results_dir)\n",
        "\n",
        "            # Generate optimization summary\n",
        "            if results:\n",
        "                Visualizer.plot_optimization_summary(results, results_dir, f\"{func_name}_exp{exp_num}\")\n",
        "\n",
        "            # Generate 2D trajectories if applicable\n",
        "            if dim == 2:\n",
        "                for method, result in results.items():\n",
        "                    if result is not None:\n",
        "                        try:\n",
        "                            Visualizer.plot_2d_trajectory(f, result, trajectory_dir, exp_num)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error plotting trajectory for {method}: {e}\")\n",
        "\n",
        "    def _format_result(self, result: OptimizationResult, opt_type: str, exp_num: int) -> dict:\n",
        "        \"\"\"Format OptimizationResult for DataFrame\"\"\"\n",
        "        return {\n",
        "            'function': result.function_name,\n",
        "            'dimension': result.dimension,\n",
        "            'experiment': exp_num,\n",
        "            'method': result.method,\n",
        "            'optimizer_type': opt_type,\n",
        "            'start_distance': np.linalg.norm(result.x_initial),\n",
        "            'final_value': result.f_final,\n",
        "            'iterations': result.iterations,\n",
        "            'runtime': result.runtime,\n",
        "            'success': result.success,\n",
        "            'distance_to_minimum': result.distance_to_minimum,\n",
        "            'f_error': result.f_error,\n",
        "            'initial_gradient_norm': np.linalg.norm(result.grad_initial),\n",
        "            'final_gradient_norm': np.linalg.norm(result.grad_final)\n",
        "        }\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "def generate_statistics(results_df: pd.DataFrame, save_dir: str):\n",
        "    \"\"\"Generate comprehensive statistical visualizations for optimization results\"\"\"\n",
        "    # Set style and color palette\n",
        "    plt.style.use('default')\n",
        "    colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC', '#99CCFF']\n",
        "\n",
        "    if len(results_df) == 0:\n",
        "        return\n",
        "\n",
        "    # 1. Distribution of final values (Violin Plot)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        sns.violinplot(data=results_df, x='method', y='final_value', color=colors[0])\n",
        "    plt.title('Distribution of Final Values by Method')\n",
        "    plt.xlabel('Optimization Method')\n",
        "    plt.ylabel('Final Function Value')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'final_values_distribution.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Success rates with confidence intervals\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    success_data = results_df.groupby('method')['success'].agg(['mean', 'count'])\n",
        "    success_data['ci'] = 1.96 * np.sqrt(success_data['mean'] * (1 - success_data['mean']) / success_data['count'])\n",
        "\n",
        "    # Use bar plot instead of success_data['mean'].plot\n",
        "    plt.bar(range(len(success_data)), success_data['mean'], yerr=success_data['ci'], capsize=5, color=colors[1])\n",
        "    plt.xticks(range(len(success_data)), success_data.index, rotation=45)\n",
        "    plt.title('Success Rates by Method with 95% Confidence Intervals')\n",
        "    plt.ylabel('Success Rate')\n",
        "    plt.xlabel('Method')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'success_rates.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Runtime comparison (Box Plot)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        sns.boxplot(data=results_df, x='method', y='runtime', color=colors[2])\n",
        "    plt.title('Runtime Distribution by Method')\n",
        "    plt.xlabel('Method')\n",
        "    plt.ylabel('Runtime (seconds)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'runtime_distribution.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Convergence efficiency (Iterations vs Final Value)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    methods = results_df['method'].unique()\n",
        "    for i, method in enumerate(methods):\n",
        "        mask = results_df['method'] == method\n",
        "        plt.scatter(results_df[mask]['iterations'],\n",
        "                   results_df[mask]['final_value'],\n",
        "                   label=method,\n",
        "                   color=colors[i % len(colors)],\n",
        "                   alpha=0.7)\n",
        "    plt.title('Convergence Efficiency')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Final Function Value')\n",
        "    plt.yscale('log')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'convergence_efficiency.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Distance to minimum vs Runtime\n",
        "    if 'distance_to_minimum' in results_df.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for i, method in enumerate(methods):\n",
        "            mask = results_df['method'] == method\n",
        "            plt.scatter(results_df[mask]['runtime'],\n",
        "                       results_df[mask]['distance_to_minimum'],\n",
        "                       label=method,\n",
        "                       color=colors[i % len(colors)],\n",
        "                       alpha=0.7)\n",
        "        plt.title('Distance to Minimum vs Runtime')\n",
        "        plt.xlabel('Runtime (seconds)')\n",
        "        plt.ylabel('Distance to Global Minimum')\n",
        "        plt.yscale('log')\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'distance_vs_runtime.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # Generate summary statistics table\n",
        "    summary_stats = results_df.groupby('method').agg({\n",
        "        'final_value': ['mean', 'std', 'min', 'max'],\n",
        "        'runtime': ['mean', 'std'],\n",
        "        'iterations': ['mean', 'std'],\n",
        "        'success': 'mean',\n",
        "        'distance_to_minimum': ['mean', 'std'] if 'distance_to_minimum' in results_df.columns else None\n",
        "    }).round(4)\n",
        "\n",
        "    # Save summary statistics\n",
        "    summary_stats.to_csv(os.path.join(save_dir, 'summary_statistics.csv'))\n",
        "\n",
        "    return summary_stats"
      ],
      "metadata": {
        "id": "nd91nH3NiT5D"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Github and main() function"
      ],
      "metadata": {
        "id": "fzdasrggF19C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from github import Github\n",
        "import git\n",
        "import json\n",
        "from pathlib import Path\n",
        "import time\n",
        "import random\n",
        "\n",
        "class AuthManager:\n",
        "    def __init__(self):\n",
        "        self.config_file = Path.home() / '.optimization_config'\n",
        "        self.credentials = self.load_credentials()\n",
        "\n",
        "    def load_credentials(self):\n",
        "        if self.config_file.exists():\n",
        "            try:\n",
        "                with open(self.config_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def save_credentials(self, credentials):\n",
        "        with open(self.config_file, 'w') as f:\n",
        "            json.dump(credentials, f)\n",
        "\n",
        "    def authenticate(self):\n",
        "        if self.credentials:\n",
        "            print(\"Found existing credentials. Would you like to use them? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                return self.credentials\n",
        "\n",
        "        print(\"\\n=== GitHub Authentication ===\")\n",
        "        print(\"Please provide your GitHub credentials to continue.\")\n",
        "        print(\"Note: Your personal access token needs repo and workflow permissions.\")\n",
        "        print(\"\\nDon't have a token? Create one at: https://github.com/settings/tokens\")\n",
        "\n",
        "        github_username = input(\"\\nGitHub Username: \")\n",
        "        github_token = getpass.getpass(\"Personal Access Token: \")\n",
        "        repo_name = input(\"Repository Name (format: username/repo): \")\n",
        "\n",
        "        # Verify credentials\n",
        "        try:\n",
        "            g = Github(github_token)\n",
        "            user = g.get_user()\n",
        "            _ = user.get_repo(repo_name.split('/')[1])\n",
        "\n",
        "            credentials = {\n",
        "                'username': github_username,\n",
        "                'token': github_token,\n",
        "                'repo': repo_name\n",
        "            }\n",
        "\n",
        "            print(\"\\nAuthentication successful!\")\n",
        "\n",
        "            # Ask to save credentials\n",
        "            print(\"Would you like to save these credentials for future use? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                self.save_credentials(credentials)\n",
        "                print(\"Credentials saved!\")\n",
        "\n",
        "            return credentials\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAuthentication failed: {str(e)}\")\n",
        "            print(\"Please try again.\")\n",
        "            return self.authenticate()\n",
        "\n",
        "class GitManager:\n",
        "    def __init__(self, credentials):\n",
        "        self.credentials = credentials\n",
        "        # Format the URL with the token in a different way\n",
        "        self.token = credentials['token']\n",
        "        self.repo_url = f\"https://x-access-token:{self.token}@github.com/{credentials['repo']}.git\"\n",
        "        self.instance_id = f\"optimization_{random.randint(1000, 9999)}\"\n",
        "        self.repo = None\n",
        "\n",
        "    def setup_repo(self, local_path):\n",
        "        \"\"\"Initialize or clone the repository\"\"\"\n",
        "        try:\n",
        "            print(f\"Creating directory: {local_path}\")\n",
        "            Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Configure git globally\n",
        "            with git.Git().custom_environment(GIT_SSL_NO_VERIFY='true'):\n",
        "                try:\n",
        "                    print(\"Checking if directory is a git repository...\")\n",
        "                    self.repo = git.Repo(local_path)\n",
        "                    print(\"Existing repository found\")\n",
        "\n",
        "                    # Update remote URL with credentials\n",
        "                    origin = self.repo.remote('origin')\n",
        "                    origin.set_url(self.repo_url)\n",
        "                    print(\"Remote URL updated\")\n",
        "\n",
        "                except git.exc.InvalidGitRepositoryError:\n",
        "                    print(\"Initializing new git repository...\")\n",
        "                    self.repo = git.Repo.init(local_path)\n",
        "                    print(\"Repository initialized\")\n",
        "\n",
        "                    print(\"Adding remote origin...\")\n",
        "                    origin = self.repo.create_remote('origin', self.repo_url)\n",
        "                    print(\"Remote added\")\n",
        "\n",
        "                    # Configure git credentials\n",
        "                    config_writer = self.repo.config_writer()\n",
        "                    config_writer.set_value(\"http\", \"sslVerify\", \"false\")\n",
        "                    config_writer.release()\n",
        "\n",
        "                    print(\"Fetching from remote...\")\n",
        "                    origin.fetch()\n",
        "                    print(\"Fetch completed\")\n",
        "\n",
        "                    print(\"Setting up main branch...\")\n",
        "                    if 'main' not in self.repo.refs:\n",
        "                        self.repo.create_head('main', origin.refs.main)\n",
        "                    self.repo.heads.main.set_tracking_branch(origin.refs.main)\n",
        "                    self.repo.heads.main.checkout()\n",
        "                    print(\"Main branch setup completed\")\n",
        "\n",
        "                    print(\"Pulling latest changes...\")\n",
        "                    origin.pull('main')\n",
        "                    print(\"Pull completed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up repository: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def push_results(self, local_path):\n",
        "        \"\"\"Push results to GitHub with conflict resolution\"\"\"\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "\n",
        "        while retry_count < max_retries:\n",
        "            try:\n",
        "                print(f\"\\nAttempting to push results (attempt {retry_count + 1}/{max_retries})...\")\n",
        "\n",
        "                # Configure git environment for this operation\n",
        "                env = {\n",
        "                    'GIT_SSL_NO_VERIFY': 'true',\n",
        "                    'GIT_TERMINAL_PROMPT': '0',\n",
        "                    'GIT_USERNAME': 'x-access-token',\n",
        "                    'GIT_PASSWORD': self.token\n",
        "                }\n",
        "\n",
        "                with self.repo.git.custom_environment(**env):\n",
        "                    print(\"Creating new branch...\")\n",
        "                    current = self.repo.create_head(self.instance_id)\n",
        "                    current.checkout()\n",
        "                    print(f\"Created and checked out branch: {self.instance_id}\")\n",
        "\n",
        "                    print(\"Pulling latest changes from main...\")\n",
        "                    self.repo.remotes.origin.pull('main')\n",
        "                    print(\"Pull completed\")\n",
        "\n",
        "                    print(\"Adding new files...\")\n",
        "                    self.repo.index.add('*')\n",
        "                    print(\"Files added\")\n",
        "\n",
        "                    print(\"Committing changes...\")\n",
        "                    self.repo.index.commit(f\"Results update from {self.instance_id}\")\n",
        "                    print(\"Changes committed\")\n",
        "\n",
        "                    print(\"Pushing to remote...\")\n",
        "                    push_info = self.repo.remotes.origin.push(self.instance_id)\n",
        "                    print(\"Push completed\")\n",
        "\n",
        "                    print(\"Creating pull request...\")\n",
        "                    self.create_pull_request()\n",
        "\n",
        "                    print(f\"\\nResults successfully pushed to branch: {self.instance_id}\")\n",
        "                    print(\"Pull request created for review.\")\n",
        "                    break\n",
        "\n",
        "            except git.exc.GitCommandError as e:\n",
        "                print(f\"Git error: {e}\")\n",
        "                retry_count += 1\n",
        "                wait_time = random.uniform(1, 5)\n",
        "                print(f\"Waiting {wait_time:.2f} seconds before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error: {e}\")\n",
        "                print(f\"Error type: {type(e)}\")\n",
        "                break\n",
        "\n",
        "    def create_pull_request(self):\n",
        "        \"\"\"Create a pull request using GitHub API\"\"\"\n",
        "        try:\n",
        "            g = Github(self.token)\n",
        "            repo = g.get_repo(self.credentials['repo'])\n",
        "\n",
        "            pr = repo.create_pull(\n",
        "                title=f\"Results update from {self.instance_id}\",\n",
        "                body=\"Automated results update from optimization experiment\",\n",
        "                head=self.instance_id,\n",
        "                base=\"main\"\n",
        "            )\n",
        "            print(f\"Created PR: {pr.html_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create PR: {e}\")\n",
        "\n",
        "def setup_experiment():\n",
        "    \"\"\"Initial setup and authentication\"\"\"\n",
        "    print(\"\\n=== Optimization Experiment Setup ===\")\n",
        "\n",
        "    # Authenticate\n",
        "    auth_manager = AuthManager()\n",
        "    credentials = auth_manager.authenticate()\n",
        "\n",
        "    if not credentials:\n",
        "        print(\"Authentication failed. Cannot continue.\")\n",
        "        return None\n",
        "\n",
        "    return credentials\n"
      ],
      "metadata": {
        "id": "cvb6HG67Fp6u"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Get credentials\n",
        "    credentials = setup_experiment()\n",
        "    if not credentials:\n",
        "        return\n",
        "\n",
        "    # Initialize Git manager\n",
        "    git_manager = GitManager(credentials)\n",
        "\n",
        "    # Setup local repository\n",
        "    local_path = \"optimization_results\"\n",
        "    try:\n",
        "        git_manager.setup_repo(local_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to setup repository: {e}\")\n",
        "        return\n",
        "\n",
        "    test_functions = {\n",
        "        'ackley': (\n",
        "            TestFunctions.ackley,\n",
        "            TestFunctions.ackley_gradient,\n",
        "            TestFunctions.ackley_hessian\n",
        "        )\n",
        "\n",
        "    }\n",
        "    \"\"\",\n",
        "        'rastrigin': (\n",
        "            TestFunctions.rastrigin,\n",
        "            TestFunctions.rastrigin_gradient,\n",
        "            TestFunctions.rastrigin_hessian\n",
        "        ),\n",
        "        'sphere': (\n",
        "            TestFunctions.sphere,\n",
        "            TestFunctions.sphere_gradient,\n",
        "            TestFunctions.sphere_hessian\n",
        "        ),\n",
        "        'rosenbrock': (\n",
        "            TestFunctions.rosenbrock,\n",
        "            TestFunctions.rosenbrock_gradient,\n",
        "            TestFunctions.rosenbrock_hessian\n",
        "        )\"\"\"\n",
        "\n",
        "    # First-order optimizers with their configurations\n",
        "    first_order_optimizers = {\n",
        "        'gradient_descent': GradientDescent(learning_rate=0.01),\n",
        "        'sgd': SGD(learning_rate=0.01),\n",
        "        'momentum': MomentumGD(learning_rate=0.01, momentum=0.9),\n",
        "        'rmsprop': RMSprop(learning_rate=0.01, decay_rate=0.9),\n",
        "        'adam': Adam(learning_rate=0.001)\n",
        "    }\n",
        "\n",
        "    # Second-order methods\n",
        "    # second_order_methods = ['BFGS', 'newton-cg', 'trust-exact', 'trust-krylov']\n",
        "    second_order_methods = [\"trust-exact\", \"trust-krylov\"]\n",
        "\n",
        "    dimensions = [32, 128] # [2, 4, 8, 16, 32, 64, 128]\n",
        "\n",
        "    # Create experiment manager\n",
        "    experiment = ExperimentManager(\n",
        "        test_functions=test_functions,\n",
        "        first_order_optimizers=first_order_optimizers,\n",
        "        second_order_methods=second_order_methods,\n",
        "        dimensions=dimensions,\n",
        "        n_experiments=1,\n",
        "        min_dist=5,\n",
        "        max_dist=64\n",
        "    )\n",
        "\n",
        "    # Run experiments\n",
        "    experiment.run_experiments()\n",
        "\n",
        "    # Push results to GitHub\n",
        "    print(\"\\nPushing results to GitHub...\")\n",
        "    git_manager.push_results(local_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqoSGTWxadj1",
        "outputId": "56a5cafa-64fe-4dce-830a-9856f3bfb22d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Optimization Experiment Setup ===\n",
            "Found existing credentials. Would you like to use them? (y/n)\n",
            "y\n",
            "Creating directory: optimization_results\n",
            "Checking if directory is a git repository...\n",
            "Existing repository found\n",
            "Remote URL updated\n",
            "\n",
            "Testing ackley function:\n",
            "\n",
            "Dimension: 32\n",
            "\n",
            "Experiment 1/1\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.102 seconds\n",
            "  Iterations: 553\n",
            "  Final value: 3.186968\n",
            "  Success: True\n",
            "  Final gradient norm: 0.000001\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.190 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 3.186976\n",
            "  Success: False\n",
            "  Final gradient norm: 0.007089\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.056 seconds\n",
            "  Iterations: 295\n",
            "  Final value: 3.186968\n",
            "  Success: True\n",
            "  Final gradient norm: 0.000001\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.190 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 3.188302\n",
            "  Success: False\n",
            "  Final gradient norm: 0.094410\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.202 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 3.186968\n",
            "  Success: False\n",
            "  Final gradient norm: 0.000269\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running trust-exact...\n",
            "  Runtime: 16.584 seconds\n",
            "  Iterations: 316\n",
            "  Final value: 3.186968\n",
            "  Success: True\n",
            "  Final gradient norm: 0.000098\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 16.289 seconds\n",
            "  Iterations: 309\n",
            "  Final value: 3.186968\n",
            "  Success: True\n",
            "  Final gradient norm: 0.000100\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cN47rPnUH5hp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}