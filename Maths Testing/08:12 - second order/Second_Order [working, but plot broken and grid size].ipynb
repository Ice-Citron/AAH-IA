{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INITIAL TEST"
      ],
      "metadata": {
        "id": "eiRTKba4bQw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KkKYrlBwFZs",
        "outputId": "dad10dee-950c-444b-d85b-1944a69b079f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jwVvDUwtaQgE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, List, Dict, Optional, Tuple\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class OptimizationResult:\n",
        "    \"\"\"Enhanced optimization result storage\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        self.x_final = kwargs.get('x_final')\n",
        "        self.f_final = kwargs.get('f_final')\n",
        "        self.success = kwargs.get('success')\n",
        "        self.iterations = kwargs.get('iterations')\n",
        "        self.runtime = kwargs.get('runtime')\n",
        "        self.path = kwargs.get('path', [])\n",
        "        self.f_path = kwargs.get('f_path', [])\n",
        "        self.grad_norm_path = kwargs.get('grad_norm_path', [])\n",
        "        self.timestamps = kwargs.get('timestamps', [])\n",
        "        self.memory_usage = kwargs.get('memory_usage', [])\n",
        "        self.flops_per_step = kwargs.get('flops_per_step', [])\n",
        "        self.method = kwargs.get('method')\n",
        "        self.dimension = kwargs.get('dimension')\n",
        "        self.function_name = kwargs.get('function_name')\n",
        "        self.x_initial = kwargs.get('x_initial')\n",
        "        self.f_initial = kwargs.get('f_initial')\n",
        "        self.grad_initial = kwargs.get('grad_initial')\n",
        "        self.grad_final = kwargs.get('grad_final')\n",
        "\n",
        "        # Calculate distance from global minimum\n",
        "        x_min, f_min = TestFunctions.get_global_minimum(self.function_name, self.dimension)\n",
        "        if x_min is not None and f_min is not None:\n",
        "            self.distance_to_minimum = np.linalg.norm(self.x_final - x_min)\n",
        "            self.f_error = abs(self.f_final - f_min)\n",
        "        else:\n",
        "            self.distance_to_minimum = None\n",
        "            self.f_error = None\n",
        "\n",
        "\n",
        "class FLOPCounter:\n",
        "    \"\"\"Tracks floating point operations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.flops = 0\n",
        "        self.operation_counts = {\n",
        "            'add': 0,\n",
        "            'multiply': 0,\n",
        "            'divide': 0,\n",
        "            'sqrt': 0,\n",
        "            'exp': 0,\n",
        "            'log': 0,\n",
        "            'trig': 0\n",
        "        }\n",
        "\n",
        "    def add_flops(self, operation: str, count: int = 1):\n",
        "        self.operation_counts[operation] += count\n",
        "        # Update total FLOPS based on operation weight\n",
        "        weights = {\n",
        "            'add': 1,\n",
        "            'multiply': 1,\n",
        "            'divide': 4,\n",
        "            'sqrt': 8,\n",
        "            'exp': 10,\n",
        "            'log': 10,\n",
        "            'trig': 15\n",
        "        }\n",
        "        self.flops += weights[operation] * count\n",
        "\n",
        "    def get_summary(self) -> dict:\n",
        "        return {\n",
        "            'total_flops': self.flops,\n",
        "            'operations': self.operation_counts\n",
        "        }\n",
        "\n",
        "class TestFunctions:\n",
        "    \"\"\"Test functions that work with any dimension\"\"\"\n",
        "    @staticmethod\n",
        "    def get_global_minimum(func_name: str, dimension: int = 2) -> tuple:\n",
        "        \"\"\"Get global minimum for a given function and dimension\"\"\"\n",
        "        global_minima = {\n",
        "            'ackley': (np.zeros(dimension), 0.0),\n",
        "            'rastrigin': (np.zeros(dimension), 0.0),\n",
        "            'rosenbrock': (np.ones(dimension), 0.0),\n",
        "            'sphere': (np.zeros(dimension), 0.0),\n",
        "            'michalewicz': (None, None),  # Varies with dimension\n",
        "        }\n",
        "        return global_minima.get(func_name, (None, None))\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley(x: np.ndarray) -> float:\n",
        "        \"\"\"Ackley function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "        return (-20 * np.exp(-0.2 * np.sqrt(sum_sq / n))\n",
        "                - np.exp(sum_cos / n)\n",
        "                + 20 + np.e)\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Ackley function\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "\n",
        "        term1 = (20 * 0.2 / np.sqrt(n * sum_sq)) * np.exp(-0.2 * np.sqrt(sum_sq / n)) * x\n",
        "        term2 = (2 * np.pi / n) * np.exp(sum_cos / n) * np.sin(2 * np.pi * x)\n",
        "        return term1 + term2\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Ackley Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.ackley_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin(x: np.ndarray) -> float:\n",
        "        \"\"\"Rastrigin function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        return 10 * n + np.sum(x**2 - 10 * np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rastrigin function\"\"\"\n",
        "        return 2 * x + 20 * np.pi * np.sin(2 * np.pi * x)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Rastrigin function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n) + 40 * np.pi**2 * np.diag(np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere(x: np.ndarray) -> float:\n",
        "        \"\"\"Sphere function for n dimensions\"\"\"\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Sphere function\"\"\"\n",
        "        return 2 * x\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Sphere function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock(x: np.ndarray) -> float:\n",
        "        \"\"\"Rosenbrock function for n dimensions\"\"\"\n",
        "        return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rosenbrock function\"\"\"\n",
        "        n = len(x)\n",
        "        grad = np.zeros(n)\n",
        "        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "        grad[-1] = 200 * (x[-1] - x[-2]**2)\n",
        "        if n > 2:\n",
        "            grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
        "        return grad\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Rosenbrock Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.rosenbrock_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "class OptimizationLogger:\n",
        "    \"\"\"Handles logging of optimization progress\"\"\"\n",
        "    def __init__(self, method: str, function_name: str, dimension: int):\n",
        "        self.method = method\n",
        "        self.function_name = function_name\n",
        "        self.dimension = dimension\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.path = []\n",
        "        self.f_path = []\n",
        "        self.grad_norm_path = []\n",
        "        self.step_sizes = []\n",
        "        self.memory_usage = []\n",
        "        self.timestamps = []\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log_iteration(self, x: np.ndarray, f: float, grad_norm: float, step_size: float):\n",
        "        self.path.append(x.copy())\n",
        "        self.f_path.append(f)\n",
        "        self.grad_norm_path.append(grad_norm)\n",
        "        self.step_sizes.append(step_size)\n",
        "        self.memory_usage.append(self.get_memory_usage())\n",
        "        self.timestamps.append(time.time() - self.start_time)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_memory_usage() -> float:\n",
        "        \"\"\"Get current memory usage in MB\"\"\"\n",
        "        import psutil\n",
        "        process = psutil.Process()\n",
        "        return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    def save_logs(self, base_dir: str):\n",
        "        \"\"\"Save optimization logs to CSV\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        log_dir = os.path.join(base_dir, self.function_name, str(self.dimension) + \"D\", self.method)\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        log_data = {\n",
        "            'iteration': range(len(self.path)),\n",
        "            'function_value': self.f_path,\n",
        "            'gradient_norm': self.grad_norm_path,\n",
        "            'step_size': self.step_sizes,\n",
        "            'memory_mb': self.memory_usage,\n",
        "            'runtime_seconds': self.timestamps\n",
        "        }\n",
        "\n",
        "        # Add parameter values\n",
        "        for i in range(self.dimension):\n",
        "            log_data[f'x{i+1}'] = [p[i] for p in self.path]\n",
        "\n",
        "        df = pd.DataFrame(log_data)\n",
        "        df.to_csv(os.path.join(log_dir, f'optimization_log_{timestamp}.csv'), index=False)\n",
        "\n",
        "class Visualizer:\n",
        "    \"\"\"Enhanced visualization capabilities\"\"\"\n",
        "    @staticmethod\n",
        "    def plot_optimization_summary(results: Dict[str, OptimizationResult], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot summary comparing initial and final states\"\"\"\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        methods = list(results.keys())\n",
        "        x = np.arange(len(methods))\n",
        "\n",
        "        # Fix the ticks warning by setting them explicitly\n",
        "        for ax in [ax1, ax2, ax3, ax4]:\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(methods, rotation=45)\n",
        "\n",
        "        # Function Values Plot\n",
        "        initial_values = [result.f_initial for result in results.values()]\n",
        "        final_values = [result.f_final for result in results.values()]\n",
        "        width = 0.35\n",
        "\n",
        "        ax1.bar(x - width/2, initial_values, width, label='Initial', color='lightcoral')\n",
        "        ax1.bar(x + width/2, final_values, width, label='Final', color='lightgreen')\n",
        "        ax1.set_ylabel('Function Value')\n",
        "        ax1.set_title('Initial vs Final Function Values')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Add global minimum line if available\n",
        "        _, f_min = TestFunctions.get_global_minimum(function_name, results[methods[0]].dimension)\n",
        "        if f_min is not None:\n",
        "            ax1.axhline(y=f_min, color='r', linestyle='--', label=f'Global Min ({f_min})')\n",
        "            ax1.legend()\n",
        "\n",
        "        # Gradient Norms Plot\n",
        "        initial_grads = [np.linalg.norm(result.grad_initial) for result in results.values()]\n",
        "        final_grads = [np.linalg.norm(result.grad_final) for result in results.values()]\n",
        "\n",
        "        ax2.bar(x - width/2, initial_grads, width, label='Initial', color='lightcoral')\n",
        "        ax2.bar(x + width/2, final_grads, width, label='Final', color='lightgreen')\n",
        "        ax2.set_ylabel('Gradient Norm')\n",
        "        ax2.set_title('Initial vs Final Gradient Norms')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        # Runtime Comparison\n",
        "        runtimes = [result.runtime for result in results.values()]\n",
        "        ax3.bar(methods, runtimes, color='skyblue')\n",
        "        ax3.set_ylabel('Runtime (seconds)')\n",
        "        ax3.set_title('Total Runtime by Method')\n",
        "        ax3.grid(True)\n",
        "\n",
        "        # Iterations Comparison\n",
        "        iterations = [result.iterations for result in results.values()]\n",
        "        ax4.bar(methods, iterations, color='lightgreen')\n",
        "        ax4.set_ylabel('Number of Iterations')\n",
        "        ax4.set_title('Total Iterations by Method')\n",
        "        ax4.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, f'optimization_summary_{function_name}.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_convergence(results: Dict[str, OptimizationResult], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot convergence with enhanced information\"\"\"\n",
        "        if not results:  # Skip if no results\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        plotted_something = False\n",
        "\n",
        "        # Get global minimum if available\n",
        "        _, f_min = TestFunctions.get_global_minimum(function_name)\n",
        "        f_min_text = f\"(Global min: {f_min})\" if f_min is not None else \"\"\n",
        "\n",
        "        # Function value convergence\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'f_path') and result.f_path:  # Check if result and f_path exist\n",
        "                ax1.semilogy(result.f_path, label=f\"{method}\")\n",
        "                plotted_something = True\n",
        "\n",
        "        if plotted_something:\n",
        "            ax1.set_xlabel('Iteration')\n",
        "            ax1.set_ylabel('Function Value (log scale)')\n",
        "            ax1.set_title(f'Function Value Convergence {f_min_text}')\n",
        "            ax1.legend()\n",
        "            ax1.grid(True)\n",
        "\n",
        "        plotted_something = False\n",
        "        # Gradient norm convergence\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'grad_norm_path') and result.grad_norm_path:\n",
        "                ax2.semilogy(result.grad_norm_path, label=f\"{method}\")\n",
        "                plotted_something = True\n",
        "\n",
        "        if plotted_something:\n",
        "            ax2.set_xlabel('Iteration')\n",
        "            ax2.set_ylabel('Gradient Norm (log scale)')\n",
        "            ax2.set_title('Gradient Norm Convergence')\n",
        "            ax2.legend()\n",
        "            ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, f'convergence_{function_name}.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_2d_trajectory(f: Callable, result: OptimizationResult, save_dir: str, experiment_num: int = None):\n",
        "      \"\"\"Plot optimization trajectory for 2D problems with two variants - with and without global minimum\"\"\"\n",
        "      if result.dimension != 2:\n",
        "          return\n",
        "\n",
        "      # Even more reduced resolution for better performance\n",
        "      grid_size = 50  # Reduced from 100 to 50\n",
        "\n",
        "      for show_global_min in [True, False]:  # Create both variants\n",
        "          plt.figure(figsize=(12, 10))\n",
        "\n",
        "          # Get path bounds and include (0,0)\n",
        "          path = np.array(result.path)\n",
        "          x_min_traj = min(float(path[:, 0].min()), 0)\n",
        "          x_max_traj = max(float(path[:, 0].max()), 0)\n",
        "          y_min_traj = min(float(path[:, 1].min()), 0)\n",
        "          y_max_traj = max(float(path[:, 1].max()), 0)\n",
        "\n",
        "          # Add margin to bounds\n",
        "          margin = 0.1\n",
        "          x_range = x_max_traj - x_min_traj\n",
        "          y_range = y_max_traj - y_min_traj\n",
        "          plot_x_min = x_min_traj - margin * x_range\n",
        "          plot_x_max = x_max_traj + margin * x_range\n",
        "          plot_y_min = y_min_traj - margin * y_range\n",
        "          plot_y_max = y_max_traj + margin * y_range\n",
        "\n",
        "          # Create contour plot with reduced resolution\n",
        "          x = np.linspace(plot_x_min, plot_x_max, grid_size)\n",
        "          y = np.linspace(plot_y_min, plot_y_max, grid_size)\n",
        "          X, Y = np.meshgrid(x, y)\n",
        "\n",
        "          # Vectorized computation of Z values\n",
        "          points = np.column_stack((X.ravel(), Y.ravel()))\n",
        "          Z = np.array([f(point) for point in points]).reshape(X.shape)\n",
        "\n",
        "          # Reduced number of contour levels\n",
        "          global_max = float(Z.max())\n",
        "          global_levels = np.linspace(0, global_max, 15)  # Further reduced from 20 to 15\n",
        "\n",
        "          # Plot contours with reduced detail\n",
        "          contour = plt.contour(X, Y, Z, levels=global_levels, cmap='viridis', alpha=0.7)\n",
        "          plt.colorbar(contour, label='Function Value')\n",
        "\n",
        "          # Plot trajectory\n",
        "          plt.plot(path[:, 0], path[:, 1], 'r.-', label='Optimization Path',\n",
        "                  linewidth=1, markersize=2, zorder=5)\n",
        "          plt.plot(path[0, 0], path[0, 1], 'go', label='Start',\n",
        "                  markersize=8, zorder=6)\n",
        "          plt.plot(path[-1, 0], path[-1, 1], 'ro', label='End',\n",
        "                  markersize=8, zorder=6)\n",
        "\n",
        "          # Only plot global minimum in the first variant\n",
        "          if show_global_min:\n",
        "              x_min, f_min = TestFunctions.get_global_minimum(result.function_name)\n",
        "              if x_min is not None:\n",
        "                  plt.plot(x_min[0], x_min[1], 'k*', label='Global Minimum',\n",
        "                          markersize=10, zorder=6)\n",
        "\n",
        "          plt.xlim(float(plot_x_min), float(plot_x_max))\n",
        "          plt.ylim(float(plot_y_min), float(plot_y_max))\n",
        "          plt.grid(True)\n",
        "\n",
        "          plt.title(f'{result.function_name} - {result.method}\\n'\n",
        "                  f'Final value: {result.f_final:.6f}\\n'\n",
        "                  f'Iterations: {result.iterations}')\n",
        "          plt.xlabel('x₁')\n",
        "          plt.ylabel('x₂')\n",
        "          plt.legend()\n",
        "\n",
        "          # Include experiment number and variant in filename\n",
        "          experiment_suffix = f'_exp{experiment_num}' if experiment_num is not None else ''\n",
        "          variant_suffix = '_with_global_min' if show_global_min else '_path_only'\n",
        "          filename = f'trajectory_{result.function_name}_{result.method}{experiment_suffix}{variant_suffix}.png'\n",
        "\n",
        "          # Save with reduced DPI\n",
        "          plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')\n",
        "          plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_computational_metrics(results: Dict[str, OptimizationResult], save_dir: str):\n",
        "        \"\"\"Plot computational metrics over time\"\"\"\n",
        "        if not results:  # Skip if no results\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Memory usage over time\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'timestamps') and hasattr(result, 'memory_usage'):\n",
        "                if result.timestamps and result.memory_usage:  # Check if data exists\n",
        "                    ax1.plot(result.timestamps, result.memory_usage, label=method)\n",
        "        ax1.set_xlabel('Time (seconds)')\n",
        "        ax1.set_ylabel('Memory Usage (MB)')\n",
        "        ax1.set_title('Memory Usage Over Time')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # FLOPS over time\n",
        "        for method, result in results.items():\n",
        "            if result and hasattr(result, 'timestamps') and hasattr(result, 'flops_per_step'):\n",
        "                if result.timestamps and result.flops_per_step:  # Check if data exists\n",
        "                    cumulative_flops = np.cumsum(result.flops_per_step)\n",
        "                    ax2.plot(result.timestamps, cumulative_flops, label=method)\n",
        "        ax2.set_xlabel('Time (seconds)')\n",
        "        ax2.set_ylabel('Cumulative FLOPS')\n",
        "        ax2.set_title('Computational Cost Over Time')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'computational_metrics.png'), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "def get_memory_usage() -> float:\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def run_optimization(f: Callable,\n",
        "                    grad: Callable,\n",
        "                    hess: Callable,\n",
        "                    x0: np.ndarray,\n",
        "                    method: str,\n",
        "                    function_name: str) -> OptimizationResult:\n",
        "    \"\"\"Enhanced optimization runner with detailed metrics\"\"\"\n",
        "    start_time = time.time()\n",
        "    flop_counter = FLOPCounter()\n",
        "\n",
        "    # Calculate initial metrics\n",
        "    f_initial = f(x0)\n",
        "    grad_initial = grad(x0)\n",
        "\n",
        "    # Storage for metrics\n",
        "    path = [x0.copy()]  # Start with initial point\n",
        "    f_path = [f_initial]\n",
        "    grad_norm_path = [np.linalg.norm(grad_initial)]\n",
        "    timestamps = [0.0]\n",
        "    memory_usage = [get_memory_usage()]\n",
        "    flops_per_step = [0]\n",
        "\n",
        "    def callback(xk):\n",
        "        current_time = time.time() - start_time\n",
        "\n",
        "        # Calculate metrics\n",
        "        f_val = f(xk)\n",
        "        grad_val = grad(xk)\n",
        "        grad_norm = np.linalg.norm(grad_val)\n",
        "\n",
        "        # Store metrics\n",
        "        path.append(xk.copy())\n",
        "        f_path.append(f_val)\n",
        "        grad_norm_path.append(grad_norm)\n",
        "        timestamps.append(current_time)\n",
        "        memory_usage.append(get_memory_usage())\n",
        "        flops_per_step.append(flop_counter.flops)\n",
        "\n",
        "    try:\n",
        "        # Run optimization with method-specific settings\n",
        "        if method == 'BFGS':\n",
        "            result = minimize(f, x0, method=method, jac=grad, callback=callback)\n",
        "        elif method == 'newton-cg':\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        elif method in ['trust-exact', 'trust-krylov']:\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n",
        "        # Calculate final gradient\n",
        "        grad_final = grad(result.x)\n",
        "        runtime = time.time() - start_time\n",
        "\n",
        "        # Print optimization results with better formatting\n",
        "        print(f\"\\n{method} Results:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"{'Initial function value:':<25} {f_initial:>12.6f}\")\n",
        "        print(f\"{'Final function value:':<25} {result.fun:>12.6f}\")\n",
        "        print(f\"{'Number of iterations:':<25} {result.nit:>12d}\")\n",
        "        print(f\"{'Runtime:':<25} {runtime:>12.4f} seconds\")\n",
        "        print(f\"{'Initial gradient norm:':<25} {np.linalg.norm(grad_initial):>12.6f}\")\n",
        "        print(f\"{'Final gradient norm:':<25} {np.linalg.norm(grad_final):>12.6f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        return OptimizationResult(\n",
        "            x_final=result.x,\n",
        "            f_final=result.fun,\n",
        "            success=result.success,\n",
        "            iterations=result.nit,\n",
        "            runtime=runtime,\n",
        "            path=path,\n",
        "            f_path=f_path,\n",
        "            grad_norm_path=grad_norm_path,\n",
        "            timestamps=timestamps,\n",
        "            memory_usage=memory_usage,\n",
        "            flops_per_step=flops_per_step,\n",
        "            method=method,\n",
        "            dimension=len(x0),\n",
        "            function_name=function_name,\n",
        "            x_initial=x0,\n",
        "            f_initial=f_initial,\n",
        "            grad_initial=grad_initial,\n",
        "            grad_final=grad_final\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Optimization failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "    \"\"\"Basic gradient descent optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            x = x - self.learning_rate * g\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            # Add small random noise to simulate stochasticity\n",
        "            noise = np.random.normal(0, 0.01, size=x.shape)\n",
        "            g = g + noise\n",
        "            x = x - self.learning_rate * g\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }\n",
        "\n",
        "class MomentumGD:\n",
        "    \"\"\"Gradient Descent with Momentum\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            v = self.momentum * v - self.learning_rate * g\n",
        "            x = x + v\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }\n",
        "\n",
        "class RMSprop:\n",
        "    \"\"\"RMSprop optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, decay_rate=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            v = self.decay_rate * v + (1 - self.decay_rate) * g**2\n",
        "            x = x - self.learning_rate * g / (np.sqrt(v) + self.epsilon)\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }\n",
        "\n",
        "class Adam:\n",
        "    \"\"\"Adam optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "                max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        m = np.zeros_like(x)\n",
        "        v = np.zeros_like(x)\n",
        "        path = [x.copy()]\n",
        "        f_path = [f(x)]\n",
        "        grad_path = [np.linalg.norm(grad(x))]\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            m = self.beta1 * m + (1 - self.beta1) * g\n",
        "            v = self.beta2 * v + (1 - self.beta2) * g**2\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = m / (1 - self.beta1**(i + 1))\n",
        "            v_hat = v / (1 - self.beta2**(i + 1))\n",
        "\n",
        "            x = x - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "            path.append(x.copy())\n",
        "            f_path.append(f(x))\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            grad_path.append(grad_norm)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': path,\n",
        "            'f_path': f_path,\n",
        "            'grad_path': grad_path,\n",
        "            'grad_final': grad(x)\n",
        "        }"
      ],
      "metadata": {
        "id": "sGlrjFy2pnuJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentManager:\n",
        "    \"\"\"Manages multiple optimization experiments\"\"\"\n",
        "    def __init__(self,\n",
        "                 test_functions: Dict[str, Tuple[Callable, Callable, Callable]],\n",
        "                 first_order_optimizers: Dict[str, object],\n",
        "                 second_order_methods: List[str],\n",
        "                 dimensions: List[int],\n",
        "                 n_experiments: int = 50,\n",
        "                 min_dist: float = 100,\n",
        "                 max_dist: float = 1000):\n",
        "        self.test_functions = test_functions\n",
        "        self.first_order_optimizers = first_order_optimizers\n",
        "        self.second_order_methods = second_order_methods\n",
        "        self.dimensions = dimensions\n",
        "        self.n_experiments = n_experiments\n",
        "        self.min_dist = min_dist\n",
        "        self.max_dist = max_dist\n",
        "\n",
        "    def generate_starting_points(self, dimension: int, seed: int = None) -> np.ndarray:\n",
        "        \"\"\"Generate random starting points with specified distance from origin\"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        starting_points = []\n",
        "\n",
        "        for _ in range(self.n_experiments):\n",
        "            direction = np.random.randn(dimension)\n",
        "            direction = direction / np.linalg.norm(direction)\n",
        "            distance = np.random.uniform(self.min_dist, self.max_dist)\n",
        "            point = direction * distance\n",
        "            starting_points.append(point)\n",
        "\n",
        "        return np.array(starting_points)\n",
        "\n",
        "    def run_experiments(self, base_dir: str = \"optimization_results\"):\n",
        "        \"\"\"Run all experiments with proper directory structure\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        experiment_dir = os.path.join(base_dir, f\"experiment_{timestamp}\")\n",
        "        all_results = []\n",
        "\n",
        "        for func_name, (f, grad, hess) in self.test_functions.items():\n",
        "            print(f\"\\nTesting {func_name} function:\")\n",
        "\n",
        "            for dim in self.dimensions:\n",
        "                print(f\"\\nDimension: {dim}\")\n",
        "\n",
        "                # Generate starting points for this dimension\n",
        "                starting_points = self.generate_starting_points(dim)\n",
        "\n",
        "                # Create directory structure\n",
        "                func_dir = os.path.join(experiment_dir, func_name, f\"{dim}D\")\n",
        "                first_order_dir = os.path.join(func_dir, \"first_order\")\n",
        "                second_order_dir = os.path.join(func_dir, \"second_order\")\n",
        "\n",
        "                for directory in [first_order_dir, second_order_dir]:\n",
        "                    os.makedirs(os.path.join(directory, \"results\"), exist_ok=True)\n",
        "                    os.makedirs(os.path.join(directory, \"trajectories\"), exist_ok=True)\n",
        "\n",
        "                # Run experiments for each starting point\n",
        "                for i, x0 in enumerate(starting_points):\n",
        "                    print(f\"\\nExperiment {i+1}/{self.n_experiments}\")\n",
        "\n",
        "                    # First order methods\n",
        "                    first_order_results = {}\n",
        "                    print(\"\\nFirst-order methods:\")\n",
        "                    for name, optimizer in self.first_order_optimizers.items():\n",
        "                        print(f\"Running {name}...\", end=\" \", flush=True)\n",
        "                        try:\n",
        "                            # Create logger for this optimization run\n",
        "                            logger = OptimizationLogger(name, func_name, dim)\n",
        "\n",
        "                            # Create wrapper function for callback\n",
        "                            def callback(xk):\n",
        "                                f_val = f(xk)\n",
        "                                grad_val = grad(xk)\n",
        "                                grad_norm = np.linalg.norm(grad_val)\n",
        "                                step_size = 0.0  # We don't track step size for these methods\n",
        "                                logger.log_iteration(xk, f_val, grad_norm, step_size)\n",
        "\n",
        "                            result = optimizer.optimize(f, grad, x0, callback=callback)\n",
        "\n",
        "                            opt_result = OptimizationResult(\n",
        "                                x_final=result['x'],\n",
        "                                f_final=result['fun'],\n",
        "                                success=result['success'],\n",
        "                                iterations=result['nit'],\n",
        "                                runtime=time.time() - logger.start_time,\n",
        "                                path=result['path'],\n",
        "                                f_path=result['f_path'],\n",
        "                                grad_norm_path=result['grad_path'],\n",
        "                                timestamps=logger.timestamps,\n",
        "                                memory_usage=logger.memory_usage,\n",
        "                                method=name,\n",
        "                                dimension=dim,\n",
        "                                function_name=func_name,\n",
        "                                x_initial=x0,\n",
        "                                f_initial=f(x0),\n",
        "                                grad_initial=grad(x0),\n",
        "                                grad_final=result['grad_final']\n",
        "                            )\n",
        "\n",
        "                            first_order_results[name] = opt_result\n",
        "                            all_results.append(self._format_result(opt_result, \"first_order\", i))\n",
        "                            print(\"Done\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Failed: {str(e)}\")\n",
        "\n",
        "                    # Second order methods\n",
        "                    second_order_results = {}\n",
        "                    print(\"\\nSecond-order methods:\")\n",
        "                    for method in self.second_order_methods:\n",
        "                        print(f\"Running {method}...\", end=\" \", flush=True)\n",
        "                        try:\n",
        "                            result = run_optimization(f, grad, hess, x0, method, func_name)\n",
        "                            if result is not None:\n",
        "                                second_order_results[method] = result\n",
        "                                all_results.append(self._format_result(result, \"second_order\", i))\n",
        "                                print(\"Done\")\n",
        "                            else:\n",
        "                                print(\"Failed\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Failed: {str(e)}\")\n",
        "\n",
        "                    # Generate plots\n",
        "                    self._generate_plots(first_order_results, first_order_dir, f, i, dim)\n",
        "                    self._generate_plots(second_order_results, second_order_dir, f, i, dim)\n",
        "\n",
        "                # Generate statistical plots for this dimension\n",
        "                for results, dir_path in [(first_order_results, first_order_dir),\n",
        "                                        (second_order_results, second_order_dir)]:\n",
        "                    if results:\n",
        "                        results_df = pd.DataFrame([r for r in all_results\n",
        "                                                if r['function'] == func_name and\n",
        "                                                r['dimension'] == dim and\n",
        "                                                r['method'] in results.keys()])\n",
        "                        generate_statistics(results_df, dir_path)\n",
        "\n",
        "        # Save complete results and generate overall statistics\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "        results_df.to_csv(os.path.join(experiment_dir, \"all_results.csv\"), index=False)\n",
        "        generate_statistics(results_df, experiment_dir)\n",
        "\n",
        "    def _generate_plots(self, results: Dict[str, OptimizationResult],\n",
        "                   base_dir: str, f: Callable, exp_num: int, dim: int):\n",
        "        \"\"\"Generate all plots for a set of results\"\"\"\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        results_dir = os.path.join(base_dir, \"results\")\n",
        "        trajectory_dir = os.path.join(base_dir, \"trajectories\")\n",
        "\n",
        "        # Only generate plots if we have results\n",
        "        if results:\n",
        "            # Get function name from first result\n",
        "            func_name = next(iter(results.values())).function_name\n",
        "\n",
        "            # Generate convergence plots\n",
        "            if any(hasattr(r, 'f_path') and r.f_path for r in results.values()):\n",
        "                Visualizer.plot_convergence(results, results_dir, f\"{func_name}_exp{exp_num}\")\n",
        "\n",
        "            # Generate computational metrics\n",
        "            if any(hasattr(r, 'timestamps') and r.timestamps for r in results.values()):\n",
        "                Visualizer.plot_computational_metrics(results, results_dir)\n",
        "\n",
        "            # Generate optimization summary\n",
        "            if results:\n",
        "                Visualizer.plot_optimization_summary(results, results_dir, f\"{func_name}_exp{exp_num}\")\n",
        "\n",
        "            # Generate 2D trajectories if applicable\n",
        "            if dim == 2:\n",
        "                for method, result in results.items():\n",
        "                    if result is not None:\n",
        "                        try:\n",
        "                            Visualizer.plot_2d_trajectory(f, result, trajectory_dir, exp_num)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error plotting trajectory for {method}: {e}\")\n",
        "\n",
        "    def _format_result(self, result: OptimizationResult, opt_type: str, exp_num: int) -> dict:\n",
        "        \"\"\"Format OptimizationResult for DataFrame\"\"\"\n",
        "        return {\n",
        "            'function': result.function_name,\n",
        "            'dimension': result.dimension,\n",
        "            'experiment': exp_num,\n",
        "            'method': result.method,\n",
        "            'optimizer_type': opt_type,\n",
        "            'start_distance': np.linalg.norm(result.x_initial),\n",
        "            'final_value': result.f_final,\n",
        "            'iterations': result.iterations,\n",
        "            'runtime': result.runtime,\n",
        "            'success': result.success,\n",
        "            'distance_to_minimum': result.distance_to_minimum,\n",
        "            'f_error': result.f_error,\n",
        "            'initial_gradient_norm': np.linalg.norm(result.grad_initial),\n",
        "            'final_gradient_norm': np.linalg.norm(result.grad_final)\n",
        "        }\n",
        "\n",
        "def generate_statistics(results_df: pd.DataFrame, save_dir: str):\n",
        "    \"\"\"Generate comprehensive statistical visualizations for optimization results\"\"\"\n",
        "    # Set style for better-looking plots\n",
        "    plt.style.use('default')  # Use default style instead of seaborn\n",
        "\n",
        "    # Set the color palette manually\n",
        "    colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC', '#99CCFF']\n",
        "\n",
        "    # 1. Distribution of final values (Violin Plot)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.violinplot(data=results_df, x='method', y='final_value')\n",
        "    plt.title('Distribution of Final Values by Method')\n",
        "    plt.xlabel('Optimization Method')\n",
        "    plt.ylabel('Final Function Value')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'final_values_distribution.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Success rates with confidence intervals\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    success_data = results_df.groupby('method')['success'].agg(['mean', 'count'])\n",
        "    success_data['ci'] = 1.96 * np.sqrt(success_data['mean'] * (1 - success_data['mean']) / success_data['count'])\n",
        "\n",
        "    ax = success_data['mean'].plot(kind='bar', yerr=success_data['ci'], capsize=5)\n",
        "    plt.title('Success Rates by Method with 95% Confidence Intervals')\n",
        "    plt.ylabel('Success Rate')\n",
        "    plt.xlabel('Method')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'success_rates.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Runtime comparison (Box Plot)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(data=results_df, x='method', y='runtime')\n",
        "    plt.title('Runtime Distribution by Method')\n",
        "    plt.xlabel('Method')\n",
        "    plt.ylabel('Runtime (seconds)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'runtime_distribution.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Convergence efficiency (Iterations vs Final Value)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(data=results_df, x='iterations', y='final_value', hue='method', style='method')\n",
        "    plt.title('Convergence Efficiency')\n",
        "    plt.xlabel('Number of Iterations')\n",
        "    plt.ylabel('Final Function Value')\n",
        "    plt.yscale('log')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(save_dir, 'convergence_efficiency.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Distance to minimum vs Runtime\n",
        "    if 'distance_to_minimum' in results_df.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.scatterplot(data=results_df, x='runtime', y='distance_to_minimum',\n",
        "                      hue='method', style='method')\n",
        "        plt.title('Distance to Minimum vs Runtime')\n",
        "        plt.xlabel('Runtime (seconds)')\n",
        "        plt.ylabel('Distance to Global Minimum')\n",
        "        plt.yscale('log')\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(save_dir, 'distance_vs_runtime.png'), dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # Generate summary statistics table\n",
        "    summary_stats = results_df.groupby('method').agg({\n",
        "        'final_value': ['mean', 'std', 'min', 'max'],\n",
        "        'runtime': ['mean', 'std'],\n",
        "        'iterations': ['mean', 'std'],\n",
        "        'success': 'mean',\n",
        "        'distance_to_minimum': ['mean', 'std'] if 'distance_to_minimum' in results_df.columns else None\n",
        "    }).round(4)\n",
        "\n",
        "    # Save summary statistics\n",
        "    summary_stats.to_csv(os.path.join(save_dir, 'summary_statistics.csv'))\n",
        "\n",
        "    return summary_stats"
      ],
      "metadata": {
        "id": "nd91nH3NiT5D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    test_functions = {\n",
        "        'ackley': (\n",
        "            TestFunctions.ackley,\n",
        "            TestFunctions.ackley_gradient,\n",
        "            TestFunctions.ackley_hessian\n",
        "        ),\n",
        "        'rastrigin': (\n",
        "            TestFunctions.rastrigin,\n",
        "            TestFunctions.rastrigin_gradient,\n",
        "            TestFunctions.rastrigin_hessian\n",
        "        ),\n",
        "        'sphere': (\n",
        "            TestFunctions.sphere,\n",
        "            TestFunctions.sphere_gradient,\n",
        "            TestFunctions.sphere_hessian\n",
        "        ),\n",
        "        'rosenbrock': (\n",
        "            TestFunctions.rosenbrock,\n",
        "            TestFunctions.rosenbrock_gradient,\n",
        "            TestFunctions.rosenbrock_hessian\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # First-order optimizers with their configurations\n",
        "    first_order_optimizers = {\n",
        "        'gradient_descent': GradientDescent(learning_rate=0.01),\n",
        "        'sgd': SGD(learning_rate=0.01),\n",
        "        'momentum': MomentumGD(learning_rate=0.01, momentum=0.9),\n",
        "        'rmsprop': RMSprop(learning_rate=0.01, decay_rate=0.9),\n",
        "        'adam': Adam(learning_rate=0.001)\n",
        "    }\n",
        "\n",
        "    # Second-order methods\n",
        "    second_order_methods = ['BFGS', 'newton-cg', 'trust-exact', 'trust-krylov']\n",
        "\n",
        "    dimensions = [2, 4, 8, 12]\n",
        "\n",
        "    # Create experiment manager\n",
        "    experiment = ExperimentManager(\n",
        "        test_functions=test_functions,\n",
        "        first_order_optimizers=first_order_optimizers,\n",
        "        second_order_methods=second_order_methods,\n",
        "        dimensions=dimensions,\n",
        "        n_experiments=5,\n",
        "        min_dist=16,\n",
        "        max_dist=64\n",
        "    )\n",
        "\n",
        "    # Run experiments\n",
        "    experiment.run_experiments()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqoSGTWxadj1",
        "outputId": "3b3b872f-51df-4939-c494-9f5117d0841a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing ackley function:\n",
            "\n",
            "Dimension: 2\n",
            "\n",
            "Experiment 1/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 22.327602\n",
            "Final function value: 19.993164\n",
            "Number of iterations: 12\n",
            "Runtime: 0.0067 seconds\n",
            "Initial gradient norm: 0.484072\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 22.327602\n",
            "Final function value: 19.993164\n",
            "Number of iterations: 20\n",
            "Runtime: 0.0118 seconds\n",
            "Initial gradient norm: 0.484072\n",
            "Final gradient norm: 0.001052\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 22.327602\n",
            "Final function value: 19.993164\n",
            "Number of iterations: 29\n",
            "Runtime: 0.0171 seconds\n",
            "Initial gradient norm: 0.484072\n",
            "Final gradient norm: 0.000080\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 22.327602\n",
            "Final function value: 19.993164\n",
            "Number of iterations: 29\n",
            "Runtime: 0.0174 seconds\n",
            "Initial gradient norm: 0.484072\n",
            "Final gradient norm: 0.000087\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 2/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 22.178146\n",
            "Final function value: 19.979295\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0052 seconds\n",
            "Initial gradient norm: 1.677944\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 22.178146\n",
            "Final function value: 19.981526\n",
            "Number of iterations: 18\n",
            "Runtime: 0.0081 seconds\n",
            "Initial gradient norm: 1.677944\n",
            "Final gradient norm: 0.001393\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 22.178146\n",
            "Final function value: 19.983013\n",
            "Number of iterations: 22\n",
            "Runtime: 0.0140 seconds\n",
            "Initial gradient norm: 1.677944\n",
            "Final gradient norm: 0.000095\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 22.178146\n",
            "Final function value: 19.983013\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0065 seconds\n",
            "Initial gradient norm: 1.677944\n",
            "Final gradient norm: 0.000094\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 3/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 22.018193\n",
            "Final function value: 19.839786\n",
            "Number of iterations: 6\n",
            "Runtime: 0.0050 seconds\n",
            "Initial gradient norm: 2.052305\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 22.018193\n",
            "Final function value: 19.958009\n",
            "Number of iterations: 16\n",
            "Runtime: 0.0099 seconds\n",
            "Initial gradient norm: 2.052305\n",
            "Final gradient norm: 0.001442\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 22.018193\n",
            "Final function value: 19.958009\n",
            "Number of iterations: 29\n",
            "Runtime: 0.0170 seconds\n",
            "Initial gradient norm: 2.052305\n",
            "Final gradient norm: 0.000081\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 22.018193\n",
            "Final function value: 19.958009\n",
            "Number of iterations: 29\n",
            "Runtime: 0.0184 seconds\n",
            "Initial gradient norm: 2.052305\n",
            "Final gradient norm: 0.000080\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 4/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 21.052027\n",
            "Final function value: 19.948739\n",
            "Number of iterations: 4\n",
            "Runtime: 0.0032 seconds\n",
            "Initial gradient norm: 6.154889\n",
            "Final gradient norm: 0.000011\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 21.052027\n",
            "Final function value: 19.948739\n",
            "Number of iterations: 16\n",
            "Runtime: 0.0090 seconds\n",
            "Initial gradient norm: 6.154889\n",
            "Final gradient norm: 0.001160\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 21.052027\n",
            "Final function value: 19.948739\n",
            "Number of iterations: 26\n",
            "Runtime: 0.0140 seconds\n",
            "Initial gradient norm: 6.154889\n",
            "Final gradient norm: 0.000098\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 21.052027\n",
            "Final function value: 19.948739\n",
            "Number of iterations: 26\n",
            "Runtime: 0.0189 seconds\n",
            "Initial gradient norm: 6.154889\n",
            "Final gradient norm: 0.000097\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 5/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 22.184065\n",
            "Final function value: 19.998067\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0063 seconds\n",
            "Initial gradient norm: 1.663219\n",
            "Final gradient norm: 0.000003\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 22.184065\n",
            "Final function value: 19.996701\n",
            "Number of iterations: 21\n",
            "Runtime: 0.0107 seconds\n",
            "Initial gradient norm: 1.663219\n",
            "Final gradient norm: 0.001277\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 22.184065\n",
            "Final function value: 19.996701\n",
            "Number of iterations: 29\n",
            "Runtime: 0.0147 seconds\n",
            "Initial gradient norm: 1.663219\n",
            "Final gradient norm: 0.000082\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 22.184065\n",
            "Final function value: 19.996701\n",
            "Number of iterations: 29\n",
            "Runtime: 0.0188 seconds\n",
            "Initial gradient norm: 1.663219\n",
            "Final gradient norm: 0.000076\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimension: 4\n",
            "\n",
            "Experiment 1/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 18.421710\n",
            "Final function value: 17.409864\n",
            "Number of iterations: 8\n",
            "Runtime: 0.0044 seconds\n",
            "Initial gradient norm: 3.905447\n",
            "Final gradient norm: 0.000001\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 18.421710\n",
            "Final function value: 17.409864\n",
            "Number of iterations: 25\n",
            "Runtime: 0.0223 seconds\n",
            "Initial gradient norm: 3.905447\n",
            "Final gradient norm: 0.001972\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 18.421710\n",
            "Final function value: 17.409864\n",
            "Number of iterations: 48\n",
            "Runtime: 0.0449 seconds\n",
            "Initial gradient norm: 3.905447\n",
            "Final gradient norm: 0.000092\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 18.421710\n",
            "Final function value: 17.409864\n",
            "Number of iterations: 47\n",
            "Runtime: 0.0463 seconds\n",
            "Initial gradient norm: 3.905447\n",
            "Final gradient norm: 0.000098\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 2/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 21.905740\n",
            "Final function value: 19.901937\n",
            "Number of iterations: 14\n",
            "Runtime: 0.0061 seconds\n",
            "Initial gradient norm: 1.471958\n",
            "Final gradient norm: 0.000006\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 21.905740\n",
            "Final function value: 19.901937\n",
            "Number of iterations: 31\n",
            "Runtime: 0.0281 seconds\n",
            "Initial gradient norm: 1.471958\n",
            "Final gradient norm: 0.001988\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 21.905740\n",
            "Final function value: 19.903308\n",
            "Number of iterations: 51\n",
            "Runtime: 0.0471 seconds\n",
            "Initial gradient norm: 1.471958\n",
            "Final gradient norm: 0.000086\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 21.905740\n",
            "Final function value: 19.903308\n",
            "Number of iterations: 50\n",
            "Runtime: 0.0469 seconds\n",
            "Initial gradient norm: 1.471958\n",
            "Final gradient norm: 0.000097\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 3/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 21.661580\n",
            "Final function value: 19.956087\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0063 seconds\n",
            "Initial gradient norm: 2.628926\n",
            "Final gradient norm: 0.000009\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 21.661580\n",
            "Final function value: 19.956087\n",
            "Number of iterations: 31\n",
            "Runtime: 0.0271 seconds\n",
            "Initial gradient norm: 2.628926\n",
            "Final gradient norm: 0.001757\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 21.661580\n",
            "Final function value: 19.956087\n",
            "Number of iterations: 44\n",
            "Runtime: 0.0473 seconds\n",
            "Initial gradient norm: 2.628926\n",
            "Final gradient norm: 0.000098\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 21.661580\n",
            "Final function value: 19.956087\n",
            "Number of iterations: 46\n",
            "Runtime: 0.0523 seconds\n",
            "Initial gradient norm: 2.628926\n",
            "Final gradient norm: 0.000092\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 4/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 21.090272\n",
            "Final function value: 19.763709\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0049 seconds\n",
            "Initial gradient norm: 3.706596\n",
            "Final gradient norm: 0.000009\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 21.090272\n",
            "Final function value: 19.763709\n",
            "Number of iterations: 29\n",
            "Runtime: 0.0251 seconds\n",
            "Initial gradient norm: 3.706596\n",
            "Final gradient norm: 0.001788\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 21.090272\n",
            "Final function value: 19.763709\n",
            "Number of iterations: 44\n",
            "Runtime: 0.0419 seconds\n",
            "Initial gradient norm: 3.706596\n",
            "Final gradient norm: 0.000094\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 21.090272\n",
            "Final function value: 19.763709\n",
            "Number of iterations: 44\n",
            "Runtime: 0.0448 seconds\n",
            "Initial gradient norm: 3.706596\n",
            "Final gradient norm: 0.000093\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 5/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 21.466505\n",
            "Final function value: 19.628115\n",
            "Number of iterations: 16\n",
            "Runtime: 0.0073 seconds\n",
            "Initial gradient norm: 2.100927\n",
            "Final gradient norm: 0.000001\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 21.466505\n",
            "Final function value: 19.628115\n",
            "Number of iterations: 28\n",
            "Runtime: 0.0274 seconds\n",
            "Initial gradient norm: 2.100927\n",
            "Final gradient norm: 0.001943\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 21.466505\n",
            "Final function value: 19.628115\n",
            "Number of iterations: 48\n",
            "Runtime: 0.0460 seconds\n",
            "Initial gradient norm: 2.100927\n",
            "Final gradient norm: 0.000094\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 21.466505\n",
            "Final function value: 19.628115\n",
            "Number of iterations: 47\n",
            "Runtime: 0.0515 seconds\n",
            "Initial gradient norm: 2.100927\n",
            "Final gradient norm: 0.000087\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimension: 8\n",
            "\n",
            "Experiment 1/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 21.425298\n",
            "Final function value: 19.670339\n",
            "Number of iterations: 12\n",
            "Runtime: 0.0064 seconds\n",
            "Initial gradient norm: 1.718181\n",
            "Final gradient norm: 0.000015\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 21.425298\n",
            "Final function value: 19.670339\n",
            "Number of iterations: 52\n",
            "Runtime: 0.1278 seconds\n",
            "Initial gradient norm: 1.718181\n",
            "Final gradient norm: 0.002988\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 21.425298\n",
            "Final function value: 19.670339\n",
            "Number of iterations: 82\n",
            "Runtime: 0.2116 seconds\n",
            "Initial gradient norm: 1.718181\n",
            "Final gradient norm: 0.000094\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 21.425298\n",
            "Final function value: 19.670339\n",
            "Number of iterations: 82\n",
            "Runtime: 0.2253 seconds\n",
            "Initial gradient norm: 1.718181\n",
            "Final gradient norm: 0.000092\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 2/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 20.629622\n",
            "Final function value: 19.105375\n",
            "Number of iterations: 17\n",
            "Runtime: 0.0076 seconds\n",
            "Initial gradient norm: 1.589715\n",
            "Final gradient norm: 0.000005\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 20.629622\n",
            "Final function value: 19.105375\n",
            "Number of iterations: 35\n",
            "Runtime: 0.0933 seconds\n",
            "Initial gradient norm: 1.589715\n",
            "Final gradient norm: 0.002794\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 20.629622\n",
            "Final function value: 19.105375\n",
            "Number of iterations: 86\n",
            "Runtime: 0.2331 seconds\n",
            "Initial gradient norm: 1.589715\n",
            "Final gradient norm: 0.000098\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 20.629622\n",
            "Final function value: 19.105375\n",
            "Number of iterations: 85\n",
            "Runtime: 0.2399 seconds\n",
            "Initial gradient norm: 1.589715\n",
            "Final gradient norm: 0.000093\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 3/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 20.285181\n",
            "Final function value: 18.996565\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0053 seconds\n",
            "Initial gradient norm: 2.446597\n",
            "Final gradient norm: 0.000002\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 20.285181\n",
            "Final function value: 18.996566\n",
            "Number of iterations: 38\n",
            "Runtime: 0.0953 seconds\n",
            "Initial gradient norm: 2.446597\n",
            "Final gradient norm: 0.002735\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 20.285181\n",
            "Final function value: 18.996565\n",
            "Number of iterations: 88\n",
            "Runtime: 0.2297 seconds\n",
            "Initial gradient norm: 2.446597\n",
            "Final gradient norm: 0.000089\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 20.285181\n",
            "Final function value: 18.996565\n",
            "Number of iterations: 87\n",
            "Runtime: 0.2313 seconds\n",
            "Initial gradient norm: 2.446597\n",
            "Final gradient norm: 0.000093\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 4/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 21.226626\n",
            "Final function value: 19.650344\n",
            "Number of iterations: 13\n",
            "Runtime: 0.0067 seconds\n",
            "Initial gradient norm: 2.112079\n",
            "Final gradient norm: 0.000005\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 21.226626\n",
            "Final function value: 19.650345\n",
            "Number of iterations: 39\n",
            "Runtime: 0.0993 seconds\n",
            "Initial gradient norm: 2.112079\n",
            "Final gradient norm: 0.002784\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 21.226626\n",
            "Final function value: 19.650344\n",
            "Number of iterations: 83\n",
            "Runtime: 0.2269 seconds\n",
            "Initial gradient norm: 2.112079\n",
            "Final gradient norm: 0.000090\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 21.226626\n",
            "Final function value: 19.650344\n",
            "Number of iterations: 80\n",
            "Runtime: 0.2173 seconds\n",
            "Initial gradient norm: 2.112079\n",
            "Final gradient norm: 0.000098\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 5/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 21.027168\n",
            "Final function value: 19.462112\n",
            "Number of iterations: 17\n",
            "Runtime: 0.0074 seconds\n",
            "Initial gradient norm: 1.976338\n",
            "Final gradient norm: 0.000001\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 21.027168\n",
            "Final function value: 19.462112\n",
            "Number of iterations: 47\n",
            "Runtime: 0.1288 seconds\n",
            "Initial gradient norm: 1.976338\n",
            "Final gradient norm: 0.002709\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 21.027168\n",
            "Final function value: 19.462112\n",
            "Number of iterations: 57\n",
            "Runtime: 0.1595 seconds\n",
            "Initial gradient norm: 1.976338\n",
            "Final gradient norm: 0.000096\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 21.027168\n",
            "Final function value: 19.462112\n",
            "Number of iterations: 75\n",
            "Runtime: 0.2021 seconds\n",
            "Initial gradient norm: 1.976338\n",
            "Final gradient norm: 0.000092\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimension: 12\n",
            "\n",
            "Experiment 1/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 18.225096\n",
            "Final function value: 16.493074\n",
            "Number of iterations: 19\n",
            "Runtime: 0.0093 seconds\n",
            "Initial gradient norm: 1.320670\n",
            "Final gradient norm: 0.000004\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 18.225096\n",
            "Final function value: 16.479601\n",
            "Number of iterations: 20\n",
            "Runtime: 0.1112 seconds\n",
            "Initial gradient norm: 1.320670\n",
            "Final gradient norm: 0.000318\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 18.225096\n",
            "Final function value: 16.407508\n",
            "Number of iterations: 123\n",
            "Runtime: 0.6963 seconds\n",
            "Initial gradient norm: 1.320670\n",
            "Final gradient norm: 0.000099\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 18.225096\n",
            "Final function value: 16.407508\n",
            "Number of iterations: 127\n",
            "Runtime: 0.7104 seconds\n",
            "Initial gradient norm: 1.320670\n",
            "Final gradient norm: 0.000097\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 2/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 19.736688\n",
            "Final function value: 17.816258\n",
            "Number of iterations: 18\n",
            "Runtime: 0.0077 seconds\n",
            "Initial gradient norm: 0.881861\n",
            "Final gradient norm: 0.000013\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 19.736688\n",
            "Final function value: 17.816258\n",
            "Number of iterations: 14\n",
            "Runtime: 0.0775 seconds\n",
            "Initial gradient norm: 0.881861\n",
            "Final gradient norm: 0.000318\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 19.736688\n",
            "Final function value: 17.843945\n",
            "Number of iterations: 133\n",
            "Runtime: 0.7526 seconds\n",
            "Initial gradient norm: 0.881861\n",
            "Final gradient norm: 0.000096\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 19.736688\n",
            "Final function value: 17.890014\n",
            "Number of iterations: 132\n",
            "Runtime: 0.7598 seconds\n",
            "Initial gradient norm: 0.881861\n",
            "Final gradient norm: 0.000093\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 3/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 16.386929\n",
            "Final function value: 14.849984\n",
            "Number of iterations: 15\n",
            "Runtime: 0.0068 seconds\n",
            "Initial gradient norm: 1.517835\n",
            "Final gradient norm: 0.000010\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 16.386929\n",
            "Final function value: 14.849984\n",
            "Number of iterations: 6\n",
            "Runtime: 0.0336 seconds\n",
            "Initial gradient norm: 1.517835\n",
            "Final gradient norm: 0.000097\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 16.386929\n",
            "Final function value: 14.849984\n",
            "Number of iterations: 111\n",
            "Runtime: 0.6310 seconds\n",
            "Initial gradient norm: 1.517835\n",
            "Final gradient norm: 0.000093\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 16.386929\n",
            "Final function value: 14.849984\n",
            "Number of iterations: 116\n",
            "Runtime: 0.6337 seconds\n",
            "Initial gradient norm: 1.517835\n",
            "Final gradient norm: 0.000099\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 4/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 19.012948\n",
            "Final function value: 17.549752\n",
            "Number of iterations: 16\n",
            "Runtime: 0.0055 seconds\n",
            "Initial gradient norm: 1.555891\n",
            "Final gradient norm: 0.000006\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 19.012948\n",
            "Final function value: 17.549752\n",
            "Number of iterations: 18\n",
            "Runtime: 0.1017 seconds\n",
            "Initial gradient norm: 1.555891\n",
            "Final gradient norm: 0.000343\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 19.012948\n",
            "Final function value: 17.597687\n",
            "Number of iterations: 127\n",
            "Runtime: 0.7312 seconds\n",
            "Initial gradient norm: 1.555891\n",
            "Final gradient norm: 0.000093\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 19.012948\n",
            "Final function value: 17.597687\n",
            "Number of iterations: 126\n",
            "Runtime: 0.7572 seconds\n",
            "Initial gradient norm: 1.555891\n",
            "Final gradient norm: 0.000099\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 5/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 20.125487\n",
            "Final function value: 18.272691\n",
            "Number of iterations: 16\n",
            "Runtime: 0.0067 seconds\n",
            "Initial gradient norm: 1.167139\n",
            "Final gradient norm: 0.000006\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 20.125487\n",
            "Final function value: 18.218622\n",
            "Number of iterations: 17\n",
            "Runtime: 0.0946 seconds\n",
            "Initial gradient norm: 1.167139\n",
            "Final gradient norm: 0.000400\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 20.125487\n",
            "Final function value: 18.278556\n",
            "Number of iterations: 131\n",
            "Runtime: 0.7667 seconds\n",
            "Initial gradient norm: 1.167139\n",
            "Final gradient norm: 0.000099\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 20.125487\n",
            "Final function value: 18.278556\n",
            "Number of iterations: 128\n",
            "Runtime: 0.7252 seconds\n",
            "Initial gradient norm: 1.167139\n",
            "Final gradient norm: 0.000100\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing rastrigin function:\n",
            "\n",
            "Dimension: 2\n",
            "\n",
            "Experiment 1/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 900.470930\n",
            "Final function value: 588.843664\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0068 seconds\n",
            "Initial gradient norm: 2.190771\n",
            "Final gradient norm: 0.000010\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 900.470930\n",
            "Final function value: 693.308145\n",
            "Number of iterations: 13\n",
            "Runtime: 0.0077 seconds\n",
            "Initial gradient norm: 2.190771\n",
            "Final gradient norm: 0.000032\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 900.470930\n",
            "Final function value: 843.366071\n",
            "Number of iterations: 8\n",
            "Runtime: 0.0078 seconds\n",
            "Initial gradient norm: 2.190771\n",
            "Final gradient norm: 0.000014\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 900.470930\n",
            "Final function value: 843.366071\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0098 seconds\n",
            "Initial gradient norm: 2.190771\n",
            "Final gradient norm: 0.000000\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 2/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 466.857968\n",
            "Final function value: 263.648796\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0048 seconds\n",
            "Initial gradient norm: 75.989263\n",
            "Final gradient norm: 0.000002\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 466.857968\n",
            "Final function value: 223.853970\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0056 seconds\n",
            "Initial gradient norm: 75.989263\n",
            "Final gradient norm: 0.000024\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 466.857968\n",
            "Final function value: 386.996410\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0061 seconds\n",
            "Initial gradient norm: 75.989263\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 466.857968\n",
            "Final function value: 386.996410\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0054 seconds\n",
            "Initial gradient norm: 75.989263\n",
            "Final gradient norm: 0.000001\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 3/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 3716.678888\n",
            "Final function value: 721.095013\n",
            "Number of iterations: 13\n",
            "Runtime: 0.0056 seconds\n",
            "Initial gradient norm: 178.807687\n",
            "Final gradient norm: 0.000003\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 3716.678888\n",
            "Final function value: 1031.461673\n",
            "Number of iterations: 15\n",
            "Runtime: 0.0065 seconds\n",
            "Initial gradient norm: 178.807687\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 3716.678888\n",
            "Final function value: 773.778388\n",
            "Number of iterations: 54\n",
            "Runtime: 0.0211 seconds\n",
            "Initial gradient norm: 178.807687\n",
            "Final gradient norm: 0.000016\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 3716.678888\n",
            "Final function value: 1450.052791\n",
            "Number of iterations: 38\n",
            "Runtime: 0.0167 seconds\n",
            "Initial gradient norm: 178.807687\n",
            "Final gradient norm: 0.000000\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 4/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 289.375891\n",
            "Final function value: 287.518724\n",
            "Number of iterations: 7\n",
            "Runtime: 0.0048 seconds\n",
            "Initial gradient norm: 36.547620\n",
            "Final gradient norm: 0.000001\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 289.375891\n",
            "Final function value: 287.518724\n",
            "Number of iterations: 5\n",
            "Runtime: 0.0036 seconds\n",
            "Initial gradient norm: 36.547620\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 289.375891\n",
            "Final function value: 287.518724\n",
            "Number of iterations: 3\n",
            "Runtime: 0.0030 seconds\n",
            "Initial gradient norm: 36.547620\n",
            "Final gradient norm: 0.000030\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 289.375891\n",
            "Final function value: 287.518724\n",
            "Number of iterations: 4\n",
            "Runtime: 0.0038 seconds\n",
            "Initial gradient norm: 36.547620\n",
            "Final gradient norm: 0.000000\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 5/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 2405.816142\n",
            "Final function value: 263.634371\n",
            "Number of iterations: 16\n",
            "Runtime: 0.0068 seconds\n",
            "Initial gradient norm: 76.690369\n",
            "Final gradient norm: 0.000001\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 2405.816142\n",
            "Final function value: 780.871192\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0055 seconds\n",
            "Initial gradient norm: 76.690369\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 2405.816142\n",
            "Final function value: 1376.393474\n",
            "Number of iterations: 17\n",
            "Runtime: 0.0093 seconds\n",
            "Initial gradient norm: 76.690369\n",
            "Final gradient norm: 0.000027\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 2405.816142\n",
            "Final function value: 1376.393474\n",
            "Number of iterations: 23\n",
            "Runtime: 0.0122 seconds\n",
            "Initial gradient norm: 76.690369\n",
            "Final gradient norm: 0.000038\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimension: 4\n",
            "\n",
            "Experiment 1/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 2682.469354\n",
            "Final function value: 287.530835\n",
            "Number of iterations: 13\n",
            "Runtime: 0.0054 seconds\n",
            "Initial gradient norm: 160.725568\n",
            "Final gradient norm: 0.000011\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 2682.469354\n",
            "Final function value: 1430.388292\n",
            "Number of iterations: 27\n",
            "Runtime: 0.0115 seconds\n",
            "Initial gradient norm: 160.725568\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 2682.469354\n",
            "Final function value: 1765.129549\n",
            "Number of iterations: 20\n",
            "Runtime: 0.0106 seconds\n",
            "Initial gradient norm: 160.725568\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 2682.469354\n",
            "Final function value: 1065.329328\n",
            "Number of iterations: 31\n",
            "Runtime: 0.0127 seconds\n",
            "Initial gradient norm: 160.725568\n",
            "Final gradient norm: 0.000000\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 2/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 1767.748759\n",
            "Final function value: 764.997788\n",
            "Number of iterations: 26\n",
            "Runtime: 0.0086 seconds\n",
            "Initial gradient norm: 100.118917\n",
            "Final gradient norm: 0.000010\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 1767.748759\n",
            "Final function value: 1223.517294\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0037 seconds\n",
            "Initial gradient norm: 100.118917\n",
            "Final gradient norm: 0.000029\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 1767.748759\n",
            "Final function value: 1657.064550\n",
            "Number of iterations: 8\n",
            "Runtime: 0.0056 seconds\n",
            "Initial gradient norm: 100.118917\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 1767.748759\n",
            "Final function value: 1657.064550\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0067 seconds\n",
            "Initial gradient norm: 100.118917\n",
            "Final gradient norm: 0.000000\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 3/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 2458.212519\n",
            "Final function value: 386.023233\n",
            "Number of iterations: 28\n",
            "Runtime: 0.0102 seconds\n",
            "Initial gradient norm: 166.829762\n",
            "Final gradient norm: 0.000004\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 2458.212519\n",
            "Final function value: 1799.476366\n",
            "Number of iterations: 7\n",
            "Runtime: 0.0035 seconds\n",
            "Initial gradient norm: 166.829762\n",
            "Final gradient norm: 0.000092\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 2458.212519\n",
            "Final function value: 2224.941507\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0049 seconds\n",
            "Initial gradient norm: 166.829762\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 2458.212519\n",
            "Final function value: 2288.622318\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0056 seconds\n",
            "Initial gradient norm: 166.829762\n",
            "Final gradient norm: 0.000000\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 4/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 2237.774407\n",
            "Final function value: 727.226274\n",
            "Number of iterations: 25\n",
            "Runtime: 0.0092 seconds\n",
            "Initial gradient norm: 149.360024\n",
            "Final gradient norm: 0.000005\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 2237.774407\n",
            "Final function value: 893.298840\n",
            "Number of iterations: 16\n",
            "Runtime: 0.0109 seconds\n",
            "Initial gradient norm: 149.360024\n",
            "Final gradient norm: 0.000008\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 2237.774407\n",
            "Final function value: 1232.406530\n",
            "Number of iterations: 25\n",
            "Runtime: 0.0108 seconds\n",
            "Initial gradient norm: 149.360024\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 2237.774407\n",
            "Final function value: 1444.011832\n",
            "Number of iterations: 25\n",
            "Runtime: 0.0127 seconds\n",
            "Initial gradient norm: 149.360024\n",
            "Final gradient norm: 0.000002\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 5/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 1120.884607\n",
            "Final function value: 139.287886\n",
            "Number of iterations: 18\n",
            "Runtime: 0.0067 seconds\n",
            "Initial gradient norm: 134.842711\n",
            "Final gradient norm: 0.000002\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 1120.884607\n",
            "Final function value: 860.484311\n",
            "Number of iterations: 9\n",
            "Runtime: 0.0059 seconds\n",
            "Initial gradient norm: 134.842711\n",
            "Final gradient norm: 0.000009\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 1120.884607\n",
            "Final function value: 947.006077\n",
            "Number of iterations: 8\n",
            "Runtime: 0.0062 seconds\n",
            "Initial gradient norm: 134.842711\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 1120.884607\n",
            "Final function value: 1030.542970\n",
            "Number of iterations: 10\n",
            "Runtime: 0.0059 seconds\n",
            "Initial gradient norm: 134.842711\n",
            "Final gradient norm: 0.000028\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimension: 8\n",
            "\n",
            "Experiment 1/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 4167.402820\n",
            "Final function value: 1681.098298\n",
            "Number of iterations: 29\n",
            "Runtime: 0.0093 seconds\n",
            "Initial gradient norm: 116.638058\n",
            "Final gradient norm: 0.000005\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 4167.402820\n",
            "Final function value: 2911.229149\n",
            "Number of iterations: 23\n",
            "Runtime: 0.0075 seconds\n",
            "Initial gradient norm: 116.638058\n",
            "Final gradient norm: 0.000004\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 4167.402820\n",
            "Final function value: 3061.339751\n",
            "Number of iterations: 20\n",
            "Runtime: 0.0089 seconds\n",
            "Initial gradient norm: 116.638058\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 4167.402820\n",
            "Final function value: 3160.771230\n",
            "Number of iterations: 30\n",
            "Runtime: 0.0117 seconds\n",
            "Initial gradient norm: 116.638058\n",
            "Final gradient norm: 0.000000\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 2/5\n",
            "\n",
            "First-order methods:\n",
            "Running gradient_descent... Done\n",
            "Running sgd... Done\n",
            "Running momentum... Done\n",
            "Running rmsprop... Done\n",
            "Running adam... Done\n",
            "\n",
            "Second-order methods:\n",
            "Running BFGS... \n",
            "BFGS Results:\n",
            "Initial function value: 2918.387245\n",
            "Final function value: 1496.170355\n",
            "Number of iterations: 21\n",
            "Runtime: 0.0086 seconds\n",
            "Initial gradient norm: 134.077206\n",
            "Final gradient norm: 0.000010\n",
            "Done\n",
            "Running newton-cg... \n",
            "newton-cg Results:\n",
            "Initial function value: 2918.387245\n",
            "Final function value: 2609.882782\n",
            "Number of iterations: 14\n",
            "Runtime: 0.0056 seconds\n",
            "Initial gradient norm: 134.077206\n",
            "Final gradient norm: 0.000000\n",
            "Done\n",
            "Running trust-exact... \n",
            "trust-exact Results:\n",
            "Initial function value: 2918.387245\n",
            "Final function value: 2769.909558\n",
            "Number of iterations: 7\n",
            "Runtime: 0.0067 seconds\n",
            "Initial gradient norm: 134.077206\n",
            "Final gradient norm: 0.000084\n",
            "Done\n",
            "Running trust-krylov... \n",
            "trust-krylov Results:\n",
            "Initial function value: 2918.387245\n",
            "Final function value: 2769.909558\n",
            "Number of iterations: 12\n",
            "Runtime: 0.0063 seconds\n",
            "Initial gradient norm: 134.077206\n",
            "Final gradient norm: 0.000015\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HGcBPbFubQFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}