{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiRTKba4bQw1"
      },
      "source": [
        "# INITIAL TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KkKYrlBwFZs",
        "outputId": "8676340f-ebdb-42b6-cdab-835c9fbe4dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.11)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.2.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.2.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.8.30)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn\n",
        "!pip install gitpython PyGithub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNwklgZfF1sA"
      },
      "source": [
        "# OptimizationResult, First-order Optimisers and Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07O9nE0oHegS"
      },
      "outputs": [],
      "source": [
        "class OptimizationResult:\n",
        "    \"\"\"Enhanced optimization result storage\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        self.x_final = kwargs.get('x_final')\n",
        "        self.f_final = kwargs.get('f_final')\n",
        "        self.success = kwargs.get('success')\n",
        "        self.iterations = kwargs.get('iterations')\n",
        "        self.runtime = kwargs.get('runtime')\n",
        "        self.path = kwargs.get('path', [])\n",
        "        self.f_path = kwargs.get('f_path', [])\n",
        "        self.grad_norm_path = kwargs.get('grad_norm_path', [])\n",
        "        self.grad_cosine_sim_path = kwargs.get('grad_cosine_sim_path', [])  # New\n",
        "        self.grad_angle_path = kwargs.get('grad_angle_path', [])            # New\n",
        "        self.timestamps = kwargs.get('timestamps', [])\n",
        "        self.method = kwargs.get('method')\n",
        "        self.dimension = kwargs.get('dimension')\n",
        "        self.function_name = kwargs.get('function_name')\n",
        "        self.x_initial = kwargs.get('x_initial')\n",
        "        self.f_initial = kwargs.get('f_initial')\n",
        "        self.grad_initial = kwargs.get('grad_initial')\n",
        "        self.grad_final = kwargs.get('grad_final')\n",
        "        self.step_sizes = kwargs.get('step_sizes', [])\n",
        "        self.improvements = kwargs.get('improvements', [])\n",
        "        self.best_so_far = kwargs.get('best_so_far', [])\n",
        "        self.relative_improvements = kwargs.get('relative_improvements', [])\n",
        "        self.distance_to_minimum_path = kwargs.get('distance_to_minimum_path', [])\n",
        "        self.final_distance_to_minimum = None\n",
        "        self.initial_distance_to_minimum = None  # Add this\n",
        "\n",
        "        # Get global minimum using the function name and dimension we already have\n",
        "        x_min, f_min = TestFunctions.get_global_minimum(self.function_name, self.dimension)\n",
        "        if x_min is not None and f_min is not None and self.x_initial is not None:\n",
        "            # Calculate initial distance only if we have both x_initial and x_min\n",
        "            self.initial_distance_to_minimum = np.linalg.norm(self.x_initial - x_min)\n",
        "\n",
        "            # Use the final value from path if available, otherwise calculate\n",
        "            if self.distance_to_minimum_path:\n",
        "                self.final_distance_to_minimum = self.distance_to_minimum_path[-1]\n",
        "            elif self.x_final is not None:\n",
        "                self.final_distance_to_minimum = np.linalg.norm(self.x_final - x_min)\n",
        "            self.f_error = abs(self.f_final - f_min) if self.f_final is not None else None\n",
        "        else:\n",
        "            self.initial_distance_to_minimum = None  # Add this\n",
        "            self.final_distance_to_minimum = None\n",
        "            self.f_error = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwVvDUwtaQgE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, List, Dict, Optional, Tuple\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "class TestFunctions:\n",
        "    \"\"\"Test functions that work with any dimension\"\"\"\n",
        "    @staticmethod\n",
        "    def get_global_minimum(func_name: str, dimension: int = 2) -> tuple:\n",
        "        \"\"\"Get global minimum for a given function and dimension\"\"\"\n",
        "        global_minima = {\n",
        "            'ackley': (np.zeros(dimension), 0.0),\n",
        "            'rastrigin': (np.zeros(dimension), 0.0),\n",
        "            'rosenbrock': (np.ones(dimension), 0.0),\n",
        "            'sphere': (np.zeros(dimension), 0.0),\n",
        "            'schwefel': (420.9687 * np.ones(dimension), 0.0),  # Add this\n",
        "            'sum_squares': (np.zeros(dimension), 0.0),         # Add this\n",
        "            'michalewicz': (None, None),  # Varies with dimension\n",
        "        }\n",
        "        return global_minima.get(func_name, (None, None))\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley(x: np.ndarray) -> float:\n",
        "        \"\"\"Ackley function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "        return (-20 * np.exp(-0.2 * np.sqrt(sum_sq / n))\n",
        "                - np.exp(sum_cos / n)\n",
        "                + 20 + np.e)\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Ackley function\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "\n",
        "        term1 = (20 * 0.2 / np.sqrt(n * sum_sq)) * np.exp(-0.2 * np.sqrt(sum_sq / n)) * x\n",
        "        term2 = (2 * np.pi / n) * np.exp(sum_cos / n) * np.sin(2 * np.pi * x)\n",
        "        return term1 + term2\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Ackley Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.ackley_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin(x: np.ndarray) -> float:\n",
        "        \"\"\"Rastrigin function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        return 10 * n + np.sum(x**2 - 10 * np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rastrigin function\"\"\"\n",
        "        return 2 * x + 20 * np.pi * np.sin(2 * np.pi * x)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Rastrigin function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n) + 40 * np.pi**2 * np.diag(np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel(x: np.ndarray) -> float:\n",
        "        \"\"\"Schwefel function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        return 418.9829 * n - np.sum(x * np.sin(np.sqrt(np.abs(x))))\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Schwefel function\"\"\"\n",
        "        sqrt_abs_x = np.sqrt(np.abs(x))\n",
        "        term1 = np.sin(sqrt_abs_x)\n",
        "        term2 = x * np.cos(sqrt_abs_x) / (2 * sqrt_abs_x)\n",
        "        return -(term1 + term2)\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Schwefel Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.schwefel_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere(x: np.ndarray) -> float:\n",
        "        \"\"\"Sphere function for n dimensions\"\"\"\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Sphere function\"\"\"\n",
        "        return 2 * x\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Sphere function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n)\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares(x: np.ndarray) -> float:\n",
        "        \"\"\"Sum squares function for n dimensions\"\"\"\n",
        "        return np.sum((np.arange(1, len(x) + 1) * x**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Sum squares function\"\"\"\n",
        "        return 2 * np.arange(1, len(x) + 1) * x\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Sum squares function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.diag(np.arange(1, n + 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock(x: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Rosenbrock function for n dimensions\n",
        "        Domain: Usually [-5, 10] or [-2.048, 2.048] per dimension\n",
        "        Global minimum: f(x) = 0 at x = [1, 1, ..., 1]\n",
        "        \"\"\"\n",
        "        return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rosenbrock function\"\"\"\n",
        "        n = len(x)\n",
        "        grad = np.zeros(n)\n",
        "        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "        grad[-1] = 200 * (x[-1] - x[-2]**2)\n",
        "        if n > 2:\n",
        "            grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
        "        return grad\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Rosenbrock Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.rosenbrock_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "    @staticmethod\n",
        "    def michalewicz(x: np.ndarray) -> float:\n",
        "        \"\"\"Michalewicz function for n dimensions\"\"\"\n",
        "        i = np.arange(1, len(x) + 1)\n",
        "        return -np.sum(np.sin(x) * (np.sin(i * x**2 / np.pi))**(2 * 10))\n",
        "\n",
        "    @staticmethod\n",
        "    def michalewicz_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Michalewicz function with numerical stability\"\"\"\n",
        "        n = len(x)\n",
        "        i = np.arange(1, n + 1)\n",
        "\n",
        "        # Add small epsilon to avoid division by zero\n",
        "        eps = 1e-10\n",
        "\n",
        "        # Compute terms separately for better numerical stability\n",
        "        sin_x = np.sin(x)\n",
        "        cos_x = np.cos(x)\n",
        "        sin_ix2 = np.sin(i * x**2 / np.pi)\n",
        "        cos_ix2 = np.cos(i * x**2 / np.pi)\n",
        "\n",
        "        # Compute the power term with clipping to avoid numerical issues\n",
        "        power_term = np.clip(sin_ix2, -1 + eps, 1 - eps)**(2 * 10 - 1)\n",
        "\n",
        "        term1 = -cos_x * (sin_ix2)**(2 * 10)\n",
        "        term2 = -sin_x * (2 * 10) * power_term * cos_ix2 * (2 * i * x / np.pi)\n",
        "\n",
        "        return -(term1 + term2)\n",
        "\n",
        "    @staticmethod\n",
        "    def michalewicz_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Michalewicz Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.michalewicz_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGlrjFy2pnuJ"
      },
      "outputs": [],
      "source": [
        "class GradientDescent:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            x = x - self.learning_rate * g\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum_path  # Changed to path\n",
        "        }\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, noise_scale=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            # Add stochastic noise\n",
        "            noise = np.random.normal(0, self.noise_scale, size=x.shape)\n",
        "            g = g + noise\n",
        "            x = x - self.learning_rate * g\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum_path  # Changed to path\n",
        "        }\n",
        "\n",
        "class SGDMomentum:\n",
        "    \"\"\"SGD with momentum optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9, noise_scale=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)  # Initialize velocity\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            # Add stochastic noise\n",
        "            noise = np.random.normal(0, self.noise_scale, size=x.shape)\n",
        "            g = g + noise\n",
        "\n",
        "            # Update with momentum\n",
        "            v = self.momentum * v - self.learning_rate * g\n",
        "            x = x + v\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum_path  # Changed to path\n",
        "        }\n",
        "\n",
        "class MomentumGD:\n",
        "    \"\"\"Gradient Descent with Momentum\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            v = self.momentum * v - self.learning_rate * g\n",
        "            x = x + v\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum_path  # Changed to path\n",
        "        }\n",
        "\n",
        "class RMSprop:\n",
        "    \"\"\"RMSprop optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, decay_rate=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            v = self.decay_rate * v + (1 - self.decay_rate) * g**2\n",
        "            x = x - self.learning_rate * g / (np.sqrt(v) + self.epsilon)\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum_path  # Changed to path\n",
        "        }\n",
        "\n",
        "class Adam:\n",
        "    \"\"\"Adam optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        m = np.zeros_like(x)\n",
        "        v = np.zeros_like(x)\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            m = self.beta1 * m + (1 - self.beta1) * g\n",
        "            v = self.beta2 * v + (1 - self.beta2) * g**2\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = m / (1 - self.beta1**(i + 1))\n",
        "            v_hat = v / (1 - self.beta2**(i + 1))\n",
        "\n",
        "            x = x - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum_path  # Changed to path\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nny9ft1FGJQv"
      },
      "source": [
        "# Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-a_OSOTFlC4"
      },
      "outputs": [],
      "source": [
        "class Visualizer:\n",
        "    @staticmethod\n",
        "    def plot_with_cleanup(plot_func):\n",
        "        \"\"\"Decorator to ensure proper figure cleanup\"\"\"\n",
        "        def wrapper(*args, **kwargs):\n",
        "            try:\n",
        "                return plot_func(*args, **kwargs)\n",
        "            finally:\n",
        "                plt.close('all')  # Ensure all figures are closed\n",
        "        return wrapper\n",
        "\n",
        "    @staticmethod\n",
        "    @plot_with_cleanup\n",
        "    def plot_convergence(results: Dict[str, List[OptimizationResult]], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot convergence with error bands showing variation across runs\"\"\"\n",
        "\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            plt.figure(figsize=(20, 8))\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "            colors = plt.cm.tab20(np.linspace(0, 1, len(results)))\n",
        "\n",
        "            _, f_min = TestFunctions.get_global_minimum(function_name)\n",
        "            f_min_text = f\"(Global min: {f_min})\" if f_min is not None else \"\"\n",
        "\n",
        "            # Plot for each method\n",
        "            for (method, method_results), color in zip(results.items(), colors):\n",
        "                try:\n",
        "                    if not isinstance(method_results, list):\n",
        "                        method_results = [method_results]  # Convert single result to list if necessary\n",
        "\n",
        "                    # Get max length of trajectories\n",
        "                    max_len = max(len(r.f_path) for r in method_results)\n",
        "\n",
        "                    # Initialize arrays for storing values\n",
        "                    f_values = np.full((len(method_results), max_len), np.nan)\n",
        "                    grad_norms = np.full((len(method_results), max_len), np.nan)\n",
        "\n",
        "                    # Fill arrays with available data\n",
        "                    for i, result in enumerate(method_results):\n",
        "                        f_values[i, :len(result.f_path)] = result.f_path\n",
        "                        grad_norms[i, :len(result.grad_norm_path)] = result.grad_norm_path\n",
        "\n",
        "                    # Calculate statistics\n",
        "                    f_mean = np.nanmean(f_values, axis=0)\n",
        "                    f_std = np.nanstd(f_values, axis=0)\n",
        "                    grad_mean = np.nanmean(grad_norms, axis=0)\n",
        "                    grad_std = np.nanstd(grad_norms, axis=0)\n",
        "\n",
        "                    # Plot with error bands\n",
        "                    x = np.arange(max_len)\n",
        "                    ax1.semilogy(x, f_mean, label=f\"{method}\", color=color)\n",
        "                    ax1.fill_between(x, f_mean - f_std, f_mean + f_std, alpha=0.2, color=color)\n",
        "\n",
        "                    ax2.semilogy(x, grad_mean, label=f\"{method}\", color=color)\n",
        "                    ax2.fill_between(x, grad_mean - grad_std, grad_mean + grad_std, alpha=0.2, color=color)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not plot method {method}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            # Set labels and titles\n",
        "            ax1.set_xlabel('Iteration')\n",
        "            ax1.set_ylabel('Function Value (log scale)')\n",
        "            ax1.set_title(f'Function Value Convergence {f_min_text}')\n",
        "            ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            ax1.grid(True)\n",
        "\n",
        "            ax2.set_xlabel('Iteration')\n",
        "            ax2.set_ylabel('Gradient Norm (log scale)')\n",
        "            ax2.set_title('Gradient Norm Convergence')\n",
        "            ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            ax2.grid(True)\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "            plt.savefig(os.path.join(save_dir, function_name), dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not create convergence plot: {str(e)}\")\n",
        "\n",
        "    @staticmethod\n",
        "    @plot_with_cleanup\n",
        "    def plot_2d_trajectory(f: Callable, result: OptimizationResult, save_dir: str,\n",
        "                          experiment_num: int = None, equal_aspect: bool = True):\n",
        "        \"\"\"Plot optimization trajectory for 2D problems\"\"\"\n",
        "\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        if result.dimension != 2:\n",
        "            return\n",
        "\n",
        "        # Even more reduced resolution for better performance\n",
        "        grid_size = 50\n",
        "\n",
        "        for show_global_min in [True, False]:  # Create both variants\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            # Get path bounds and include (0,0)\n",
        "            path = np.array(result.path)\n",
        "            x_min_traj = min(float(path[:, 0].min()), 0)\n",
        "            x_max_traj = max(float(path[:, 0].max()), 0)\n",
        "            y_min_traj = min(float(path[:, 1].min()), 0)\n",
        "            y_max_traj = max(float(path[:, 1].max()), 0)\n",
        "\n",
        "            # Add margin to bounds\n",
        "            margin = 0.1\n",
        "            x_range = x_max_traj - x_min_traj\n",
        "            y_range = y_max_traj - y_min_traj\n",
        "\n",
        "            if equal_aspect:\n",
        "                max_range = max(x_range, y_range)\n",
        "                x_center = (x_max_traj + x_min_traj) / 2\n",
        "                y_center = (y_max_traj + y_min_traj) / 2\n",
        "                x_min_traj = x_center - max_range/2\n",
        "                x_max_traj = x_center + max_range/2\n",
        "                y_min_traj = y_center - max_range/2\n",
        "                y_max_traj = y_center + max_range/2\n",
        "                x_range = y_range = max_range\n",
        "\n",
        "            plot_x_min = x_min_traj - margin * x_range\n",
        "            plot_x_max = x_max_traj + margin * x_range\n",
        "            plot_y_min = y_min_traj - margin * y_range\n",
        "            plot_y_max = y_max_traj + margin * y_range\n",
        "\n",
        "            # Create contour plot with reduced resolution\n",
        "            x = np.linspace(plot_x_min, plot_x_max, grid_size)\n",
        "            y = np.linspace(plot_y_min, plot_y_max, grid_size)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "\n",
        "            # Compute Z values\n",
        "            points = np.column_stack((X.ravel(), Y.ravel()))\n",
        "            Z = np.array([f(point) for point in points]).reshape(X.shape)\n",
        "\n",
        "            # Reduced number of contour levels\n",
        "            global_max = float(Z.max())\n",
        "            global_levels = np.linspace(0, global_max, 15)\n",
        "\n",
        "            # Plot contours\n",
        "            contour = plt.contour(X, Y, Z, levels=global_levels, cmap='viridis', alpha=0.7)\n",
        "            plt.colorbar(contour, label='Function Value')\n",
        "\n",
        "            # Plot trajectory\n",
        "            plt.plot(path[:, 0], path[:, 1], 'r.-', label='Optimization Path',\n",
        "                    linewidth=1, markersize=2, zorder=5)\n",
        "            plt.plot(path[0, 0], path[0, 1], 'go', label='Start',\n",
        "                    markersize=8, zorder=6)\n",
        "            plt.plot(path[-1, 0], path[-1, 1], 'ro', label='End',\n",
        "                    markersize=8, zorder=6)\n",
        "\n",
        "            # Only plot global minimum in the first variant\n",
        "            if show_global_min:\n",
        "                x_min, f_min = TestFunctions.get_global_minimum(result.function_name)\n",
        "                if x_min is not None:\n",
        "                    plt.plot(x_min[0], x_min[1], 'k*', label='Global Minimum',\n",
        "                            markersize=10, zorder=6)\n",
        "\n",
        "            plt.xlim(float(plot_x_min), float(plot_x_max))\n",
        "            plt.ylim(float(plot_y_min), float(plot_y_max))\n",
        "\n",
        "            if equal_aspect:\n",
        "                plt.gca().set_aspect('equal')\n",
        "\n",
        "            plt.grid(True)\n",
        "            plt.title(f'{result.function_name} - {result.method}\\n'\n",
        "                    f'Final value: {result.f_final:.6f}\\n'\n",
        "                    f'Iterations: {result.iterations}')\n",
        "            plt.xlabel('x₁')\n",
        "            plt.ylabel('x₂')\n",
        "            plt.legend()\n",
        "\n",
        "            # Include experiment number and variant in filename\n",
        "            experiment_suffix = f'_exp{experiment_num}' if experiment_num is not None else ''\n",
        "            variant_suffix = '_with_global_min' if show_global_min else '_path_only'\n",
        "            aspect_suffix = '_equal_aspect' if equal_aspect else ''\n",
        "            filename = f'trajectory_{result.function_name}_{result.method}{experiment_suffix}{variant_suffix}{aspect_suffix}.png'\n",
        "\n",
        "            plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    @plot_with_cleanup\n",
        "    def plot_metric_matrix(results: Dict[str, List[OptimizationResult]], save_dir: str,\n",
        "                          metric_name: str, function_name: str, y_label: str,\n",
        "                          log_scale: bool = True):\n",
        "        \"\"\"\n",
        "        Plot any optimization metric in a 2x2 matrix layout.\n",
        "\n",
        "        Args:\n",
        "            results: Dictionary of results by method\n",
        "            save_dir: Directory to save the plot\n",
        "            metric_name: Name of the metric to plot (e.g., 'grad_norm_path', 'best_so_far')\n",
        "            function_name: Name of the optimization function\n",
        "            y_label: Label for y-axis\n",
        "            log_scale: Whether to use log scale for y-axis\n",
        "        \"\"\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # Create figure with 2x2 subplot layout\n",
        "            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 20))\n",
        "\n",
        "            # Method groupings\n",
        "            first_order = ['gradient_descent', 'sgd', 'sgd_momentum', 'momentum', 'rmsprop', 'adam']\n",
        "            second_order = ['BFGS', 'newton-cg', 'trust-exact', 'trust-krylov']\n",
        "\n",
        "            # Colors for each group\n",
        "            first_order_colors = plt.cm.tab10(np.linspace(0, 1, len(first_order)))\n",
        "            second_order_colors = plt.cm.Set2(np.linspace(0, 1, len(second_order)))\n",
        "\n",
        "            def plot_group(methods, colors, ax_mean, ax_std, results):\n",
        "                \"\"\"Plot a group of methods on given axes\"\"\"\n",
        "                for method, color in zip(methods, colors):\n",
        "                    if method in results:\n",
        "                        method_results = results[method]\n",
        "                        if not isinstance(method_results, list):\n",
        "                            method_results = [method_results]\n",
        "\n",
        "                        # Get the metric data from each result\n",
        "                        metric_data = [getattr(r, metric_name) for r in method_results]\n",
        "                        max_len = max(len(d) for d in metric_data)\n",
        "\n",
        "                        # Create array and fill with data\n",
        "                        values = np.full((len(method_results), max_len), np.nan)\n",
        "                        for i, data in enumerate(metric_data):\n",
        "                            values[i, :len(data)] = data\n",
        "\n",
        "                        # Calculate statistics\n",
        "                        mean_values = np.nanmean(values, axis=0)\n",
        "                        std_values = np.nanstd(values, axis=0)\n",
        "                        x = np.arange(max_len)\n",
        "\n",
        "                        # Plot on mean-only axis\n",
        "                        if log_scale:\n",
        "                            ax_mean.semilogy(x, mean_values, label=method, color=color, linewidth=2)\n",
        "                        else:\n",
        "                            ax_mean.plot(x, mean_values, label=method, color=color, linewidth=2)\n",
        "\n",
        "                        # Plot on mean±std axis\n",
        "                        if log_scale:\n",
        "                            ax_std.semilogy(x, mean_values, label=method, color=color, linewidth=2)\n",
        "                        else:\n",
        "                            ax_std.plot(x, mean_values, label=method, color=color, linewidth=2)\n",
        "\n",
        "                        if len(method_results) > 1:  # Only show std if we have multiple runs\n",
        "                            ax_std.fill_between(x, mean_values - std_values, mean_values + std_values,\n",
        "                                            alpha=0.15, color=color)\n",
        "\n",
        "            # Plot each group\n",
        "            plot_group(first_order, first_order_colors, ax1, ax3, results)\n",
        "            plot_group(second_order, second_order_colors, ax2, ax4, results)\n",
        "\n",
        "            # Titles for each subplot\n",
        "            titles = [\n",
        "                'First-Order Methods (Mean)',\n",
        "                'Second-Order Methods (Mean)',\n",
        "                'First-Order Methods (Mean ± Std)',\n",
        "                'Second-Order Methods (Mean ± Std)'\n",
        "            ]\n",
        "\n",
        "            # Style all subplots\n",
        "            for ax, title in zip([ax1, ax2, ax3, ax4], titles):\n",
        "                ax.set_xlabel('Iteration')\n",
        "                ax.set_ylabel(y_label)\n",
        "                ax.set_title(title)\n",
        "                ax.grid(True, alpha=0.3)\n",
        "                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "            plt.suptitle(f'{function_name} - {metric_name}', y=1.02, fontsize=16)\n",
        "            plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
        "\n",
        "            # Save the plot\n",
        "            filename = f'{metric_name}_{function_name}_matrix.png'\n",
        "            plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not create {metric_name} plot: {str(e)}\")\n",
        "            plt.close()\n",
        "\n",
        "    # Updated Visualizer class method to plot all metrics\n",
        "    @staticmethod\n",
        "    @plot_with_cleanup\n",
        "    def plot_all_metrics(results: Dict[str, List[OptimizationResult]], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot all optimization metrics in consistent 2x2 matrix layouts\"\"\"\n",
        "        metrics_config = [\n",
        "            ('grad_norm_path', 'Gradient Norm', True),\n",
        "            ('best_so_far', 'Best Value So Far', True),\n",
        "            ('distance_to_minimum_path', 'Distance to Minimum', True),\n",
        "            ('improvements', 'Improvement (%)', False),\n",
        "            ('relative_improvements', 'Relative Improvement (%)', False),\n",
        "            ('step_sizes', 'Step Size', True)\n",
        "        ]\n",
        "\n",
        "        # Create subdirectories for each metric type\n",
        "        for metric_name, label, log_scale in metrics_config:\n",
        "            metric_dir = os.path.join(save_dir, metric_name)\n",
        "            os.makedirs(metric_dir, exist_ok=True)\n",
        "            Visualizer.plot_metric_matrix(results, metric_dir, metric_name, function_name, label, log_scale)\n",
        "\n",
        "    @staticmethod\n",
        "    @plot_with_cleanup\n",
        "    def plot_step_metrics(results: Dict[str, List[OptimizationResult]], save_dir: str,\n",
        "                         function_name: str, experiment_num: Optional[int] = None):\n",
        "        \"\"\"Plot step-related metrics with error bands\"\"\"\n",
        "        metrics = [\n",
        "            ('step_sizes', 'Step Size', 'Step Size'),\n",
        "            ('improvements', 'Improvement per Step (%)', 'Improvement'),\n",
        "            ('distance_to_minimum_path', 'Distance to Global Minimum', 'Distance'),  # Changed from distance_to_minimum\n",
        "            ('best_so_far', 'Best Value So Far', 'Value'),\n",
        "            ('relative_improvements', 'Relative Improvement (%)', 'Improvement')\n",
        "        ]\n",
        "\n",
        "        for metric_name, ylabel, title_suffix in metrics:\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            colors = plt.cm.tab20(np.linspace(0, 1, len(results)))\n",
        "\n",
        "            for (method, method_results), color in zip(results.items(), colors):\n",
        "                try:\n",
        "                    # Get valid data for this metric\n",
        "                    valid_data = []\n",
        "                    for result in method_results:\n",
        "                        data = getattr(result, metric_name, None)\n",
        "                        if data is not None and len(data) > 0:\n",
        "                            valid_data.append(data)\n",
        "\n",
        "                    if not valid_data:\n",
        "                        continue\n",
        "\n",
        "                    # Get max length for this metric\n",
        "                    max_len = max(len(data) for data in valid_data)\n",
        "\n",
        "                    # Initialize array\n",
        "                    values = np.full((len(valid_data), max_len), np.nan)\n",
        "\n",
        "                    # Fill array\n",
        "                    for i, data in enumerate(valid_data):\n",
        "                        values[i, :len(data)] = data\n",
        "\n",
        "                    # Calculate statistics\n",
        "                    mean_values = np.nanmean(values, axis=0)\n",
        "                    std_values = np.nanstd(values, axis=0)\n",
        "\n",
        "                    # Plot with error bands\n",
        "                    x = np.arange(max_len)\n",
        "                    plt.plot(x, mean_values, label=method, color=color)\n",
        "                    plt.fill_between(x, mean_values - std_values, mean_values + std_values,\n",
        "                                   alpha=0.2, color=color)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not plot {metric_name} for {method}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if plt.gca().get_lines():  # Only add labels if there are plots\n",
        "                plt.xlabel('Iteration')\n",
        "                plt.ylabel(ylabel)\n",
        "                plt.title(f'{function_name} - {title_suffix}')\n",
        "                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "                plt.grid(True)\n",
        "\n",
        "                # Create metric-specific directories\n",
        "                metric_dir = os.path.join(save_dir, metric_name.replace('_', ''))\n",
        "                os.makedirs(metric_dir, exist_ok=True)\n",
        "\n",
        "                # Save with experiment number if provided\n",
        "                suffix = f'_exp_{experiment_num}' if experiment_num is not None else '_summary'\n",
        "                filename = f'{metric_name}_{function_name}{suffix}.png'\n",
        "                plt.savefig(os.path.join(metric_dir, filename), dpi=300, bbox_inches='tight')\n",
        "\n",
        "            plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngaQ15naF6yJ"
      },
      "source": [
        "# Metrics and Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVdCMRiQTPfg"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_metrics(grad_current: np.ndarray, grad_previous: np.ndarray) -> Tuple[float, float]:\n",
        "    \"\"\"Compute cosine similarity and angle between two gradient vectors\"\"\"\n",
        "    if np.all(grad_previous == 0) or np.all(grad_current == 0):\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    cosine_sim = np.dot(grad_current, grad_previous) / (np.linalg.norm(grad_current) * np.linalg.norm(grad_previous))\n",
        "    # Clip to handle numerical errors\n",
        "    cosine_sim = np.clip(cosine_sim, -1.0, 1.0)\n",
        "    angle = np.arccos(cosine_sim) * 180 / np.pi\n",
        "\n",
        "    return cosine_sim, angle\n",
        "\n",
        "# Helper function to compute summary metrics\n",
        "def compute_summary_metrics(results: List[OptimizationResult]) -> Dict:\n",
        "    \"\"\"Compute summary statistics for new metrics\"\"\"\n",
        "    summary = {}\n",
        "\n",
        "    # Step size statistics\n",
        "    step_sizes = [np.mean(r.step_sizes) for r in results if hasattr(r, 'step_sizes')]\n",
        "    if step_sizes:\n",
        "        summary.update({\n",
        "            'mean_step_size': np.mean(step_sizes),\n",
        "            'std_step_size': np.std(step_sizes)\n",
        "        })\n",
        "\n",
        "    # Improvement statistics\n",
        "    improvements = [np.mean(r.improvements) for r in results if hasattr(r, 'improvements')]\n",
        "    if improvements:\n",
        "        summary.update({\n",
        "            'mean_improvement_per_step': np.mean(improvements),\n",
        "            'std_improvement_per_step': np.std(improvements)\n",
        "        })\n",
        "\n",
        "    # Plateau statistics\n",
        "    plateau_metrics = [StepMetricsCalculator.compute_plateau_metrics(r.f_path)\n",
        "                      for r in results]\n",
        "    if plateau_metrics:\n",
        "        summary.update({\n",
        "            'mean_plateau_percentage': np.mean([m['plateau_percentage'] for m in plateau_metrics]),\n",
        "            'mean_plateau_length': np.mean([m['max_plateau_length'] for m in plateau_metrics]),\n",
        "            'mean_num_plateaus': np.mean([m['num_plateaus'] for m in plateau_metrics])\n",
        "        })\n",
        "\n",
        "    # Improvement threshold statistics\n",
        "    threshold_metrics = [StepMetricsCalculator.compute_improvement_thresholds(r.f_path)\n",
        "                        for r in results]\n",
        "    if threshold_metrics:\n",
        "        for threshold in [10, 20, 30, 40, 50]:\n",
        "            steps = [m[f\"{threshold}%_improvement_steps\"] for m in threshold_metrics\n",
        "                    if m[f\"{threshold}%_improvement_steps\"] is not None]\n",
        "            if steps:\n",
        "                summary.update({\n",
        "                    f'mean_steps_to_{threshold}%_improvement': np.mean(steps),\n",
        "                    f'std_steps_to_{threshold}%_improvement': np.std(steps)\n",
        "                })\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjbLokFAHBgt"
      },
      "outputs": [],
      "source": [
        "# New helper class for computing step metrics\n",
        "class StepMetricsCalculator:\n",
        "    \"\"\"Helper class for computing various step-based metrics\"\"\"\n",
        "    @staticmethod\n",
        "    def compute_step_size(x_current: np.ndarray, x_previous: np.ndarray) -> float:\n",
        "        \"\"\"Compute Euclidean distance between consecutive steps\"\"\"\n",
        "        return np.linalg.norm(x_current - x_previous)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_improvement(f_current: float, f_previous: float) -> float:\n",
        "        \"\"\"Compute relative improvement between steps\"\"\"\n",
        "        if f_previous == 0:\n",
        "            return 0.0\n",
        "        return (f_previous - f_current) / abs(f_previous) * 100\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_plateau_metrics(f_path: List[float], threshold: float = 1.0) -> Dict:\n",
        "        \"\"\"Compute plateau-related metrics\"\"\"\n",
        "        improvements = np.array([abs((f_path[i] - f_path[i-1])/f_path[i-1]*100)\n",
        "                               for i in range(1, len(f_path))])\n",
        "        plateau_mask = improvements < threshold\n",
        "\n",
        "        # Find plateau sequences\n",
        "        plateau_sequences = []\n",
        "        current_sequence = []\n",
        "        for i, is_plateau in enumerate(plateau_mask):\n",
        "            if is_plateau:\n",
        "                current_sequence.append(i)\n",
        "            elif current_sequence:\n",
        "                plateau_sequences.append(current_sequence)\n",
        "                current_sequence = []\n",
        "        if current_sequence:\n",
        "            plateau_sequences.append(current_sequence)\n",
        "\n",
        "        return {\n",
        "            'total_plateau_steps': np.sum(plateau_mask),\n",
        "            'plateau_percentage': np.mean(plateau_mask) * 100 if len(plateau_mask) > 0 else 0,\n",
        "            'max_plateau_length': max([len(seq) for seq in plateau_sequences]) if plateau_sequences else 0,\n",
        "            'num_plateaus': len(plateau_sequences),\n",
        "            'plateau_sequences': plateau_sequences\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_improvement_thresholds(f_path: List[float],\n",
        "                                     thresholds: List[float] = [10, 20, 30, 40, 50]) -> Dict:\n",
        "        \"\"\"Compute steps required for various improvement thresholds\"\"\"\n",
        "        if not f_path:\n",
        "            return {f\"{t}%_improvement_steps\": None for t in thresholds}\n",
        "\n",
        "        initial_value = f_path[0]\n",
        "        results = {}\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            target_value = initial_value * (1 - threshold/100)\n",
        "            steps = next((i for i, v in enumerate(f_path) if v <= target_value), None)\n",
        "            results[f\"{threshold}%_improvement_steps\"] = steps\n",
        "\n",
        "        return results\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_best_so_far(f_path: List[float]) -> Tuple[List[float], List[float]]:\n",
        "        \"\"\"Compute best value so far and relative improvement from initial\"\"\"\n",
        "        if not f_path:\n",
        "            return [], []\n",
        "\n",
        "        best_so_far = []\n",
        "        relative_improvement = []\n",
        "        current_best = float('inf')\n",
        "        initial_value = f_path[0]\n",
        "\n",
        "        for value in f_path:\n",
        "            current_best = min(current_best, value)\n",
        "            best_so_far.append(current_best)\n",
        "            rel_imp = (initial_value - current_best) / abs(initial_value) * 100\n",
        "            relative_improvement.append(rel_imp)\n",
        "\n",
        "        return best_so_far, relative_improvement\n",
        "\n",
        "\n",
        "# Enhance StepLogger class\n",
        "class StepLogger:\n",
        "    \"\"\"Enhanced step logger with additional metrics\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all logging arrays\"\"\"\n",
        "        # Existing attributes\n",
        "        self.path = []\n",
        "        self.f_path = []\n",
        "        self.grad_norm_path = []\n",
        "        self.grad_cosine_sim_path = []\n",
        "        self.grad_angle_path = []\n",
        "        self.timestamps = []\n",
        "        self.start_time = time.time()\n",
        "        self.previous_grad = None\n",
        "\n",
        "        # New attributes\n",
        "        self.step_sizes = []\n",
        "        self.improvements = []\n",
        "        self.best_so_far = []\n",
        "        self.relative_improvements = []\n",
        "        self.distance_to_minimum_path = []  # Initialize this instead of distance_to_minimum\n",
        "\n",
        "    def log_iteration(self, x: np.ndarray, f: float, grad: np.ndarray,\n",
        "                    global_minimum: Optional[np.ndarray] = None):\n",
        "        \"\"\"Enhanced logging with better None handling\"\"\"\n",
        "        # Existing logging\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "        if self.previous_grad is not None:\n",
        "            cosine_sim, angle = compute_gradient_metrics(grad, self.previous_grad)\n",
        "            self.grad_cosine_sim_path.append(cosine_sim)\n",
        "            self.grad_angle_path.append(angle)\n",
        "        else:\n",
        "            self.grad_cosine_sim_path.append(0.0)\n",
        "            self.grad_angle_path.append(0.0)\n",
        "\n",
        "        # New metrics\n",
        "        if len(self.path) > 0:\n",
        "            step_size = StepMetricsCalculator.compute_step_size(x, self.path[-1])\n",
        "            improvement = StepMetricsCalculator.compute_improvement(f, self.f_path[-1])\n",
        "            self.step_sizes.append(step_size)\n",
        "            self.improvements.append(improvement)\n",
        "        else:\n",
        "            self.step_sizes.append(0.0)\n",
        "            self.improvements.append(0.0)\n",
        "\n",
        "        # Update best so far and relative improvement\n",
        "        if not self.f_path:\n",
        "            self.best_so_far.append(f)\n",
        "            self.relative_improvements.append(0.0)\n",
        "        else:\n",
        "            self.best_so_far.append(min(f, self.best_so_far[-1]))\n",
        "            rel_imp = (self.f_path[0] - self.best_so_far[-1]) / abs(self.f_path[0]) * 100\n",
        "            self.relative_improvements.append(rel_imp)\n",
        "\n",
        "        # Distance to global minimum if provided\n",
        "        if global_minimum is not None and x is not None:\n",
        "            try:\n",
        "                dist = np.linalg.norm(x - global_minimum)\n",
        "                self.distance_to_minimum_path.append(dist)\n",
        "            except:\n",
        "                self.distance_to_minimum_path.append(None)\n",
        "        else:\n",
        "            self.distance_to_minimum_path.append(None)\n",
        "\n",
        "        # Standard logging\n",
        "        self.path.append(x.copy())\n",
        "        self.f_path.append(f)\n",
        "        self.grad_norm_path.append(grad_norm)\n",
        "        self.timestamps.append(time.time() - self.start_time)\n",
        "        self.previous_grad = grad.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiMJFlqjGPQ1"
      },
      "outputs": [],
      "source": [
        "class OptimizationLogger:\n",
        "    \"\"\"Handles logging of complete optimization experiments\"\"\"\n",
        "    def __init__(self, base_dir: str):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    def log_run(self, result: OptimizationResult, experiment_num: int):\n",
        "        \"\"\"Enhanced log detailed results for a single optimization run\"\"\"\n",
        "        log_dir = os.path.join(\n",
        "            self.base_dir,\n",
        "            result.function_name,\n",
        "            f\"{result.dimension}D\",\n",
        "            'first_order' if result.method in ['gradient_descent', 'sgd', 'sgd_momentum', 'momentum', 'rmsprop', 'adam'] else 'second_order',\n",
        "            result.method,\n",
        "            \"results\"\n",
        "        )\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Get the minimum length of all arrays\n",
        "        min_length = min(\n",
        "            len(result.timestamps),\n",
        "            len(result.f_path),\n",
        "            len(result.grad_norm_path),\n",
        "            len(result.grad_cosine_sim_path),\n",
        "            len(result.grad_angle_path),\n",
        "            len(result.step_sizes),\n",
        "            len(result.improvements),\n",
        "            len(result.best_so_far),\n",
        "            len(result.relative_improvements),\n",
        "            len(result.distance_to_minimum_path) if result.distance_to_minimum_path is not None else float('inf')\n",
        "        )\n",
        "\n",
        "        # Create detailed step-by-step log with truncated arrays\n",
        "        run_data = {\n",
        "            'iteration': range(min_length),\n",
        "            'timestamp': result.timestamps[:min_length],\n",
        "            'function_value': result.f_path[:min_length],\n",
        "            'gradient_norm': result.grad_norm_path[:min_length],\n",
        "            'gradient_cosine_similarity': result.grad_cosine_sim_path[:min_length],\n",
        "            'gradient_angle': result.grad_angle_path[:min_length],\n",
        "            'step_size': result.step_sizes[:min_length],\n",
        "            'improvement': result.improvements[:min_length],\n",
        "            'best_so_far': result.best_so_far[:min_length],\n",
        "            'relative_improvement': result.relative_improvements[:min_length],\n",
        "            'distance_to_minimum': result.distance_to_minimum_path[:min_length] if result.distance_to_minimum_path is not None else None,\n",
        "        }\n",
        "\n",
        "        # Add parameter values for each dimension\n",
        "        for i in range(result.dimension):\n",
        "            run_data[f'x{i+1}'] = [p[i] for p in result.path[:min_length]]\n",
        "\n",
        "        # Save detailed step-by-step log\n",
        "        step_df = pd.DataFrame(run_data)\n",
        "        step_df.to_csv(\n",
        "            os.path.join(log_dir, f'run_{experiment_num}_steps_{self.timestamp}.csv'),\n",
        "            index=False\n",
        "        )\n",
        "\n",
        "        # Calculate window size for segments\n",
        "        window_size = max(len(result.f_path) // 10, 1)\n",
        "\n",
        "        # Calculate segment means with None value handling\n",
        "        segment_means = {\n",
        "            'gradient_norm': [np.mean(result.grad_norm_path[i:i+window_size])\n",
        "                            for i in range(0, len(result.grad_norm_path), window_size)],\n",
        "            'cosine_similarity': [np.mean(result.grad_cosine_sim_path[i:i+window_size])\n",
        "                                for i in range(1, len(result.grad_cosine_sim_path), window_size)],\n",
        "            'gradient_angle': [np.mean(result.grad_angle_path[i:i+window_size])\n",
        "                              for i in range(1, len(result.grad_angle_path), window_size)],\n",
        "            'best_so_far': [np.mean(result.best_so_far[i:i+window_size])\n",
        "                          for i in range(0, len(result.best_so_far), window_size)],\n",
        "            'distance_to_minimum': ([np.mean([x for x in result.distance_to_minimum_path[i:i+window_size] if x is not None])\n",
        "                                  for i in range(0, len(result.distance_to_minimum_path), window_size)]\n",
        "                                  if result.distance_to_minimum_path and any(x is not None for x in result.distance_to_minimum_path)\n",
        "                                  else None),\n",
        "            'improvements': [np.mean(result.improvements[i:i+window_size])\n",
        "                            for i in range(0, len(result.improvements), window_size)],\n",
        "            'relative_improvements': [np.mean(result.relative_improvements[i:i+window_size])\n",
        "                                    for i in range(0, len(result.relative_improvements), window_size)],\n",
        "            'step_sizes': [np.mean(result.step_sizes[i:i+window_size])\n",
        "                          for i in range(0, len(result.step_sizes), window_size)]\n",
        "        }\n",
        "\n",
        "        run_summary = {\n",
        "            'experiment_num': experiment_num,\n",
        "            'initial_value': result.f_initial,\n",
        "            'final_value': result.f_final,\n",
        "            'iterations': result.iterations,\n",
        "            'runtime': result.runtime,\n",
        "            'success': result.success,\n",
        "            'initial_distance_to_minimum': result.initial_distance_to_minimum,\n",
        "            'final_distance_to_minimum': result.final_distance_to_minimum,\n",
        "            'f_error': result.f_error,\n",
        "            'initial_gradient_norm': np.linalg.norm(result.grad_initial),\n",
        "            'final_gradient_norm': np.linalg.norm(result.grad_final),\n",
        "\n",
        "            # Calculate statistics across segments\n",
        "            'mean_gradient_norm': np.mean(segment_means['gradient_norm']) if segment_means['gradient_norm'] else None,\n",
        "            'std_gradient_norm': np.std(segment_means['gradient_norm']) if segment_means['gradient_norm'] else None,\n",
        "\n",
        "            'mean_cosine_similarity': np.mean(segment_means['cosine_similarity']) if segment_means['cosine_similarity'] else None,\n",
        "            'std_cosine_similarity': np.std(segment_means['cosine_similarity']) if segment_means['cosine_similarity'] else None,\n",
        "\n",
        "            'mean_gradient_angle': np.mean(segment_means['gradient_angle']) if segment_means['gradient_angle'] else None,\n",
        "            'std_gradient_angle': np.std(segment_means['gradient_angle']) if segment_means['gradient_angle'] else None,\n",
        "\n",
        "            'mean_best_so_far': np.mean(segment_means['best_so_far']) if segment_means['best_so_far'] else None,\n",
        "            'std_best_so_far': np.std(segment_means['best_so_far']) if segment_means['best_so_far'] else None,\n",
        "\n",
        "            'mean_distance_to_minimum': np.mean(segment_means['distance_to_minimum']) if segment_means['distance_to_minimum'] else None,\n",
        "            'std_distance_to_minimum': np.std(segment_means['distance_to_minimum']) if segment_means['distance_to_minimum'] else None,\n",
        "\n",
        "            'mean_improvement': np.mean(segment_means['improvements']) if segment_means['improvements'] else None,\n",
        "            'std_improvement': np.std(segment_means['improvements']) if segment_means['improvements'] else None,\n",
        "\n",
        "            'mean_relative_improvement': np.mean(segment_means['relative_improvements']) if segment_means['relative_improvements'] else None,\n",
        "            'std_relative_improvement': np.std(segment_means['relative_improvements']) if segment_means['relative_improvements'] else None,\n",
        "\n",
        "            'mean_step_size': np.mean(segment_means['step_sizes']) if segment_means['step_sizes'] else None,\n",
        "            'std_step_size': np.std(segment_means['step_sizes']) if segment_means['step_sizes'] else None\n",
        "        }\n",
        "\n",
        "        # Save run summary\n",
        "        summary_df = pd.DataFrame([run_summary])\n",
        "        summary_path = os.path.join(log_dir, f'run_summaries_{self.timestamp}.csv')\n",
        "\n",
        "        if os.path.exists(summary_path):\n",
        "            summary_df.to_csv(summary_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            summary_df.to_csv(summary_path, index=False)\n",
        "\n",
        "    def create_dimension_summary(self, function_name: str, dimension: int, results: List[OptimizationResult]):\n",
        "        \"\"\"Create summary statistics across all experiments for all methods at a given dimension\"\"\"\n",
        "        # Group results by method first\n",
        "        method_groups = {}\n",
        "        for result in results:\n",
        "            if result.method not in method_groups:\n",
        "                method_groups[result.method] = []\n",
        "            method_groups[result.method].append(result)\n",
        "\n",
        "        summary_data = []\n",
        "\n",
        "        for method, method_results in method_groups.items():\n",
        "            # Calculate means for each run first\n",
        "            run_means = {\n",
        "                'best_so_far': [np.mean(r.best_so_far) for r in method_results if r.best_so_far],\n",
        "                'distance_to_minimum_path': [np.mean(r.distance_to_minimum_path) for r in method_results if r.distance_to_minimum_path is not None],\n",
        "                'improvements': [np.mean(r.improvements) for r in method_results if r.improvements],\n",
        "                'relative_improvements': [np.mean(r.relative_improvements) for r in method_results if r.relative_improvements],\n",
        "                'step_sizes': [np.mean(r.step_sizes) for r in method_results if r.step_sizes],\n",
        "                'gradient_norms': [np.mean(r.grad_norm_path) for r in method_results if r.grad_norm_path],\n",
        "                'cosine_similarities': [np.mean(r.grad_cosine_sim_path[1:]) for r in method_results if len(r.grad_cosine_sim_path) > 1],\n",
        "                'gradient_angles': [np.mean(r.grad_angle_path[1:]) for r in method_results if len(r.grad_angle_path) > 1]\n",
        "            }\n",
        "\n",
        "            # Calculate means and stds across all experiments for this method\n",
        "            summary_data.append({\n",
        "                'function': function_name,\n",
        "                'dimension': dimension,\n",
        "                'method': method,\n",
        "                'initial_value_mean': np.mean([r.f_initial for r in method_results]),\n",
        "                'initial_value_std': np.std([r.f_initial for r in method_results]),\n",
        "                'final_value_mean': np.mean([r.f_final for r in method_results]),\n",
        "                'final_value_std': np.std([r.f_final for r in method_results]),\n",
        "                'iterations_mean': np.mean([r.iterations for r in method_results]),\n",
        "                'iterations_std': np.std([r.iterations for r in method_results]),\n",
        "                'runtime_mean': np.mean([r.runtime for r in method_results]),\n",
        "                'runtime_std': np.std([r.runtime for r in method_results]),\n",
        "                'success_rate': np.mean([1 if r.success else 0 for r in method_results]),\n",
        "\n",
        "                # Calculate statistics across runs' means\n",
        "                'best_so_far_mean': np.mean(run_means['best_so_far']) if run_means['best_so_far'] else None,\n",
        "                'best_so_far_std': np.std(run_means['best_so_far']) if run_means['best_so_far'] else None,\n",
        "\n",
        "                'initial_distance_to_minimum_mean': np.mean([r.initial_distance_to_minimum for r in method_results]),  # Add this\n",
        "                'initial_distance_to_minimum_std': np.std([r.initial_distance_to_minimum for r in method_results]),    # Add this\n",
        "                'final_distance_to_minimum_mean': np.mean([r.final_distance_to_minimum for r in method_results]),      # Add this\n",
        "                'final_distance_to_minimum_std': np.std([r.final_distance_to_minimum for r in method_results]),        # Add this\n",
        "                'mean_distance_to_minimum': np.mean(run_means['distance_to_minimum_path']) if run_means['distance_to_minimum_path'] else None,  # Updated key\n",
        "                'std_distance_to_minimum': np.std(run_means['distance_to_minimum_path']) if run_means['distance_to_minimum_path'] else None,    # Updated key\n",
        "\n",
        "                'improvements_mean': np.mean(run_means['improvements']) if run_means['improvements'] else None,\n",
        "                'improvements_std': np.std(run_means['improvements']) if run_means['improvements'] else None,\n",
        "\n",
        "                'relative_improvements_mean': np.mean(run_means['relative_improvements']) if run_means['relative_improvements'] else None,\n",
        "                'relative_improvements_std': np.std(run_means['relative_improvements']) if run_means['relative_improvements'] else None,\n",
        "\n",
        "                'step_sizes_mean': np.mean(run_means['step_sizes']) if run_means['step_sizes'] else None,\n",
        "                'step_sizes_std': np.std(run_means['step_sizes']) if run_means['step_sizes'] else None,\n",
        "\n",
        "                'gradient_norm_mean': np.mean(run_means['gradient_norms']) if run_means['gradient_norms'] else None,\n",
        "                'gradient_norm_std': np.std(run_means['gradient_norms']) if run_means['gradient_norms'] else None,\n",
        "\n",
        "                'cosine_similarity_mean': np.mean(run_means['cosine_similarities']) if run_means['cosine_similarities'] else None,\n",
        "                'cosine_similarity_std': np.std(run_means['cosine_similarities']) if run_means['cosine_similarities'] else None,\n",
        "\n",
        "                'gradient_angle_mean': np.mean(run_means['gradient_angles']) if run_means['gradient_angles'] else None,\n",
        "                'gradient_angle_std': np.std(run_means['gradient_angles']) if run_means['gradient_angles'] else None\n",
        "            })\n",
        "\n",
        "        # Save dimension summary with means and stds\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_dir = os.path.join(self.base_dir, function_name, f\"{dimension}D\")\n",
        "        os.makedirs(summary_dir, exist_ok=True)\n",
        "        summary_df.to_csv(\n",
        "            os.path.join(summary_dir, f'dimension_summary_{self.timestamp}.csv'),\n",
        "            index=False\n",
        "        )\n",
        "\n",
        "    def create_method_summary(self, results: List[OptimizationResult]):\n",
        "        \"\"\"Create summary statistics for each optimization method across all dimensions\"\"\"\n",
        "        for method in set(r.method for r in results):\n",
        "            method_results = [r for r in results if r.method == method]\n",
        "\n",
        "            summary_data = []\n",
        "            for result in method_results:\n",
        "                summary_data.append({\n",
        "                    'function': result.function_name,\n",
        "                    'dimension': result.dimension,\n",
        "                    'final_value': result.f_final,\n",
        "                    'iterations': result.iterations,\n",
        "                    'runtime': result.runtime,\n",
        "                    'success': result.success,\n",
        "                    'initial_distance_to_minimum': result.initial_distance_to_minimum,  # Add this\n",
        "                    'final_distance_to_minimum': result.final_distance_to_minimum,      # Changed from distance_to_minimum\n",
        "                    'mean_distance_to_minimum': np.mean(result.distance_to_minimum_path) if result.distance_to_minimum_path is not None else None,  # Add this\n",
        "                    'f_error': result.f_error,\n",
        "                    'mean_cosine_similarity': np.mean(result.grad_cosine_sim_path[1:]),\n",
        "                    'mean_gradient_angle': np.mean(result.grad_angle_path[1:]),\n",
        "                    'convergence_rate': (result.f_path[-1] - result.f_path[0]) / len(result.f_path) if result.f_path else None\n",
        "                })\n",
        "\n",
        "            # Save method summary\n",
        "            summary_df = pd.DataFrame(summary_data)\n",
        "            method_dir = os.path.join(self.base_dir, 'method_summaries')\n",
        "            os.makedirs(method_dir, exist_ok=True)\n",
        "            summary_df.to_csv(\n",
        "                os.path.join(method_dir, f'{method}_summary_{self.timestamp}.csv'),\n",
        "                index=False\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0OSkcfuGTRs"
      },
      "source": [
        "# Experiment Executions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_iehVE2FnKu"
      },
      "outputs": [],
      "source": [
        "def run_optimization(f: Callable, grad: Callable, hess: Callable, x0: np.ndarray,\n",
        "                    method: str, function_name: str) -> OptimizationResult:\n",
        "    \"\"\"Enhanced optimization runner with detailed metrics\"\"\"\n",
        "    start_time = time.time()\n",
        "    step_logger = StepLogger()\n",
        "\n",
        "    # Calculate initial metrics\n",
        "    f_initial = f(x0)\n",
        "    grad_initial = grad(x0)\n",
        "\n",
        "    # Get global minimum for this function\n",
        "    x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "    # Log initial point\n",
        "    step_logger.log_iteration(x0, f_initial, grad_initial, global_minimum=x_min)\n",
        "\n",
        "    def callback(xk):\n",
        "        f_val = f(xk)\n",
        "        grad_val = grad(xk)\n",
        "        step_logger.log_iteration(xk, f_val, grad_val, global_minimum=x_min)\n",
        "\n",
        "    try:\n",
        "        # Run optimization with method-specific settings\n",
        "        if method == 'BFGS':\n",
        "            result = minimize(f, x0, method=method, jac=grad, callback=callback)\n",
        "        elif method == 'newton-cg':\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        elif method in ['trust-exact', 'trust-krylov']:\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n",
        "        # Calculate final gradient\n",
        "        grad_final = grad(result.x)\n",
        "        runtime = time.time() - start_time\n",
        "\n",
        "        return OptimizationResult(\n",
        "            x_final=result.x,\n",
        "            f_final=result.fun,\n",
        "            success=result.success,\n",
        "            iterations=result.nit,\n",
        "            runtime=runtime,\n",
        "            path=step_logger.path,\n",
        "            f_path=step_logger.f_path,\n",
        "            grad_norm_path=step_logger.grad_norm_path,\n",
        "            grad_cosine_sim_path=step_logger.grad_cosine_sim_path,\n",
        "            grad_angle_path=step_logger.grad_angle_path,\n",
        "            timestamps=step_logger.timestamps,\n",
        "            method=method,\n",
        "            dimension=len(x0),\n",
        "            function_name=function_name,\n",
        "            x_initial=x0,\n",
        "            f_initial=f_initial,\n",
        "            grad_initial=grad_initial,\n",
        "            grad_final=grad_final,\n",
        "            step_sizes=step_logger.step_sizes,\n",
        "            improvements=step_logger.improvements,\n",
        "            best_so_far=step_logger.best_so_far,\n",
        "            relative_improvements=step_logger.relative_improvements,\n",
        "            distance_to_minimum_path=step_logger.distance_to_minimum_path\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Optimization failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd91nH3NiT5D"
      },
      "outputs": [],
      "source": [
        "class ExperimentManager:\n",
        "    \"\"\"Manages multiple optimization experiments\"\"\"\n",
        "    def __init__(self,\n",
        "                 test_functions: Dict[str, Tuple[Callable, Callable, Callable]],\n",
        "                 first_order_optimizers: Dict[str, object],\n",
        "                 second_order_methods: List[str],\n",
        "                 dimensions: List[int],\n",
        "                 n_experiments: int = 50,\n",
        "                 min_dist: float = 4,  # Default min distance\n",
        "                 max_dist: float = 8,  # Default max distance\n",
        "                 distance_constraints: Dict[str, Dict[str, float]] = None):  # New parameter\n",
        "        self.test_functions = test_functions\n",
        "        self.first_order_optimizers = first_order_optimizers\n",
        "        self.second_order_methods = second_order_methods\n",
        "        self.dimensions = dimensions\n",
        "        self.n_experiments = n_experiments\n",
        "        self.min_dist = min_dist\n",
        "        self.max_dist = max_dist\n",
        "        self.distance_constraints = distance_constraints or {}\n",
        "\n",
        "    def generate_starting_points(self, dimension: int, function_name: str, seed: int = None) -> np.ndarray:\n",
        "        \"\"\"Generate random starting points with function-specific domain and distance constraints\"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        # Define domain constraints for each function\n",
        "        domain_constraints = {\n",
        "            'michalewicz': (0, np.pi),  # Domain [0, π] for each dimension\n",
        "            'ackley': (-32.768, 32.768),\n",
        "            'rastrigin': (-5.12, 5.12),\n",
        "            'schwefel': (-500, 500),\n",
        "            'sphere': (-5.12, 5.12),\n",
        "            'sum_squares': (-10, 10),\n",
        "            'rosenbrock': (-2.048, 2.048)\n",
        "        }\n",
        "\n",
        "        # Use function-specific distance constraints if provided, otherwise use defaults\n",
        "        min_dist = self.distance_constraints.get(function_name, {}).get('min', self.min_dist)\n",
        "        max_dist = self.distance_constraints.get(function_name, {}).get('max', self.max_dist)\n",
        "\n",
        "        starting_points = []\n",
        "\n",
        "        # Get domain constraints for the specific function\n",
        "        if function_name in domain_constraints:\n",
        "            lower_bound, upper_bound = domain_constraints[function_name]\n",
        "\n",
        "            for _ in range(self.n_experiments):\n",
        "                if function_name == 'michalewicz':\n",
        "                    # For Michalewicz, generate points uniformly within [0, π]\n",
        "                    point = np.random.uniform(lower_bound, upper_bound, dimension)\n",
        "                else:\n",
        "                    # For other functions, use the distance-based approach\n",
        "                    # but ensure it stays within domain by scaling\n",
        "                    direction = np.random.randn(dimension)\n",
        "                    direction = direction / np.linalg.norm(direction)\n",
        "                    distance = np.random.uniform(min_dist, max_dist)\n",
        "                    point = direction * distance\n",
        "\n",
        "                    # Scale point to fit within domain if necessary\n",
        "                    max_abs_coord = np.max(np.abs(point))\n",
        "                    if max_abs_coord > abs(lower_bound):  # assuming symmetric bounds\n",
        "                        scale_factor = abs(lower_bound) / max_abs_coord\n",
        "                        point = point * scale_factor\n",
        "\n",
        "                starting_points.append(point)\n",
        "        else:\n",
        "            # Default behavior for unknown functions\n",
        "            for _ in range(self.n_experiments):\n",
        "                direction = np.random.randn(dimension)\n",
        "                direction = direction / np.linalg.norm(direction)\n",
        "                distance = np.random.uniform(min_dist, max_dist)\n",
        "                point = direction * distance\n",
        "                starting_points.append(point)\n",
        "\n",
        "        return np.array(starting_points)\n",
        "\n",
        "    def get_available_methods(self, dimension: int) -> tuple:\n",
        "        \"\"\"\n",
        "        Filter optimization methods based on dimension.\n",
        "        Returns (first_order_dict, second_order_list) tuple.\n",
        "        \"\"\"\n",
        "        # First order methods are always available\n",
        "        available_first_order = self.first_order_optimizers.copy()\n",
        "\n",
        "        # For second order methods, filter out trust-region methods for high dimensions\n",
        "        available_second_order = []\n",
        "        for method in self.second_order_methods:\n",
        "            # Skip trust-region methods for dimension >= 128\n",
        "            if dimension >= 128 and ('trust' in method.lower()):\n",
        "                continue\n",
        "            available_second_order.append(method)\n",
        "\n",
        "        return available_first_order, available_second_order\n",
        "\n",
        "    # Then in run_experiments, modify the method selection part:\n",
        "    def run_experiments(self, base_dir: str = \"optimization_results\"):\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        experiment_dir = os.path.join(base_dir, f\"experiment_{timestamp}\")\n",
        "        experiment_logger = OptimizationLogger(experiment_dir)\n",
        "\n",
        "        final_results = []\n",
        "\n",
        "        for func_name, (f, grad, hess) in self.test_functions.items():\n",
        "            print(f\"\\nTesting {func_name} function:\")\n",
        "            function_results = []\n",
        "\n",
        "            for dim in self.dimensions:\n",
        "                print(f\"\\nDimension: {dim}\")\n",
        "\n",
        "                # Get available methods for this dimension\n",
        "                available_first_order, available_second_order = self.get_available_methods(dim)\n",
        "\n",
        "                if dim >= 128:\n",
        "                    print(f\"Dimension {dim}: Skipping trust-region methods\")\n",
        "\n",
        "                dimension_results = []\n",
        "                starting_points = self.generate_starting_points(dim, func_name)\n",
        "\n",
        "                # Create directory structure\n",
        "                func_dir = os.path.join(experiment_dir, func_name, f\"{dim}D\")\n",
        "                first_order_dir = os.path.join(func_dir, \"first_order\")\n",
        "                second_order_dir = os.path.join(func_dir, \"second_order\")\n",
        "                results_dir = os.path.join(func_dir, \"results\")\n",
        "\n",
        "                # Create all necessary directories\n",
        "                os.makedirs(results_dir, exist_ok=True)\n",
        "                os.makedirs(os.path.join(results_dir, \"gradient_metrics\"), exist_ok=True)\n",
        "\n",
        "                for method_name in self.first_order_optimizers.keys():\n",
        "                    method_dir = os.path.join(first_order_dir, method_name)\n",
        "                    os.makedirs(os.path.join(method_dir, \"results\"), exist_ok=True)\n",
        "                    os.makedirs(os.path.join(method_dir, \"trajectories\"), exist_ok=True)\n",
        "\n",
        "                for method_name in self.second_order_methods:\n",
        "                    method_dir = os.path.join(second_order_dir, method_name)\n",
        "                    os.makedirs(os.path.join(method_dir, \"results\"), exist_ok=True)\n",
        "                    os.makedirs(os.path.join(method_dir, \"trajectories\"), exist_ok=True)\n",
        "\n",
        "                # Run experiments for each starting point\n",
        "                for i, x0 in enumerate(starting_points):\n",
        "                    print(f\"\\nExperiment {i+1}/{self.n_experiments}\")\n",
        "\n",
        "                    # Store results for this experiment\n",
        "                    first_order_results = {}\n",
        "                    second_order_results = {}\n",
        "\n",
        "                    # First order methods (using filtered methods)\n",
        "                    print(\"\\nFirst-order methods:\")\n",
        "                    print(\"-\" * 50)\n",
        "                    for name, optimizer in available_first_order.items():\n",
        "                        print(f\"Running {name}...\")\n",
        "                        try:\n",
        "                            step_logger = StepLogger()\n",
        "                            start_time = time.time()\n",
        "\n",
        "                            # Initialize with first gradient\n",
        "                            g_initial = grad(x0)\n",
        "                            x_min, _ = TestFunctions.get_global_minimum(func_name, dim)\n",
        "                            step_logger.log_iteration(x0, f(x0), g_initial, global_minimum=x_min)\n",
        "\n",
        "                            def callback(x):\n",
        "                                g = grad(x)\n",
        "                                step_logger.log_iteration(x, f(x), g, global_minimum=x_min)\n",
        "\n",
        "                            result = optimizer.optimize(f, grad, x0, func_name, callback=callback)\n",
        "                            runtime = time.time() - start_time\n",
        "\n",
        "                            opt_result = OptimizationResult(\n",
        "                                x_final=result['x'],\n",
        "                                f_final=result['fun'],\n",
        "                                success=result['success'],\n",
        "                                iterations=result['nit'],\n",
        "                                runtime=runtime,\n",
        "                                path=step_logger.path,\n",
        "                                f_path=step_logger.f_path,\n",
        "                                grad_norm_path=step_logger.grad_norm_path,\n",
        "                                grad_cosine_sim_path=step_logger.grad_cosine_sim_path,\n",
        "                                grad_angle_path=step_logger.grad_angle_path,\n",
        "                                timestamps=step_logger.timestamps,\n",
        "                                method=name,\n",
        "                                dimension=dim,\n",
        "                                function_name=func_name,\n",
        "                                x_initial=x0,\n",
        "                                f_initial=f(x0),\n",
        "                                grad_initial=g_initial,\n",
        "                                grad_final=grad(result['x']),\n",
        "                                step_sizes=step_logger.step_sizes,\n",
        "                                improvements=step_logger.improvements,\n",
        "                                best_so_far=step_logger.best_so_far,\n",
        "                                relative_improvements=step_logger.relative_improvements,\n",
        "                                distance_to_minimum_path=step_logger.distance_to_minimum_path  # This is correct\n",
        "                            )\n",
        "\n",
        "                            first_order_results[name] = opt_result\n",
        "                            dimension_results.append(opt_result)\n",
        "                            function_results.append(opt_result)\n",
        "                            final_results.append(opt_result)\n",
        "                            experiment_logger.log_run(opt_result, i)\n",
        "\n",
        "                            print(f\"  Runtime: {runtime:.3f} seconds\")\n",
        "                            print(f\"  Iterations: {result['nit']}\")\n",
        "                            print(f\"  Final value: {result['fun']:.6f}\")\n",
        "                            print(f\"  Success: {result['success']}\")\n",
        "                            print(\"Done\")\n",
        "                            print(\"-\" * 20)\n",
        "                            print()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Failed: {str(e)}\")\n",
        "\n",
        "                    # Generate plots for first-order methods\n",
        "                    if first_order_results:\n",
        "                        for name, result in first_order_results.items():\n",
        "                            method_dir = os.path.join(first_order_dir, name)\n",
        "                            self._generate_plots({name: result}, method_dir, f, i, dim)\n",
        "\n",
        "                    # Second order methods (using filtered methods)\n",
        "                    print(\"\\nSecond-order methods:\")\n",
        "                    print(\"-\" * 50)\n",
        "                    for method in available_second_order:\n",
        "                        print(f\"Running {method}...\")\n",
        "                        result = run_optimization(f, grad, hess, x0, method, func_name)\n",
        "                        if result:\n",
        "                            second_order_results[method] = result\n",
        "                            dimension_results.append(result)\n",
        "                            function_results.append(result)\n",
        "                            final_results.append(result)\n",
        "                            experiment_logger.log_run(result, i)\n",
        "                            print(f\"  Runtime: {result.runtime:.3f} seconds\")\n",
        "                            print(f\"  Iterations: {result.iterations}\")\n",
        "                            print(f\"  Final value: {result.f_final:.6f}\")\n",
        "                            print(\"Done\")\n",
        "                        else:\n",
        "                            print(\"Failed\")\n",
        "                        print(\"-\" * 20)\n",
        "                        print()\n",
        "\n",
        "                    # Generate plots for second-order methods\n",
        "                    if second_order_results:\n",
        "                        for name, result in second_order_results.items():\n",
        "                            method_dir = os.path.join(second_order_dir, name)\n",
        "                            self._generate_plots({name: result}, method_dir, f, i, dim)\n",
        "\n",
        "                    # Generate combined plots for this experiment\n",
        "                    all_results_this_batch = {**first_order_results, **second_order_results}\n",
        "                    if all_results_this_batch:\n",
        "                        try:\n",
        "                            # Generate convergence plot with experiment number\n",
        "                            convergence_filename = f'convergence_{func_name}_exp_{i}'\n",
        "                            Visualizer.plot_convergence(\n",
        "                                all_results_this_batch,\n",
        "                                os.path.join(results_dir),\n",
        "                                convergence_filename\n",
        "                            )\n",
        "\n",
        "                            # Generate all metrics plots\n",
        "                            Visualizer.plot_all_metrics(\n",
        "                                all_results_this_batch,\n",
        "                                results_dir,\n",
        "                                f'{func_name}_exp_{i}'\n",
        "                            )\n",
        "                        except Exception as e:\n",
        "                            print(f\"Warning: Could not create plots for experiment {i}: {str(e)}\")\n",
        "\n",
        "                    # Clear batch results to free memory\n",
        "                    first_order_results.clear()\n",
        "                    second_order_results.clear()\n",
        "\n",
        "                # After all experiments for this dimension\n",
        "                # Create dimension summary plots\n",
        "                dimension_results_combined = {}\n",
        "                for result in dimension_results:\n",
        "                    method = result.method\n",
        "                    if method not in dimension_results_combined:\n",
        "                        dimension_results_combined[method] = []\n",
        "                    dimension_results_combined[method].append(result)\n",
        "\n",
        "                # Create summary plots by averaging results for each method\n",
        "                if dimension_results_combined:\n",
        "                    summary_results = {}\n",
        "                    for method, results in dimension_results_combined.items():\n",
        "                        try:\n",
        "                            # Get the first result to access method-specific attributes\n",
        "                            first_result = results[0]\n",
        "\n",
        "                            # Calculate average paths and values\n",
        "                            max_path_length = max(len(r.path) for r in results)\n",
        "                            avg_path = []\n",
        "                            for i in range(max_path_length):\n",
        "                                valid_points = [r.path[i] for r in results if i < len(r.path)]\n",
        "                                if valid_points:\n",
        "                                    avg_path.append(np.mean(valid_points, axis=0))\n",
        "\n",
        "                            # Create averaged result\n",
        "                            avg_result = OptimizationResult(\n",
        "                                x_final=np.mean([r.x_final for r in results], axis=0),\n",
        "                                f_final=np.mean([r.f_final for r in results]),\n",
        "                                success=np.mean([r.success for r in results]),\n",
        "                                iterations=int(np.mean([r.iterations for r in results])),\n",
        "                                runtime=np.mean([r.runtime for r in results]),\n",
        "                                path=avg_path,\n",
        "                                f_path=[np.mean([r.f_path[i] for r in results if i < len(r.f_path)])\n",
        "                                      for i in range(max(len(r.f_path) for r in results))],\n",
        "                                grad_norm_path=[np.mean([r.grad_norm_path[i] for r in results if i < len(r.grad_norm_path)])\n",
        "                                              for i in range(max(len(r.grad_norm_path) for r in results))],\n",
        "                                grad_cosine_sim_path=[np.mean([r.grad_cosine_sim_path[i] for r in results if i < len(r.grad_cosine_sim_path)])\n",
        "                                                    for i in range(max(len(r.grad_cosine_sim_path) for r in results))],\n",
        "                                grad_angle_path=[np.mean([r.grad_angle_path[i] for r in results if i < len(r.grad_angle_path)])\n",
        "                                              for i in range(max(len(r.grad_angle_path) for r in results))],\n",
        "                                step_sizes=[np.mean([r.step_sizes[i] for r in results if i < len(r.step_sizes)])\n",
        "                                          for i in range(max(len(r.step_sizes) for r in results))],\n",
        "                                improvements=[np.mean([r.improvements[i] for r in results if i < len(r.improvements)])\n",
        "                                            for i in range(max(len(r.improvements) for r in results))],\n",
        "                                best_so_far=[np.mean([r.best_so_far[i] for r in results if i < len(r.best_so_far)])\n",
        "                                            for i in range(max(len(r.best_so_far) for r in results))],\n",
        "                                relative_improvements=[np.mean([r.relative_improvements[i] for r in results if i < len(r.relative_improvements)])\n",
        "                                                    for i in range(max(len(r.relative_improvements) for r in results))],\n",
        "                                distance_to_minimum_path=[np.mean([r.distance_to_minimum_path[i] for r in results if i < len(r.distance_to_minimum_path)])\n",
        "                                                      for i in range(max(len(r.distance_to_minimum_path) for r in results))],\n",
        "                                method=method,\n",
        "                                dimension=first_result.dimension,\n",
        "                                function_name=first_result.function_name,\n",
        "                                x_initial=first_result.x_initial,  # Use the first result's initial point\n",
        "                                f_initial=np.mean([r.f_initial for r in results]),\n",
        "                                grad_initial=np.mean([r.grad_initial for r in results], axis=0),\n",
        "                                grad_final=np.mean([r.grad_final for r in results], axis=0)\n",
        "                            )\n",
        "                            summary_results[method] = avg_result\n",
        "                        except Exception as e:\n",
        "                            print(f\"Warning: Could not create average result for method {method}: {str(e)}\")\n",
        "                            continue\n",
        "\n",
        "                    # Create summary plots using averaged results\n",
        "                    # Before generating plots, group results by method\n",
        "                    dimension_results_by_method = {}\n",
        "                    for result in dimension_results:\n",
        "                        if result.method not in dimension_results_by_method:\n",
        "                            dimension_results_by_method[result.method] = []\n",
        "                        dimension_results_by_method[result.method].append(result)\n",
        "\n",
        "                    # Update the trajectory plotting section in run_experiments:\n",
        "                    if dimension_results_by_method:\n",
        "                        try:\n",
        "                            # Generate all metric plots using the new matrix layout\n",
        "                            Visualizer.plot_all_metrics(dimension_results_by_method, results_dir, func_name)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Warning: Could not create summary plots: {str(e)}\")\n",
        "                        finally:\n",
        "                            plt.close('all')  # Final cleanup\n",
        "\n",
        "                    # Create dimension summary\n",
        "                    experiment_logger.create_dimension_summary(func_name, dim, dimension_results)\n",
        "\n",
        "                    # Save dimension results\n",
        "                    results_df = pd.DataFrame([self._format_result(r, \"dimension\", i)\n",
        "                                            for i, r in enumerate(dimension_results)])\n",
        "                    results_df.to_csv(\n",
        "                        os.path.join(results_dir, f'dimension_results_{timestamp}.csv'),\n",
        "                        index=False\n",
        "                    )\n",
        "\n",
        "                    # Clear dimension results after saving\n",
        "                    dimension_results.clear()\n",
        "                    dimension_results_by_method.clear()\n",
        "\n",
        "            # After all dimensions for this function, save function results\n",
        "            results_df = pd.DataFrame([self._format_result(r, \"function\", i)\n",
        "                                    for i, r in enumerate(function_results)])\n",
        "            results_df.to_csv(\n",
        "                os.path.join(experiment_dir, func_name, f'function_results_{timestamp}.csv'),\n",
        "                index=False\n",
        "            )\n",
        "\n",
        "            # Clear function results after saving\n",
        "            function_results.clear()\n",
        "\n",
        "        # Save final results and generate statistics\n",
        "        results_df = pd.DataFrame([self._format_result(r, \"final\", i)\n",
        "                                for i, r in enumerate(final_results)])\n",
        "        results_df.to_csv(os.path.join(experiment_dir, \"all_results.csv\"), index=False)\n",
        "        generate_statistics(results_df, experiment_dir)\n",
        "\n",
        "        # Clear final results\n",
        "        final_results.clear()\n",
        "\n",
        "    def _generate_plots(self, results: Dict[str, OptimizationResult],\n",
        "                      base_dir: str, f: Callable, exp_num: int, dim: int):\n",
        "        \"\"\"Generate trajectory and convergence plots\"\"\"\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        results_dir = os.path.join(base_dir, \"results\")\n",
        "        trajectory_dir = os.path.join(base_dir, \"trajectories\")\n",
        "\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        os.makedirs(trajectory_dir, exist_ok=True)\n",
        "\n",
        "        # Get function name from first result\n",
        "        func_name = next(iter(results.values())).function_name\n",
        "\n",
        "        # Convert results to the expected format (Dict[str, List[OptimizationResult]])\n",
        "        results_list = {}\n",
        "        for method, result in results.items():\n",
        "            if result is not None:\n",
        "                # Create a new list with the single result\n",
        "                results_list[method] = [result]\n",
        "\n",
        "        # Generate convergence plots if we have results to plot\n",
        "        if results_list:\n",
        "            try:\n",
        "                Visualizer.plot_convergence(results_list, results_dir, func_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not create convergence plot: {str(e)}\")\n",
        "\n",
        "            try:\n",
        "                Visualizer.plot_step_metrics(results_list, results_dir, func_name, exp_num)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not create step metrics plot: {str(e)}\")\n",
        "\n",
        "        # Generate 2D trajectories if applicable\n",
        "        if dim == 2:\n",
        "            for method, result in results.items():\n",
        "                if result is not None:\n",
        "                    try:\n",
        "                        Visualizer.plot_2d_trajectory(f, result, trajectory_dir, exp_num)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Could not create trajectory plot for {method}: {str(e)}\")\n",
        "\n",
        "    def _format_result(self, result: OptimizationResult, opt_type: str, exp_num: int) -> dict:\n",
        "        \"\"\"Format OptimizationResult for DataFrame with error handling\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'function': result.function_name,\n",
        "                'dimension': result.dimension,\n",
        "                'experiment': exp_num,\n",
        "                'method': result.method,\n",
        "                'optimizer_type': opt_type,\n",
        "                'start_distance': np.linalg.norm(result.x_initial),\n",
        "                'final_value': result.f_final,\n",
        "                'iterations': result.iterations,\n",
        "                'runtime': result.runtime,\n",
        "                'success': False if any(np.isinf(x) for x in result.f_path) else result.success,\n",
        "                'distance_to_minimum': result.final_distance_to_minimum,  # Changed from distance_to_minimum\n",
        "                'initial_distance_to_minimum': result.initial_distance_to_minimum,  # Add this\n",
        "                'f_error': result.f_error,\n",
        "                'initial_gradient_norm': np.linalg.norm(result.grad_initial),\n",
        "                'final_gradient_norm': np.linalg.norm(result.grad_final),\n",
        "                'computation_error': False,\n",
        "                # Add new metrics\n",
        "                'mean_best_so_far': np.mean(result.best_so_far) if result.best_so_far else None,\n",
        "                'std_best_so_far': np.std(result.best_so_far) if result.best_so_far else None,\n",
        "                'mean_distance_to_minimum': np.mean(result.distance_to_minimum_path) if result.distance_to_minimum_path is not None else None,\n",
        "                'std_distance_to_minimum': np.std(result.distance_to_minimum_path) if result.distance_to_minimum_path is not None else None,\n",
        "                'mean_improvement': np.mean(result.improvements) if result.improvements else None,\n",
        "                'std_improvement': np.std(result.improvements) if result.improvements else None,\n",
        "                'mean_relative_improvement': np.mean(result.relative_improvements) if result.relative_improvements else None,\n",
        "                'std_relative_improvement': np.std(result.relative_improvements) if result.relative_improvements else None,\n",
        "                'mean_step_size': np.mean(result.step_sizes) if result.step_sizes else None,\n",
        "                'std_step_size': np.std(result.step_sizes) if result.step_sizes else None,\n",
        "                'mean_gradient_norm': np.mean(result.grad_norm_path) if result.grad_norm_path else None,\n",
        "                'std_gradient_norm': np.std(result.grad_norm_path) if result.grad_norm_path else None,\n",
        "                'mean_cosine_similarity': np.mean(result.grad_cosine_sim_path[1:]) if len(result.grad_cosine_sim_path) > 1 else None,\n",
        "                'std_cosine_similarity': np.std(result.grad_cosine_sim_path[1:]) if len(result.grad_cosine_sim_path) > 1 else None,\n",
        "                'mean_gradient_angle': np.mean(result.grad_angle_path[1:]) if len(result.grad_angle_path) > 1 else None,\n",
        "                'std_gradient_angle': np.std(result.grad_angle_path[1:]) if len(result.grad_angle_path) > 1 else None\n",
        "            }\n",
        "        except (OverflowError, ValueError, RuntimeError) as e:\n",
        "            # Return a failed result entry\n",
        "            return {\n",
        "                'function': result.function_name,\n",
        "                'dimension': result.dimension,\n",
        "                'experiment': exp_num,\n",
        "                'method': result.method,\n",
        "                'optimizer_type': opt_type,\n",
        "                'success': False,\n",
        "                'computation_error': True,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "\n",
        "def generate_statistics(results_df: pd.DataFrame, save_dir: str):\n",
        "    \"\"\"Generate comprehensive statistical visualizations for optimization results\"\"\"\n",
        "    if len(results_df) == 0:\n",
        "        return\n",
        "\n",
        "    print(\"Available columns in DataFrame:\", results_df.columns.tolist())\n",
        "\n",
        "    # Define metrics with correct column names\n",
        "    metrics = {\n",
        "        'final_value': ['mean', 'std', 'min', 'max'],\n",
        "        'runtime': ['mean', 'std'],\n",
        "        'iterations': ['mean', 'std'],\n",
        "        'success': 'mean',\n",
        "        'distance_to_minimum': ['mean', 'std'],           # Per-timestep distance\n",
        "        'final_distance_to_minimum': ['mean', 'std'],     # Final distance at end\n",
        "        'initial_distance_to_minimum': ['mean', 'std'],   # Initial distance\n",
        "        'mean_step_size': ['mean', 'std'],\n",
        "        'mean_improvement': ['mean', 'std'],\n",
        "        'mean_relative_improvement': ['mean', 'std'],\n",
        "        'mean_gradient_norm': ['mean', 'std'],\n",
        "        'mean_cosine_similarity': ['mean', 'std'],\n",
        "        'mean_gradient_angle': ['mean', 'std']\n",
        "    }\n",
        "\n",
        "    # Only include metrics that exist in the DataFrame\n",
        "    available_metrics = {\n",
        "        col: metrics[col]\n",
        "        for col in metrics.keys()\n",
        "        if col in results_df.columns\n",
        "    }\n",
        "\n",
        "    # Group by method first, then calculate statistics across runs\n",
        "    try:\n",
        "        summary_stats = results_df.groupby(['method', 'function', 'dimension']).agg(available_metrics)\n",
        "        summary_stats = summary_stats.round(4)\n",
        "\n",
        "        # Save summary statistics\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        summary_stats.to_csv(os.path.join(save_dir, 'summary_statistics.csv'))\n",
        "\n",
        "        return summary_stats\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating statistics: {str(e)}\")\n",
        "        print(\"Available columns:\", results_df.columns.tolist())\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzdasrggF19C"
      },
      "source": [
        "# Github and main() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvb6HG67Fp6u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from github import Github\n",
        "import git\n",
        "import json\n",
        "from pathlib import Path\n",
        "import time\n",
        "import random\n",
        "\n",
        "class AuthManager:\n",
        "    def __init__(self):\n",
        "        self.config_file = Path.home() / '.optimization_config'\n",
        "        self.credentials = self.load_credentials()\n",
        "\n",
        "    def load_credentials(self):\n",
        "        if self.config_file.exists():\n",
        "            try:\n",
        "                with open(self.config_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def save_credentials(self, credentials):\n",
        "        with open(self.config_file, 'w') as f:\n",
        "            json.dump(credentials, f)\n",
        "\n",
        "    def authenticate(self):\n",
        "        if self.credentials:\n",
        "            print(\"Found existing credentials. Would you like to use them? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                return self.credentials\n",
        "\n",
        "        print(\"\\n=== GitHub Authentication ===\")\n",
        "        print(\"Please provide your GitHub credentials to continue.\")\n",
        "        print(\"Note: Your personal access token needs repo and workflow permissions.\")\n",
        "        print(\"\\nDon't have a token? Create one at: https://github.com/settings/tokens\")\n",
        "\n",
        "        github_username = input(\"\\nGitHub Username: \")\n",
        "        github_token = getpass.getpass(\"Personal Access Token: \")\n",
        "        repo_name = input(\"Repository Name (format: username/repo): \")\n",
        "\n",
        "        # Verify credentials\n",
        "        try:\n",
        "            g = Github(github_token)\n",
        "            user = g.get_user()\n",
        "            _ = user.get_repo(repo_name.split('/')[1])\n",
        "\n",
        "            credentials = {\n",
        "                'username': github_username,\n",
        "                'token': github_token,\n",
        "                'repo': repo_name\n",
        "            }\n",
        "\n",
        "            print(\"\\nAuthentication successful!\")\n",
        "\n",
        "            # Ask to save credentials\n",
        "            print(\"Would you like to save these credentials for future use? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                self.save_credentials(credentials)\n",
        "                print(\"Credentials saved!\")\n",
        "\n",
        "            return credentials\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAuthentication failed: {str(e)}\")\n",
        "            print(\"Please try again.\")\n",
        "            return self.authenticate()\n",
        "\n",
        "class GitManager:\n",
        "    def __init__(self, credentials):\n",
        "        self.credentials = credentials\n",
        "        self.token = credentials['token']\n",
        "        self.repo_url = f\"https://x-access-token:{self.token}@github.com/{credentials['repo']}.git\"\n",
        "        self.instance_id = f\"optimization_{random.randint(1000, 9999)}\"\n",
        "        self.repo = None\n",
        "\n",
        "    def setup_repo(self, local_path):\n",
        "        \"\"\"Initialize or clone the repository and create a new branch\"\"\"\n",
        "        try:\n",
        "            print(f\"Creating directory: {local_path}\")\n",
        "            Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Configure git globally\n",
        "            with git.Git().custom_environment(GIT_SSL_NO_VERIFY='true'):\n",
        "                try:\n",
        "                    print(\"Checking if directory is a git repository...\")\n",
        "                    self.repo = git.Repo(local_path)\n",
        "                    print(\"Existing repository found\")\n",
        "\n",
        "                    # Update remote URL with credentials\n",
        "                    origin = self.repo.remote('origin')\n",
        "                    origin.set_url(self.repo_url)\n",
        "                    print(\"Remote URL updated\")\n",
        "\n",
        "                except git.exc.InvalidGitRepositoryError:\n",
        "                    print(\"Initializing new git repository...\")\n",
        "                    self.repo = git.Repo.init(local_path)\n",
        "                    print(\"Repository initialized\")\n",
        "\n",
        "                    print(\"Adding remote origin...\")\n",
        "                    origin = self.repo.create_remote('origin', self.repo_url)\n",
        "                    print(\"Remote added\")\n",
        "\n",
        "                    # Configure git credentials\n",
        "                    config_writer = self.repo.config_writer()\n",
        "                    config_writer.set_value(\"http\", \"sslVerify\", \"false\")\n",
        "                    config_writer.release()\n",
        "\n",
        "                    print(\"Fetching from remote...\")\n",
        "                    origin.fetch()\n",
        "                    print(\"Fetch completed\")\n",
        "\n",
        "                # Pull main branch to get latest changes\n",
        "                print(\"Pulling latest changes from main...\")\n",
        "                origin.pull('main')\n",
        "                print(\"Pull completed\")\n",
        "\n",
        "                # Create and checkout new branch\n",
        "                print(f\"Creating and checking out new branch: {self.instance_id}...\")\n",
        "                new_branch = self.repo.create_head(self.instance_id, origin.refs.main)\n",
        "                new_branch.checkout()\n",
        "                print(f\"Successfully switched to branch: {self.instance_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up repository: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def push_results(self, local_path):\n",
        "        \"\"\"Push results to GitHub with conflict resolution\"\"\"\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "\n",
        "        while retry_count < max_retries:\n",
        "            try:\n",
        "                print(f\"\\nAttempting to push results (attempt {retry_count + 1}/{max_retries})...\")\n",
        "\n",
        "                # Configure git environment for this operation\n",
        "                env = {\n",
        "                    'GIT_SSL_NO_VERIFY': 'true',\n",
        "                    'GIT_TERMINAL_PROMPT': '0',\n",
        "                    'GIT_USERNAME': 'x-access-token',\n",
        "                    'GIT_PASSWORD': self.token\n",
        "                }\n",
        "\n",
        "                with self.repo.git.custom_environment(**env):\n",
        "                    print(\"Adding new files...\")\n",
        "                    self.repo.index.add('*')\n",
        "                    print(\"Files added\")\n",
        "\n",
        "                    print(\"Committing changes...\")\n",
        "                    self.repo.index.commit(f\"Results update from {self.instance_id}\")\n",
        "                    print(\"Changes committed\")\n",
        "\n",
        "                    print(\"Pushing to remote...\")\n",
        "                    push_info = self.repo.remotes.origin.push(self.instance_id)\n",
        "                    print(\"Push completed\")\n",
        "\n",
        "                    print(\"Creating pull request...\")\n",
        "                    self.create_pull_request()\n",
        "\n",
        "                    print(f\"\\nResults successfully pushed to branch: {self.instance_id}\")\n",
        "                    print(\"Pull request created for review.\")\n",
        "                    break\n",
        "\n",
        "            except git.exc.GitCommandError as e:\n",
        "                print(f\"Git error: {e}\")\n",
        "                retry_count += 1\n",
        "                wait_time = random.uniform(1, 5)\n",
        "                print(f\"Waiting {wait_time:.2f} seconds before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error: {e}\")\n",
        "                print(f\"Error type: {type(e)}\")\n",
        "                break\n",
        "\n",
        "    def create_pull_request(self):\n",
        "        \"\"\"Create a pull request using GitHub API\"\"\"\n",
        "        try:\n",
        "            g = Github(self.token)\n",
        "            repo = g.get_repo(self.credentials['repo'])\n",
        "\n",
        "            pr = repo.create_pull(\n",
        "                title=f\"Results update from {self.instance_id}\",\n",
        "                body=\"Automated results update from optimization experiment\",\n",
        "                head=self.instance_id,\n",
        "                base=\"main\"\n",
        "            )\n",
        "            print(f\"Created PR: {pr.html_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create PR: {e}\")\n",
        "\n",
        "def setup_experiment():\n",
        "    \"\"\"Initial setup and authentication\"\"\"\n",
        "    print(\"\\n=== Optimization Experiment Setup ===\")\n",
        "\n",
        "    # Authenticate\n",
        "    auth_manager = AuthManager()\n",
        "    credentials = auth_manager.authenticate()\n",
        "\n",
        "    if not credentials:\n",
        "        print(\"Authentication failed. Cannot continue.\")\n",
        "        return None\n",
        "\n",
        "    return credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqoSGTWxadj1",
        "outputId": "6ea34158-95e9-4939-f493-44da663cfac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Optimization Experiment Setup ===\n",
            "Found existing credentials. Would you like to use them? (y/n)\n",
            "Creating directory: optimization_results\n",
            "Checking if directory is a git repository...\n",
            "Existing repository found\n",
            "Remote URL updated\n",
            "Pulling latest changes from main...\n",
            "Pull completed\n",
            "Creating and checking out new branch: optimization_4848...\n",
            "Successfully switched to branch: optimization_4848\n",
            "\n",
            "Testing rosenbrock function:\n",
            "\n",
            "Dimension: 2\n",
            "\n",
            "Experiment 1/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-8aff6a75a1a8>:157: RuntimeWarning: overflow encountered in square\n",
            "  return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
            "<ipython-input-18-f547604badb2>:6: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_sim = np.dot(grad_current, grad_previous) / (np.linalg.norm(grad_current) * np.linalg.norm(grad_previous))\n",
            "<ipython-input-15-8aff6a75a1a8>:164: RuntimeWarning: overflow encountered in scalar power\n",
            "  grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
            "<ipython-input-15-8aff6a75a1a8>:165: RuntimeWarning: overflow encountered in scalar power\n",
            "  grad[-1] = 200 * (x[-1] - x[-2]**2)\n",
            "<ipython-input-15-8aff6a75a1a8>:157: RuntimeWarning: invalid value encountered in subtract\n",
            "  return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
            "<ipython-input-15-8aff6a75a1a8>:164: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
            "<ipython-input-15-8aff6a75a1a8>:165: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  grad[-1] = 200 * (x[-1] - x[-2]**2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.145 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.152 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.149 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.142 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.149 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.031246\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.162 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 12.539763\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-675b75c20866>:47: RuntimeWarning: Mean of empty slice\n",
            "  f_mean = np.nanmean(f_values, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1741: RuntimeWarning: invalid value encountered in subtract\n",
            "  np.subtract(arr, avg, out=arr, casting='unsafe', where=where)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "<ipython-input-17-675b75c20866>:49: RuntimeWarning: Mean of empty slice\n",
            "  grad_mean = np.nanmean(grad_norms, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py:2392: RuntimeWarning: overflow encountered in power\n",
            "  ticklocs = b ** decades\n",
            "<ipython-input-17-675b75c20866>:344: RuntimeWarning: Mean of empty slice\n",
            "  mean_values = np.nanmean(values, axis=0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Could not create convergence plot: cannot convert float infinity to integer\n",
            "Warning: Could not create trajectory plot for gradient_descent: Axis limits cannot be NaN or Inf\n",
            "Warning: Could not create convergence plot: cannot convert float infinity to integer\n",
            "Warning: Could not create trajectory plot for sgd: Axis limits cannot be NaN or Inf\n",
            "Warning: Could not create convergence plot: cannot convert float infinity to integer\n",
            "Warning: Could not create trajectory plot for sgd_momentum: Axis limits cannot be NaN or Inf\n",
            "Warning: Could not create convergence plot: cannot convert float infinity to integer\n",
            "Warning: Could not create trajectory plot for momentum: Axis limits cannot be NaN or Inf\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running BFGS...\n",
            "  Runtime: 0.009 seconds\n",
            "  Iterations: 26\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running newton-cg...\n",
            "  Runtime: 0.124 seconds\n",
            "  Iterations: 400\n",
            "  Final value: 0.020079\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-exact...\n",
            "  Runtime: 0.136 seconds\n",
            "  Iterations: 400\n",
            "  Final value: 0.092370\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.184 seconds\n",
            "  Iterations: 400\n",
            "  Final value: 0.092151\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Warning: Could not create convergence plot: cannot convert float infinity to integer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-675b75c20866>:231: RuntimeWarning: Mean of empty slice\n",
            "  mean_values = np.nanmean(values, axis=0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Experiment 2/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.140 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-8aff6a75a1a8>:164: RuntimeWarning: overflow encountered in scalar multiply\n",
            "  grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
            "<ipython-input-19-d5f145bdae92>:14: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  return (f_previous - f_current) / abs(f_previous) * 100\n",
            "<ipython-input-16-6ffe1a315b08>:21: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = x - self.learning_rate * g\n",
            "<ipython-input-16-6ffe1a315b08>:76: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = x - self.learning_rate * g\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.151 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.144 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-6ffe1a315b08>:135: RuntimeWarning: invalid value encountered in subtract\n",
            "  v = self.momentum * v - self.learning_rate * g\n",
            "<ipython-input-16-6ffe1a315b08>:189: RuntimeWarning: invalid value encountered in subtract\n",
            "  v = self.momentum * v - self.learning_rate * g\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.142 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.144 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.023777\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.158 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.028217\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Warning: Could not create trajectory plot for gradient_descent: Axis limits cannot be NaN or Inf\n",
            "Warning: Could not create trajectory plot for sgd: Axis limits cannot be NaN or Inf\n",
            "Warning: Could not create trajectory plot for sgd_momentum: Axis limits cannot be NaN or Inf\n",
            "Warning: Could not create trajectory plot for momentum: Axis limits cannot be NaN or Inf\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running BFGS...\n",
            "  Runtime: 0.006 seconds\n",
            "  Iterations: 14\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running newton-cg...\n",
            "  Runtime: 0.107 seconds\n",
            "  Iterations: 400\n",
            "  Final value: 0.014084\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-exact...\n",
            "  Runtime: 0.136 seconds\n",
            "  Iterations: 400\n",
            "  Final value: 0.015071\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.204 seconds\n",
            "  Iterations: 400\n",
            "  Final value: 0.013864\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:118: RuntimeWarning: invalid value encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1746: RuntimeWarning: overflow encountered in multiply\n",
            "  sqr = np.multiply(arr, arr, out=arr, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dimension: 8\n",
            "\n",
            "Experiment 1/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.167 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: overflow encountered in square\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: overflow encountered in multiply\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: invalid value encountered in subtract\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.181 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.177 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-8aff6a75a1a8>:157: RuntimeWarning: overflow encountered in multiply\n",
            "  return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.180 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.181 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.210094\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.184 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 17.071883\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/matplotlib/scale.py:255: RuntimeWarning: overflow encountered in power\n",
            "  return np.power(self.base, values)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running BFGS...\n",
            "  Runtime: 0.019 seconds\n",
            "  Iterations: 52\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running newton-cg...\n",
            "  Runtime: 4.004 seconds\n",
            "  Iterations: 1600\n",
            "  Final value: 0.033232\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-exact...\n",
            "  Runtime: 4.458 seconds\n",
            "  Iterations: 1600\n",
            "  Final value: 1.588421\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 4.225 seconds\n",
            "  Iterations: 1600\n",
            "  Final value: 1.444782\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Experiment 2/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.167 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.174 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.184 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.173 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.181 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.210523\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.187 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 5.852614\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running BFGS...\n",
            "  Runtime: 0.021 seconds\n",
            "  Iterations: 57\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running newton-cg...\n",
            "  Runtime: 3.950 seconds\n",
            "  Iterations: 1600\n",
            "  Final value: 0.025753\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-exact...\n",
            "  Runtime: 4.393 seconds\n",
            "  Iterations: 1600\n",
            "  Final value: 1.536815\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 4.102 seconds\n",
            "  Iterations: 1600\n",
            "  Final value: 1.410606\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Dimension: 32\n",
            "\n",
            "Experiment 1/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.169 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.179 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.182 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.173 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.178 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 17.300437\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.193 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 30.317067\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Warning: Could not create convergence plot: cannot convert float infinity to integer\n",
            "Warning: Could not create convergence plot: cannot convert float infinity to integer\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running BFGS...\n",
            "  Runtime: 0.063 seconds\n",
            "  Iterations: 170\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running newton-cg...\n",
            "  Runtime: 175.772 seconds\n",
            "  Iterations: 4942\n",
            "  Final value: 0.020632\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-exact...\n",
            "  Runtime: 243.026 seconds\n",
            "  Iterations: 6400\n",
            "  Final value: 18.761311\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 218.935 seconds\n",
            "  Iterations: 6400\n",
            "  Final value: 19.369334\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-675b75c20866>:47: RuntimeWarning: Mean of empty slice\n",
            "  f_mean = np.nanmean(f_values, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1741: RuntimeWarning: invalid value encountered in subtract\n",
            "  np.subtract(arr, avg, out=arr, casting='unsafe', where=where)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "<ipython-input-17-675b75c20866>:49: RuntimeWarning: Mean of empty slice\n",
            "  grad_mean = np.nanmean(grad_norms, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/matplotlib/ticker.py:2392: RuntimeWarning: overflow encountered in power\n",
            "  ticklocs = b ** decades\n",
            "<ipython-input-17-675b75c20866>:231: RuntimeWarning: Mean of empty slice\n",
            "  mean_values = np.nanmean(values, axis=0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Could not create convergence plot: cannot convert float infinity to integer\n",
            "\n",
            "Experiment 2/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-8aff6a75a1a8>:157: RuntimeWarning: overflow encountered in square\n",
            "  return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
            "<ipython-input-18-f547604badb2>:6: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_sim = np.dot(grad_current, grad_previous) / (np.linalg.norm(grad_current) * np.linalg.norm(grad_previous))\n",
            "<ipython-input-19-d5f145bdae92>:14: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  return (f_previous - f_current) / abs(f_previous) * 100\n",
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: overflow encountered in square\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: overflow encountered in multiply\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: invalid value encountered in subtract\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
            "<ipython-input-15-8aff6a75a1a8>:157: RuntimeWarning: invalid value encountered in subtract\n",
            "  return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.172 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.176 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-8aff6a75a1a8>:164: RuntimeWarning: overflow encountered in scalar power\n",
            "  grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.177 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-6ffe1a315b08>:189: RuntimeWarning: invalid value encountered in subtract\n",
            "  v = self.momentum * v - self.learning_rate * g\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.172 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.178 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 16.737352\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.188 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 29.067257\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-675b75c20866>:344: RuntimeWarning: Mean of empty slice\n",
            "  mean_values = np.nanmean(values, axis=0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running BFGS...\n",
            "  Runtime: 0.065 seconds\n",
            "  Iterations: 170\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running newton-cg...\n",
            "  Runtime: 171.082 seconds\n",
            "  Iterations: 4938\n",
            "  Final value: 0.019051\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-exact...\n",
            "  Runtime: 237.614 seconds\n",
            "  Iterations: 6400\n",
            "  Final value: 18.898570\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 212.740 seconds\n",
            "  Iterations: 6400\n",
            "  Final value: 19.201061\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-675b75c20866>:47: RuntimeWarning: Mean of empty slice\n",
            "  f_mean = np.nanmean(f_values, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1741: RuntimeWarning: invalid value encountered in subtract\n",
            "  np.subtract(arr, avg, out=arr, casting='unsafe', where=where)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "<ipython-input-17-675b75c20866>:49: RuntimeWarning: Mean of empty slice\n",
            "  grad_mean = np.nanmean(grad_norms, axis=0)\n",
            "<ipython-input-17-675b75c20866>:231: RuntimeWarning: Mean of empty slice\n",
            "  mean_values = np.nanmean(values, axis=0)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:118: RuntimeWarning: invalid value encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1746: RuntimeWarning: overflow encountered in multiply\n",
            "  sqr = np.multiply(arr, arr, out=arr, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dimension: 128\n",
            "Dimension 128: Skipping trust-region methods\n",
            "\n",
            "Experiment 1/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-8aff6a75a1a8>:157: RuntimeWarning: overflow encountered in square\n",
            "  return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
            "<ipython-input-18-f547604badb2>:6: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  cosine_sim = np.dot(grad_current, grad_previous) / (np.linalg.norm(grad_current) * np.linalg.norm(grad_previous))\n",
            "<ipython-input-15-8aff6a75a1a8>:157: RuntimeWarning: overflow encountered in multiply\n",
            "  return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
            "<ipython-input-19-d5f145bdae92>:14: RuntimeWarning: invalid value encountered in scalar subtract\n",
            "  return (f_previous - f_current) / abs(f_previous) * 100\n",
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: overflow encountered in square\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: overflow encountered in multiply\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
            "<ipython-input-15-8aff6a75a1a8>:167: RuntimeWarning: invalid value encountered in subtract\n",
            "  grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
            "<ipython-input-15-8aff6a75a1a8>:157: RuntimeWarning: invalid value encountered in subtract\n",
            "  return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
            "<ipython-input-15-8aff6a75a1a8>:164: RuntimeWarning: overflow encountered in scalar power\n",
            "  grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.192 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.216 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-6ffe1a315b08>:135: RuntimeWarning: invalid value encountered in subtract\n",
            "  v = self.momentum * v - self.learning_rate * g\n",
            "<ipython-input-15-8aff6a75a1a8>:164: RuntimeWarning: overflow encountered in scalar multiply\n",
            "  grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.211 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-6ffe1a315b08>:189: RuntimeWarning: invalid value encountered in subtract\n",
            "  v = self.momentum * v - self.learning_rate * g\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Runtime: 0.192 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: nan\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.198 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 112.051371\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.218 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 124.044062\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-675b75c20866>:344: RuntimeWarning: Mean of empty slice\n",
            "  mean_values = np.nanmean(values, axis=0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running BFGS...\n",
            "  Runtime: 0.662 seconds\n",
            "  Iterations: 608\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running newton-cg...\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Get credentials\n",
        "    credentials = setup_experiment()\n",
        "    if not credentials:\n",
        "        return\n",
        "\n",
        "    # Initialize Git manager\n",
        "    git_manager = GitManager(credentials)\n",
        "\n",
        "    # Setup local repository\n",
        "    local_path = \"optimization_results\"\n",
        "    try:\n",
        "        git_manager.setup_repo(local_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to setup repository: {e}\")\n",
        "        return\n",
        "\n",
        "    # Add all test functions\n",
        "    test_functions = {\n",
        "        'rosenbrock': (\n",
        "            TestFunctions.rosenbrock,\n",
        "            TestFunctions.rosenbrock_gradient,\n",
        "            TestFunctions.rosenbrock_hessian\n",
        "        )\n",
        "    }\n",
        "    \"\"\"\n",
        "    test_functions = {\n",
        "        'ackley': (\n",
        "            TestFunctions.ackley,\n",
        "            TestFunctions.ackley_gradient,\n",
        "            TestFunctions.ackley_hessian\n",
        "        ),\n",
        "        'rastrigin': (\n",
        "            TestFunctions.rastrigin,\n",
        "            TestFunctions.rastrigin_gradient,\n",
        "            TestFunctions.rastrigin_hessian\n",
        "        ),\n",
        "        'schwefel': (\n",
        "            TestFunctions.schwefel,\n",
        "            TestFunctions.schwefel_gradient,\n",
        "            TestFunctions.schwefel_hessian\n",
        "        ),\n",
        "        'sphere': (\n",
        "            TestFunctions.sphere,\n",
        "            TestFunctions.sphere_gradient,\n",
        "            TestFunctions.sphere_hessian\n",
        "        ),\n",
        "        'sum_squares': (\n",
        "            TestFunctions.sum_squares,\n",
        "            TestFunctions.sum_squares_gradient,\n",
        "            TestFunctions.sum_squares_hessian\n",
        "        ),\n",
        "        'rosenbrock': (\n",
        "            TestFunctions.rosenbrock,\n",
        "            TestFunctions.rosenbrock_gradient,\n",
        "            TestFunctions.rosenbrock_hessian\n",
        "        )\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    distance_constraints = {\n",
        "        'ackley': {'min': 10, 'max': 30},\n",
        "        'rastrigin': {'min': 2, 'max': 5},\n",
        "        'schwefel': {'min': 1, 'max': 2},\n",
        "        'sphere': {'min': 1, 'max': 5},\n",
        "        'sum_squares': {'min': 1, 'max': 2},\n",
        "        'rosenbrock': {'min': 0.5, 'max': 1.5}  # Keep points relatively close to global minimum at [1,1]\n",
        "    }\n",
        "\n",
        "    # All first-order optimizers with configurations\n",
        "    first_order_optimizers = {\n",
        "        'gradient_descent': GradientDescent(learning_rate=0.01),\n",
        "        'sgd': SGD(learning_rate=0.01, noise_scale=0.01),\n",
        "        'sgd_momentum': SGDMomentum(learning_rate=0.01, momentum=0.9, noise_scale=0.01),\n",
        "        'momentum': MomentumGD(learning_rate=0.01, momentum=0.9),\n",
        "        'rmsprop': RMSprop(learning_rate=0.01, decay_rate=0.9),\n",
        "        'adam': Adam(learning_rate=0.001)\n",
        "    }\n",
        "\n",
        "    # Second-order methods\n",
        "    second_order_methods = ['BFGS', 'newton-cg', 'trust-exact', 'trust-krylov']\n",
        "\n",
        "    # Just test 2D for now\n",
        "    dimensions = [2, 8, 32, 128, 512]\n",
        "\n",
        "    # Create experiment manager with distance constraints\n",
        "    experiment = ExperimentManager(\n",
        "        test_functions=test_functions,\n",
        "        first_order_optimizers=first_order_optimizers,\n",
        "        second_order_methods=second_order_methods,\n",
        "        dimensions=dimensions,\n",
        "        n_experiments=2,\n",
        "        min_dist=2,  # These become the defaults for functions not in distance_constraints\n",
        "        max_dist=5,\n",
        "        distance_constraints=distance_constraints\n",
        "    )\n",
        "\n",
        "    # Run experiments\n",
        "    experiment.run_experiments()\n",
        "\n",
        "    # Push results to GitHub\n",
        "    print(\"\\nPushing results to GitHub...\")\n",
        "    git_manager.push_results(local_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN47rPnUH5hp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}