{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INITIAL TEST"
      ],
      "metadata": {
        "id": "eiRTKba4bQw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn\n",
        "!pip install gitpython PyGithub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KkKYrlBwFZs",
        "outputId": "85535de3-ab8a-4251-d14a-aeef9f32a031"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (3.1.43)\n",
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython) (4.0.11)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.2.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from PyGithub) (1.2.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.8.30)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OptimizationResult, First-order Optimisers and Functions"
      ],
      "metadata": {
        "id": "nNwklgZfF1sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizationResult:\n",
        "    \"\"\"Enhanced optimization result storage\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        self.x_final = kwargs.get('x_final')\n",
        "        self.f_final = kwargs.get('f_final')\n",
        "        self.success = kwargs.get('success')\n",
        "        self.iterations = kwargs.get('iterations')\n",
        "        self.runtime = kwargs.get('runtime')\n",
        "        self.path = kwargs.get('path', [])\n",
        "        self.f_path = kwargs.get('f_path', [])\n",
        "        self.grad_norm_path = kwargs.get('grad_norm_path', [])\n",
        "        self.grad_cosine_sim_path = kwargs.get('grad_cosine_sim_path', [])  # New\n",
        "        self.grad_angle_path = kwargs.get('grad_angle_path', [])            # New\n",
        "        self.timestamps = kwargs.get('timestamps', [])\n",
        "        self.method = kwargs.get('method')\n",
        "        self.dimension = kwargs.get('dimension')\n",
        "        self.function_name = kwargs.get('function_name')\n",
        "        self.x_initial = kwargs.get('x_initial')\n",
        "        self.f_initial = kwargs.get('f_initial')\n",
        "        self.grad_initial = kwargs.get('grad_initial')\n",
        "        self.grad_final = kwargs.get('grad_final')\n",
        "        self.step_sizes = kwargs.get('step_sizes', [])\n",
        "        self.improvements = kwargs.get('improvements', [])\n",
        "        self.best_so_far = kwargs.get('best_so_far', [])\n",
        "        self.relative_improvements = kwargs.get('relative_improvements', [])\n",
        "        self.distance_to_minimum_path = kwargs.get('distance_to_minimum_path', [])\n",
        "        self.distance_to_minimum = None\n",
        "\n",
        "        # Get global minimum using the function name and dimension we already have\n",
        "        x_min, f_min = TestFunctions.get_global_minimum(self.function_name, self.dimension)\n",
        "        if x_min is not None and f_min is not None:\n",
        "            # Use the final value from path if available, otherwise calculate\n",
        "            if self.distance_to_minimum_path:\n",
        "                self.distance_to_minimum = self.distance_to_minimum_path[-1]\n",
        "            else:\n",
        "                self.distance_to_minimum = np.linalg.norm(self.x_final - x_min)\n",
        "            self.f_error = abs(self.f_final - f_min)\n",
        "        else:\n",
        "            self.distance_to_minimum = None\n",
        "            self.f_error = None"
      ],
      "metadata": {
        "id": "07O9nE0oHegS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jwVvDUwtaQgE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, List, Dict, Optional, Tuple\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "class TestFunctions:\n",
        "    \"\"\"Test functions that work with any dimension\"\"\"\n",
        "    @staticmethod\n",
        "    def get_global_minimum(func_name: str, dimension: int = 2) -> tuple:\n",
        "        \"\"\"Get global minimum for a given function and dimension\"\"\"\n",
        "        global_minima = {\n",
        "            'ackley': (np.zeros(dimension), 0.0),\n",
        "            'rastrigin': (np.zeros(dimension), 0.0),\n",
        "            'rosenbrock': (np.ones(dimension), 0.0),\n",
        "            'sphere': (np.zeros(dimension), 0.0),\n",
        "            'schwefel': (420.9687 * np.ones(dimension), 0.0),  # Add this\n",
        "            'sum_squares': (np.zeros(dimension), 0.0),         # Add this\n",
        "            'michalewicz': (None, None),  # Varies with dimension\n",
        "        }\n",
        "        return global_minima.get(func_name, (None, None))\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley(x: np.ndarray) -> float:\n",
        "        \"\"\"Ackley function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "        return (-20 * np.exp(-0.2 * np.sqrt(sum_sq / n))\n",
        "                - np.exp(sum_cos / n)\n",
        "                + 20 + np.e)\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Ackley function\"\"\"\n",
        "        n = len(x)\n",
        "        sum_sq = np.sum(x**2)\n",
        "        sum_cos = np.sum(np.cos(2 * np.pi * x))\n",
        "\n",
        "        term1 = (20 * 0.2 / np.sqrt(n * sum_sq)) * np.exp(-0.2 * np.sqrt(sum_sq / n)) * x\n",
        "        term2 = (2 * np.pi / n) * np.exp(sum_cos / n) * np.sin(2 * np.pi * x)\n",
        "        return term1 + term2\n",
        "\n",
        "    @staticmethod\n",
        "    def ackley_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Ackley Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.ackley_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin(x: np.ndarray) -> float:\n",
        "        \"\"\"Rastrigin function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        return 10 * n + np.sum(x**2 - 10 * np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rastrigin function\"\"\"\n",
        "        return 2 * x + 20 * np.pi * np.sin(2 * np.pi * x)\n",
        "\n",
        "    @staticmethod\n",
        "    def rastrigin_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Rastrigin function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n) + 40 * np.pi**2 * np.diag(np.cos(2 * np.pi * x))\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel(x: np.ndarray) -> float:\n",
        "        \"\"\"Schwefel function for n dimensions\"\"\"\n",
        "        n = len(x)\n",
        "        return 418.9829 * n - np.sum(x * np.sin(np.sqrt(np.abs(x))))\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Schwefel function\"\"\"\n",
        "        sqrt_abs_x = np.sqrt(np.abs(x))\n",
        "        term1 = np.sin(sqrt_abs_x)\n",
        "        term2 = x * np.cos(sqrt_abs_x) / (2 * sqrt_abs_x)\n",
        "        return -(term1 + term2)\n",
        "\n",
        "    @staticmethod\n",
        "    def schwefel_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Schwefel Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.schwefel_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere(x: np.ndarray) -> float:\n",
        "        \"\"\"Sphere function for n dimensions\"\"\"\n",
        "        return np.sum(x**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Sphere function\"\"\"\n",
        "        return 2 * x\n",
        "\n",
        "    @staticmethod\n",
        "    def sphere_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Sphere function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.eye(n)\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares(x: np.ndarray) -> float:\n",
        "        \"\"\"Sum squares function for n dimensions\"\"\"\n",
        "        return np.sum((np.arange(1, len(x) + 1) * x**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Sum squares function\"\"\"\n",
        "        return 2 * np.arange(1, len(x) + 1) * x\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_squares_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Hessian of Sum squares function\"\"\"\n",
        "        n = len(x)\n",
        "        return 2 * np.diag(np.arange(1, n + 1))\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock(x: np.ndarray) -> float:\n",
        "        \"\"\"Rosenbrock function for n dimensions\"\"\"\n",
        "        return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rosenbrock function\"\"\"\n",
        "        n = len(x)\n",
        "        grad = np.zeros(n)\n",
        "        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "        grad[-1] = 200 * (x[-1] - x[-2]**2)\n",
        "        if n > 2:\n",
        "            grad[1:-1] = 200 * (x[1:-1] - x[:-2]**2) - 400 * x[1:-1] * (x[2:] - x[1:-1]**2) - 2 * (1 - x[1:-1])\n",
        "        return grad\n",
        "\n",
        "    @staticmethod\n",
        "    def rosenbrock_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Rosenbrock Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.rosenbrock_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2  # Ensure symmetry\n",
        "\n",
        "    @staticmethod\n",
        "    def michalewicz(x: np.ndarray) -> float:\n",
        "        \"\"\"Michalewicz function for n dimensions\"\"\"\n",
        "        i = np.arange(1, len(x) + 1)\n",
        "        return -np.sum(np.sin(x) * (np.sin(i * x**2 / np.pi))**(2 * 10))\n",
        "\n",
        "    @staticmethod\n",
        "    def michalewicz_gradient(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Michalewicz function\"\"\"\n",
        "        n = len(x)\n",
        "        i = np.arange(1, n + 1)\n",
        "        term1 = -np.cos(x) * (np.sin(i * x**2 / np.pi))**(2 * 10)\n",
        "        term2 = -np.sin(x) * (2 * 10) * (np.sin(i * x**2 / np.pi))**(2 * 10 - 1) * \\\n",
        "                np.cos(i * x**2 / np.pi) * (2 * i * x / np.pi)\n",
        "        return -(term1 + term2)\n",
        "\n",
        "    @staticmethod\n",
        "    def michalewicz_hessian(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Numerical approximation of Michalewicz Hessian\"\"\"\n",
        "        eps = 1e-8\n",
        "        n = len(x)\n",
        "        H = np.zeros((n, n))\n",
        "        grad = TestFunctions.michalewicz_gradient\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                x_ij = x.copy()\n",
        "                x_ij[i] += eps\n",
        "                x_ij[j] += eps\n",
        "                H[i,j] = (grad(x_ij)[i] - grad(x)[i]) / eps\n",
        "\n",
        "        return (H + H.T) / 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            x = x - self.learning_rate * g\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum  # Changed to path\n",
        "        }\n",
        "\n",
        "class SGD:\n",
        "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, noise_scale=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            # Add stochastic noise\n",
        "            noise = np.random.normal(0, self.noise_scale, size=x.shape)\n",
        "            g = g + noise\n",
        "            x = x - self.learning_rate * g\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum\n",
        "        }\n",
        "\n",
        "class SGDMomentum:\n",
        "    \"\"\"SGD with momentum optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9, noise_scale=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)  # Initialize velocity\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            # Add stochastic noise\n",
        "            noise = np.random.normal(0, self.noise_scale, size=x.shape)\n",
        "            g = g + noise\n",
        "\n",
        "            # Update with momentum\n",
        "            v = self.momentum * v - self.learning_rate * g\n",
        "            x = x + v\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum\n",
        "        }\n",
        "\n",
        "class MomentumGD:\n",
        "    \"\"\"Gradient Descent with Momentum\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            v = self.momentum * v - self.learning_rate * g\n",
        "            x = x + v\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum\n",
        "        }\n",
        "\n",
        "class RMSprop:\n",
        "    \"\"\"RMSprop optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.01, decay_rate=0.9, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        v = np.zeros_like(x)\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            v = self.decay_rate * v + (1 - self.decay_rate) * g**2\n",
        "            x = x - self.learning_rate * g / (np.sqrt(v) + self.epsilon)\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum\n",
        "        }\n",
        "\n",
        "class Adam:\n",
        "    \"\"\"Adam optimizer\"\"\"\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def optimize(self, f: Callable, grad: Callable, x0: np.ndarray,\n",
        "            function_name: str,  # Add this parameter\n",
        "            max_iter=1000, tol=1e-6, callback=None) -> dict:\n",
        "        x = x0.copy()\n",
        "        m = np.zeros_like(x)\n",
        "        v = np.zeros_like(x)\n",
        "        step_logger = StepLogger()\n",
        "\n",
        "        # Get the global minimum for distance tracking\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "\n",
        "        # Log initial point\n",
        "        f_initial = f(x)\n",
        "        g_initial = grad(x)\n",
        "        step_logger.log_iteration(x, f_initial, g_initial, global_minimum=x_min)\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            g = grad(x)\n",
        "            m = self.beta1 * m + (1 - self.beta1) * g\n",
        "            v = self.beta2 * v + (1 - self.beta2) * g**2\n",
        "\n",
        "            # Bias correction\n",
        "            m_hat = m / (1 - self.beta1**(i + 1))\n",
        "            v_hat = v / (1 - self.beta2**(i + 1))\n",
        "\n",
        "            x = x - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "            # Log step\n",
        "            f_val = f(x)\n",
        "            grad_norm = np.linalg.norm(g)\n",
        "            step_logger.log_iteration(x, f_val, grad_norm, global_minimum=x_min)\n",
        "\n",
        "            if callback:\n",
        "                callback(x)\n",
        "\n",
        "            if grad_norm < tol:\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'x': x,\n",
        "            'fun': f(x),\n",
        "            'success': grad_norm < tol,\n",
        "            'nit': i + 1,\n",
        "            'path': step_logger.path,\n",
        "            'f_path': step_logger.f_path,\n",
        "            'grad_path': step_logger.grad_norm_path,\n",
        "            'timestamps': step_logger.timestamps,\n",
        "            'grad_final': grad(x),\n",
        "            'step_sizes': step_logger.step_sizes,\n",
        "            'improvements': step_logger.improvements,\n",
        "            'best_so_far': step_logger.best_so_far,\n",
        "            'relative_improvements': step_logger.relative_improvements,\n",
        "            'distance_to_minimum_path': step_logger.distance_to_minimum\n",
        "        }"
      ],
      "metadata": {
        "id": "sGlrjFy2pnuJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation"
      ],
      "metadata": {
        "id": "Nny9ft1FGJQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Visualizer:\n",
        "    \"\"\"Visualization of optimization trajectories and convergence\"\"\"\n",
        "    @staticmethod\n",
        "    def plot_convergence(results: Dict[str, OptimizationResult], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot convergence with error handling\"\"\"\n",
        "        try:\n",
        "            plt.figure(figsize=(20, 8))\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "            # Color map for different optimizers\n",
        "            colors = plt.cm.tab20(np.linspace(0, 1, len(results)))\n",
        "\n",
        "            # Get global minimum if available\n",
        "            _, f_min = TestFunctions.get_global_minimum(function_name)\n",
        "            f_min_text = f\"(Global min: {f_min})\" if f_min is not None else \"\"\n",
        "\n",
        "            # Function value convergence with error handling\n",
        "            for (method, result), color in zip(results.items(), colors):\n",
        "                try:\n",
        "                    if result and hasattr(result, 'f_path') and result.f_path:\n",
        "                        ax1.semilogy(result.f_path, label=f\"{method}\", color=color)\n",
        "                except (OverflowError, ValueError, RuntimeError) as e:\n",
        "                    print(f\"Warning: Could not plot convergence for {method}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            ax1.set_xlabel('Iteration')\n",
        "            ax1.set_ylabel('Function Value (log scale)')\n",
        "            ax1.set_title(f'Function Value Convergence {f_min_text}')\n",
        "            ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            ax1.grid(True)\n",
        "\n",
        "            # Gradient norm convergence with error handling\n",
        "            for (method, result), color in zip(results.items(), colors):\n",
        "                try:\n",
        "                    if result and hasattr(result, 'grad_norm_path') and result.grad_norm_path:\n",
        "                        ax2.semilogy(result.grad_norm_path, label=f\"{method}\", color=color)\n",
        "                except (OverflowError, ValueError, RuntimeError) as e:\n",
        "                    print(f\"Warning: Could not plot gradient norm for {method}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            ax2.set_xlabel('Iteration')\n",
        "            ax2.set_ylabel('Gradient Norm (log scale)')\n",
        "            ax2.set_title('Gradient Norm Convergence')\n",
        "            ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            ax2.grid(True)\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "            plt.savefig(os.path.join(save_dir, function_name), dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not create convergence plot: {str(e)}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_2d_trajectory(f: Callable, result: OptimizationResult, save_dir: str,\n",
        "                          experiment_num: int = None, equal_aspect: bool = True):\n",
        "        \"\"\"Plot optimization trajectory for 2D problems\"\"\"\n",
        "        if result.dimension != 2:\n",
        "            return\n",
        "\n",
        "        # Even more reduced resolution for better performance\n",
        "        grid_size = 50\n",
        "\n",
        "        for show_global_min in [True, False]:  # Create both variants\n",
        "            plt.figure(figsize=(12, 10))\n",
        "\n",
        "            # Get path bounds and include (0,0)\n",
        "            path = np.array(result.path)\n",
        "            x_min_traj = min(float(path[:, 0].min()), 0)\n",
        "            x_max_traj = max(float(path[:, 0].max()), 0)\n",
        "            y_min_traj = min(float(path[:, 1].min()), 0)\n",
        "            y_max_traj = max(float(path[:, 1].max()), 0)\n",
        "\n",
        "            # Add margin to bounds\n",
        "            margin = 0.1\n",
        "            x_range = x_max_traj - x_min_traj\n",
        "            y_range = y_max_traj - y_min_traj\n",
        "\n",
        "            if equal_aspect:\n",
        "                max_range = max(x_range, y_range)\n",
        "                x_center = (x_max_traj + x_min_traj) / 2\n",
        "                y_center = (y_max_traj + y_min_traj) / 2\n",
        "                x_min_traj = x_center - max_range/2\n",
        "                x_max_traj = x_center + max_range/2\n",
        "                y_min_traj = y_center - max_range/2\n",
        "                y_max_traj = y_center + max_range/2\n",
        "                x_range = y_range = max_range\n",
        "\n",
        "            plot_x_min = x_min_traj - margin * x_range\n",
        "            plot_x_max = x_max_traj + margin * x_range\n",
        "            plot_y_min = y_min_traj - margin * y_range\n",
        "            plot_y_max = y_max_traj + margin * y_range\n",
        "\n",
        "            # Create contour plot with reduced resolution\n",
        "            x = np.linspace(plot_x_min, plot_x_max, grid_size)\n",
        "            y = np.linspace(plot_y_min, plot_y_max, grid_size)\n",
        "            X, Y = np.meshgrid(x, y)\n",
        "\n",
        "            # Compute Z values\n",
        "            points = np.column_stack((X.ravel(), Y.ravel()))\n",
        "            Z = np.array([f(point) for point in points]).reshape(X.shape)\n",
        "\n",
        "            # Reduced number of contour levels\n",
        "            global_max = float(Z.max())\n",
        "            global_levels = np.linspace(0, global_max, 15)\n",
        "\n",
        "            # Plot contours\n",
        "            contour = plt.contour(X, Y, Z, levels=global_levels, cmap='viridis', alpha=0.7)\n",
        "            plt.colorbar(contour, label='Function Value')\n",
        "\n",
        "            # Plot trajectory\n",
        "            plt.plot(path[:, 0], path[:, 1], 'r.-', label='Optimization Path',\n",
        "                    linewidth=1, markersize=2, zorder=5)\n",
        "            plt.plot(path[0, 0], path[0, 1], 'go', label='Start',\n",
        "                    markersize=8, zorder=6)\n",
        "            plt.plot(path[-1, 0], path[-1, 1], 'ro', label='End',\n",
        "                    markersize=8, zorder=6)\n",
        "\n",
        "            # Only plot global minimum in the first variant\n",
        "            if show_global_min:\n",
        "                x_min, f_min = TestFunctions.get_global_minimum(result.function_name)\n",
        "                if x_min is not None:\n",
        "                    plt.plot(x_min[0], x_min[1], 'k*', label='Global Minimum',\n",
        "                            markersize=10, zorder=6)\n",
        "\n",
        "            plt.xlim(float(plot_x_min), float(plot_x_max))\n",
        "            plt.ylim(float(plot_y_min), float(plot_y_max))\n",
        "\n",
        "            if equal_aspect:\n",
        "                plt.gca().set_aspect('equal')\n",
        "\n",
        "            plt.grid(True)\n",
        "            plt.title(f'{result.function_name} - {result.method}\\n'\n",
        "                    f'Final value: {result.f_final:.6f}\\n'\n",
        "                    f'Iterations: {result.iterations}')\n",
        "            plt.xlabel('x₁')\n",
        "            plt.ylabel('x₂')\n",
        "            plt.legend()\n",
        "\n",
        "            # Include experiment number and variant in filename\n",
        "            experiment_suffix = f'_exp{experiment_num}' if experiment_num is not None else ''\n",
        "            variant_suffix = '_with_global_min' if show_global_min else '_path_only'\n",
        "            aspect_suffix = '_equal_aspect' if equal_aspect else ''\n",
        "            filename = f'trajectory_{result.function_name}_{result.method}{experiment_suffix}{variant_suffix}{aspect_suffix}.png'\n",
        "\n",
        "            plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_gradient_metrics(results: Dict[str, OptimizationResult], save_dir: str, function_name: str):\n",
        "        \"\"\"Plot gradient metrics with error handling\"\"\"\n",
        "        try:\n",
        "            plt.figure(figsize=(20, 15))\n",
        "            fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 15))\n",
        "\n",
        "            colors = plt.cm.tab20(np.linspace(0, 1, len(results)))\n",
        "\n",
        "            # Plot each metric with error handling\n",
        "            for (method, result), color in zip(results.items(), colors):\n",
        "                try:\n",
        "                    if result and hasattr(result, 'grad_norm_path') and result.grad_norm_path:\n",
        "                        ax1.semilogy(result.grad_norm_path, label=f\"{method}\", color=color)\n",
        "                except (OverflowError, ValueError, RuntimeError) as e:\n",
        "                    print(f\"Warning: Could not plot gradient norm for {method}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            ax1.set_xlabel('Iteration')\n",
        "            ax1.set_ylabel('Gradient Norm (log scale)')\n",
        "            ax1.set_title('Gradient Norm Convergence')\n",
        "            ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            ax1.grid(True)\n",
        "\n",
        "            for (method, result), color in zip(results.items(), colors):\n",
        "                try:\n",
        "                    if result and hasattr(result, 'grad_cosine_sim_path') and result.grad_cosine_sim_path:\n",
        "                        ax2.plot(result.grad_cosine_sim_path[1:], label=f\"{method}\", color=color)\n",
        "                except (OverflowError, ValueError, RuntimeError) as e:\n",
        "                    print(f\"Warning: Could not plot cosine similarity for {method}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            ax2.set_xlabel('Iteration')\n",
        "            ax2.set_ylabel('Cosine Similarity')\n",
        "            ax2.set_title('Gradient Cosine Similarity')\n",
        "            ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            ax2.grid(True)\n",
        "\n",
        "            for (method, result), color in zip(results.items(), colors):\n",
        "                try:\n",
        "                    if result and hasattr(result, 'grad_angle_path') and result.grad_angle_path:\n",
        "                        ax3.plot(result.grad_angle_path[1:], label=f\"{method}\", color=color)\n",
        "                except (OverflowError, ValueError, RuntimeError) as e:\n",
        "                    print(f\"Warning: Could not plot gradient angle for {method}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            ax3.set_xlabel('Iteration')\n",
        "            ax3.set_ylabel('Angle (degrees)')\n",
        "            ax3.set_title('Gradient Angle Between Steps')\n",
        "            ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            ax3.grid(True)\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "            plt.savefig(os.path.join(save_dir, function_name), dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not create gradient metrics plot: {str(e)}\")\n",
        "\n",
        "    @staticmethod\n",
        "    # Enhanced OptimizationLogger visualization methods\n",
        "    def plot_step_metrics(results: Dict[str, OptimizationResult], save_dir: str,\n",
        "                        function_name: str, experiment_num: Optional[int] = None):\n",
        "        \"\"\"Plot step-related metrics\"\"\"\n",
        "        metrics = [\n",
        "            ('step_sizes', 'Step Size', 'Step Size'),\n",
        "            ('improvements', 'Improvement per Step (%)', 'Improvement'),\n",
        "            ('distance_to_minimum', 'Distance to Global Minimum', 'Distance'),\n",
        "            ('best_so_far', 'Best Value So Far', 'Value'),\n",
        "            ('relative_improvements', 'Relative Improvement (%)', 'Improvement')\n",
        "        ]\n",
        "\n",
        "        for metric_name, ylabel, title_suffix in metrics:\n",
        "            plt.figure(figsize=(12, 8))\n",
        "\n",
        "            for method, result in results.items():\n",
        "                if hasattr(result, metric_name):\n",
        "                    data = getattr(result, metric_name)\n",
        "                    if data:  # Only plot if we have data\n",
        "                        plt.plot(data, label=method)\n",
        "\n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel(ylabel)\n",
        "            plt.title(f'{function_name} - {title_suffix}')\n",
        "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "\n",
        "            # Create metric-specific directories\n",
        "            metric_dir = os.path.join(save_dir, metric_name.replace('_', ''))\n",
        "            os.makedirs(metric_dir, exist_ok=True)\n",
        "\n",
        "            # Save with experiment number if provided\n",
        "            suffix = f'_exp_{experiment_num}' if experiment_num is not None else '_summary'\n",
        "            filename = f'{metric_name}_{function_name}{suffix}.png'\n",
        "            plt.savefig(os.path.join(metric_dir, filename), dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n"
      ],
      "metadata": {
        "id": "T-a_OSOTFlC4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics and Logging"
      ],
      "metadata": {
        "id": "ngaQ15naF6yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_metrics(grad_current: np.ndarray, grad_previous: np.ndarray) -> Tuple[float, float]:\n",
        "    \"\"\"Compute cosine similarity and angle between two gradient vectors\"\"\"\n",
        "    if np.all(grad_previous == 0) or np.all(grad_current == 0):\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    cosine_sim = np.dot(grad_current, grad_previous) / (np.linalg.norm(grad_current) * np.linalg.norm(grad_previous))\n",
        "    # Clip to handle numerical errors\n",
        "    cosine_sim = np.clip(cosine_sim, -1.0, 1.0)\n",
        "    angle = np.arccos(cosine_sim) * 180 / np.pi\n",
        "\n",
        "    return cosine_sim, angle\n",
        "\n",
        "# Helper function to compute summary metrics\n",
        "def compute_summary_metrics(results: List[OptimizationResult]) -> Dict:\n",
        "    \"\"\"Compute summary statistics for new metrics\"\"\"\n",
        "    summary = {}\n",
        "\n",
        "    # Step size statistics\n",
        "    step_sizes = [np.mean(r.step_sizes) for r in results if hasattr(r, 'step_sizes')]\n",
        "    if step_sizes:\n",
        "        summary.update({\n",
        "            'mean_step_size': np.mean(step_sizes),\n",
        "            'std_step_size': np.std(step_sizes)\n",
        "        })\n",
        "\n",
        "    # Improvement statistics\n",
        "    improvements = [np.mean(r.improvements) for r in results if hasattr(r, 'improvements')]\n",
        "    if improvements:\n",
        "        summary.update({\n",
        "            'mean_improvement_per_step': np.mean(improvements),\n",
        "            'std_improvement_per_step': np.std(improvements)\n",
        "        })\n",
        "\n",
        "    # Plateau statistics\n",
        "    plateau_metrics = [StepMetricsCalculator.compute_plateau_metrics(r.f_path)\n",
        "                      for r in results]\n",
        "    if plateau_metrics:\n",
        "        summary.update({\n",
        "            'mean_plateau_percentage': np.mean([m['plateau_percentage'] for m in plateau_metrics]),\n",
        "            'mean_plateau_length': np.mean([m['max_plateau_length'] for m in plateau_metrics]),\n",
        "            'mean_num_plateaus': np.mean([m['num_plateaus'] for m in plateau_metrics])\n",
        "        })\n",
        "\n",
        "    # Improvement threshold statistics\n",
        "    threshold_metrics = [StepMetricsCalculator.compute_improvement_thresholds(r.f_path)\n",
        "                        for r in results]\n",
        "    if threshold_metrics:\n",
        "        for threshold in [10, 20, 30, 40, 50]:\n",
        "            steps = [m[f\"{threshold}%_improvement_steps\"] for m in threshold_metrics\n",
        "                    if m[f\"{threshold}%_improvement_steps\"] is not None]\n",
        "            if steps:\n",
        "                summary.update({\n",
        "                    f'mean_steps_to_{threshold}%_improvement': np.mean(steps),\n",
        "                    f'std_steps_to_{threshold}%_improvement': np.std(steps)\n",
        "                })\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "xVdCMRiQTPfg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New helper class for computing step metrics\n",
        "class StepMetricsCalculator:\n",
        "    \"\"\"Helper class for computing various step-based metrics\"\"\"\n",
        "    @staticmethod\n",
        "    def compute_step_size(x_current: np.ndarray, x_previous: np.ndarray) -> float:\n",
        "        \"\"\"Compute Euclidean distance between consecutive steps\"\"\"\n",
        "        return np.linalg.norm(x_current - x_previous)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_improvement(f_current: float, f_previous: float) -> float:\n",
        "        \"\"\"Compute relative improvement between steps\"\"\"\n",
        "        if f_previous == 0:\n",
        "            return 0.0\n",
        "        return (f_previous - f_current) / abs(f_previous) * 100\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_plateau_metrics(f_path: List[float], threshold: float = 1.0) -> Dict:\n",
        "        \"\"\"Compute plateau-related metrics\"\"\"\n",
        "        improvements = np.array([abs((f_path[i] - f_path[i-1])/f_path[i-1]*100)\n",
        "                               for i in range(1, len(f_path))])\n",
        "        plateau_mask = improvements < threshold\n",
        "\n",
        "        # Find plateau sequences\n",
        "        plateau_sequences = []\n",
        "        current_sequence = []\n",
        "        for i, is_plateau in enumerate(plateau_mask):\n",
        "            if is_plateau:\n",
        "                current_sequence.append(i)\n",
        "            elif current_sequence:\n",
        "                plateau_sequences.append(current_sequence)\n",
        "                current_sequence = []\n",
        "        if current_sequence:\n",
        "            plateau_sequences.append(current_sequence)\n",
        "\n",
        "        return {\n",
        "            'total_plateau_steps': np.sum(plateau_mask),\n",
        "            'plateau_percentage': np.mean(plateau_mask) * 100 if len(plateau_mask) > 0 else 0,\n",
        "            'max_plateau_length': max([len(seq) for seq in plateau_sequences]) if plateau_sequences else 0,\n",
        "            'num_plateaus': len(plateau_sequences),\n",
        "            'plateau_sequences': plateau_sequences\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_improvement_thresholds(f_path: List[float],\n",
        "                                     thresholds: List[float] = [10, 20, 30, 40, 50]) -> Dict:\n",
        "        \"\"\"Compute steps required for various improvement thresholds\"\"\"\n",
        "        if not f_path:\n",
        "            return {f\"{t}%_improvement_steps\": None for t in thresholds}\n",
        "\n",
        "        initial_value = f_path[0]\n",
        "        results = {}\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            target_value = initial_value * (1 - threshold/100)\n",
        "            steps = next((i for i, v in enumerate(f_path) if v <= target_value), None)\n",
        "            results[f\"{threshold}%_improvement_steps\"] = steps\n",
        "\n",
        "        return results\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_best_so_far(f_path: List[float]) -> Tuple[List[float], List[float]]:\n",
        "        \"\"\"Compute best value so far and relative improvement from initial\"\"\"\n",
        "        if not f_path:\n",
        "            return [], []\n",
        "\n",
        "        best_so_far = []\n",
        "        relative_improvement = []\n",
        "        current_best = float('inf')\n",
        "        initial_value = f_path[0]\n",
        "\n",
        "        for value in f_path:\n",
        "            current_best = min(current_best, value)\n",
        "            best_so_far.append(current_best)\n",
        "            rel_imp = (initial_value - current_best) / abs(initial_value) * 100\n",
        "            relative_improvement.append(rel_imp)\n",
        "\n",
        "        return best_so_far, relative_improvement\n",
        "\n",
        "\n",
        "# Enhance StepLogger class\n",
        "class StepLogger:\n",
        "    \"\"\"Enhanced step logger with additional metrics\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all logging arrays\"\"\"\n",
        "        # Existing attributes\n",
        "        self.path = []\n",
        "        self.f_path = []\n",
        "        self.grad_norm_path = []\n",
        "        self.grad_cosine_sim_path = []\n",
        "        self.grad_angle_path = []\n",
        "        self.timestamps = []\n",
        "        self.start_time = time.time()\n",
        "        self.previous_grad = None\n",
        "\n",
        "        # New attributes\n",
        "        self.step_sizes = []\n",
        "        self.improvements = []\n",
        "        self.best_so_far = []\n",
        "        self.relative_improvements = []\n",
        "        self.distance_to_minimum = []\n",
        "\n",
        "    def log_iteration(self, x: np.ndarray, f: float, grad: np.ndarray,\n",
        "                    global_minimum: Optional[np.ndarray] = None):\n",
        "        \"\"\"Enhanced logging with new metrics\"\"\"\n",
        "        # Existing logging\n",
        "        grad_norm = np.linalg.norm(grad)\n",
        "        if self.previous_grad is not None:\n",
        "            cosine_sim, angle = compute_gradient_metrics(grad, self.previous_grad)\n",
        "            self.grad_cosine_sim_path.append(cosine_sim)\n",
        "            self.grad_angle_path.append(angle)\n",
        "        else:\n",
        "            self.grad_cosine_sim_path.append(0.0)\n",
        "            self.grad_angle_path.append(0.0)\n",
        "\n",
        "        # New metrics\n",
        "        if len(self.path) > 0:\n",
        "            step_size = StepMetricsCalculator.compute_step_size(x, self.path[-1])\n",
        "            improvement = StepMetricsCalculator.compute_improvement(f, self.f_path[-1])\n",
        "            self.step_sizes.append(step_size)\n",
        "            self.improvements.append(improvement)\n",
        "        else:\n",
        "            self.step_sizes.append(0.0)\n",
        "            self.improvements.append(0.0)\n",
        "\n",
        "        # Update best so far and relative improvement\n",
        "        if not self.f_path:\n",
        "            self.best_so_far.append(f)\n",
        "            self.relative_improvements.append(0.0)\n",
        "        else:\n",
        "            self.best_so_far.append(min(f, self.best_so_far[-1]))\n",
        "            rel_imp = (self.f_path[0] - self.best_so_far[-1]) / abs(self.f_path[0]) * 100\n",
        "            self.relative_improvements.append(rel_imp)\n",
        "\n",
        "        # Distance to global minimum if provided\n",
        "        if global_minimum is not None:\n",
        "            dist = np.linalg.norm(x - global_minimum)\n",
        "            self.distance_to_minimum.append(dist)\n",
        "\n",
        "        # Standard logging\n",
        "        self.path.append(x.copy())\n",
        "        self.f_path.append(f)\n",
        "        self.grad_norm_path.append(grad_norm)\n",
        "        self.timestamps.append(time.time() - self.start_time)\n",
        "        self.previous_grad = grad.copy()"
      ],
      "metadata": {
        "id": "IjbLokFAHBgt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizationLogger:\n",
        "    \"\"\"Handles logging of complete optimization experiments\"\"\"\n",
        "    def __init__(self, base_dir: str):\n",
        "        self.base_dir = base_dir\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    def log_run(self, result: OptimizationResult, experiment_num: int):\n",
        "        \"\"\"Enhanced log detailed results for a single optimization run\"\"\"\n",
        "        log_dir = os.path.join(\n",
        "            self.base_dir,\n",
        "            result.function_name,\n",
        "            f\"{result.dimension}D\",\n",
        "            'first_order' if result.method in ['gradient_descent', 'sgd', 'sgd_momentum', 'momentum', 'rmsprop', 'adam'] else 'second_order',\n",
        "            result.method,\n",
        "            \"results\"\n",
        "        )\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Get the minimum length of all arrays\n",
        "        min_length = min(\n",
        "            len(result.timestamps),\n",
        "            len(result.f_path),\n",
        "            len(result.grad_norm_path),\n",
        "            len(result.grad_cosine_sim_path),\n",
        "            len(result.grad_angle_path),\n",
        "            len(result.step_sizes),\n",
        "            len(result.improvements),\n",
        "            len(result.best_so_far),\n",
        "            len(result.relative_improvements),\n",
        "            len(result.distance_to_minimum_path) if result.distance_to_minimum_path is not None else float('inf')\n",
        "        )\n",
        "\n",
        "        # Create detailed step-by-step log with truncated arrays\n",
        "        run_data = {\n",
        "            'iteration': range(min_length),\n",
        "            'timestamp': result.timestamps[:min_length],\n",
        "            'function_value': result.f_path[:min_length],\n",
        "            'gradient_norm': result.grad_norm_path[:min_length],\n",
        "            'gradient_cosine_similarity': result.grad_cosine_sim_path[:min_length],\n",
        "            'gradient_angle': result.grad_angle_path[:min_length],\n",
        "            'step_size': result.step_sizes[:min_length],\n",
        "            'improvement': result.improvements[:min_length],\n",
        "            'best_so_far': result.best_so_far[:min_length],\n",
        "            'relative_improvement': result.relative_improvements[:min_length],\n",
        "        }\n",
        "\n",
        "        # Only add distance_to_minimum if it exists\n",
        "        if result.distance_to_minimum_path is not None:\n",
        "            run_data['distance_to_minimum'] = result.distance_to_minimum_path[:min_length]\n",
        "\n",
        "        # Add parameter values for each dimension\n",
        "        for i in range(result.dimension):\n",
        "            run_data[f'x{i+1}'] = [p[i] for p in result.path[:min_length]]\n",
        "\n",
        "        # Save detailed step-by-step log\n",
        "        step_df = pd.DataFrame(run_data)\n",
        "        step_df.to_csv(\n",
        "            os.path.join(log_dir, f'run_{experiment_num}_steps_{self.timestamp}.csv'),\n",
        "            index=False\n",
        "        )\n",
        "\n",
        "        # Create and save run summary (new)\n",
        "        run_summary = {\n",
        "            'experiment_num': experiment_num,\n",
        "            'initial_value': result.f_initial,\n",
        "            'final_value': result.f_final,\n",
        "            'iterations': result.iterations,\n",
        "            'runtime': result.runtime,\n",
        "            'success': result.success,\n",
        "            'distance_to_minimum': result.distance_to_minimum,\n",
        "            'f_error': result.f_error,\n",
        "            'initial_gradient_norm': np.linalg.norm(result.grad_initial),\n",
        "            'final_gradient_norm': np.linalg.norm(result.grad_final),\n",
        "            'mean_gradient_norm': np.mean(result.grad_norm_path),\n",
        "            'mean_cosine_similarity': np.mean(result.grad_cosine_sim_path[1:]),  # Skip first entry\n",
        "            'mean_gradient_angle': np.mean(result.grad_angle_path[1:]),          # Skip first entry\n",
        "            'std_gradient_norm': np.std(result.grad_norm_path),\n",
        "            'std_cosine_similarity': np.std(result.grad_cosine_sim_path[1:]),\n",
        "            'std_gradient_angle': np.std(result.grad_angle_path[1:])\n",
        "        }\n",
        "\n",
        "        # Save run summary\n",
        "        summary_df = pd.DataFrame([run_summary])\n",
        "        summary_path = os.path.join(log_dir, f'run_summaries_{self.timestamp}.csv')\n",
        "\n",
        "        # Append to existing summary file if it exists, otherwise create new\n",
        "        if os.path.exists(summary_path):\n",
        "            summary_df.to_csv(summary_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            summary_df.to_csv(summary_path, index=False)\n",
        "\n",
        "    def create_dimension_summary(self, function_name: str, dimension: int, results: List[OptimizationResult]):\n",
        "        \"\"\"Create summary statistics across all experiments for all methods at a given dimension\"\"\"\n",
        "        # Group results by method first\n",
        "        method_groups = {}\n",
        "        for result in results:\n",
        "            if result.method not in method_groups:\n",
        "                method_groups[result.method] = []\n",
        "            method_groups[result.method].append(result)\n",
        "\n",
        "        summary_data = []\n",
        "\n",
        "        for method, method_results in method_groups.items():\n",
        "            # Calculate means and stds across all experiments for this method\n",
        "            summary_data.append({\n",
        "                'function': function_name,\n",
        "                'dimension': dimension,\n",
        "                'method': method,\n",
        "                'initial_value_mean': np.mean([r.f_initial for r in method_results]),\n",
        "                'initial_value_std': np.std([r.f_initial for r in method_results]),\n",
        "                'final_value_mean': np.mean([r.f_final for r in method_results]),\n",
        "                'final_value_std': np.std([r.f_final for r in method_results]),\n",
        "                'iterations_mean': np.mean([r.iterations for r in method_results]),\n",
        "                'iterations_std': np.std([r.iterations for r in method_results]),\n",
        "                'runtime_mean': np.mean([r.runtime for r in method_results]),\n",
        "                'runtime_std': np.std([r.runtime for r in method_results]),\n",
        "                'success_rate': np.mean([1 if r.success else 0 for r in method_results]),\n",
        "                'distance_to_minimum_mean': np.mean([r.distance_to_minimum for r in method_results if r.distance_to_minimum is not None]),\n",
        "                'distance_to_minimum_std': np.std([r.distance_to_minimum for r in method_results if r.distance_to_minimum is not None]),\n",
        "                'f_error_mean': np.mean([r.f_error for r in method_results if r.f_error is not None]),\n",
        "                'f_error_std': np.std([r.f_error for r in method_results if r.f_error is not None]),\n",
        "\n",
        "                # Gradient metrics\n",
        "                'initial_gradient_norm_mean': np.mean([np.linalg.norm(r.grad_initial) for r in method_results]),\n",
        "                'initial_gradient_norm_std': np.std([np.linalg.norm(r.grad_initial) for r in method_results]),\n",
        "                'final_gradient_norm_mean': np.mean([np.linalg.norm(r.grad_final) for r in method_results]),\n",
        "                'final_gradient_norm_std': np.std([np.linalg.norm(r.grad_final) for r in method_results]),\n",
        "\n",
        "                # New gradient metrics\n",
        "                'mean_cosine_similarity': np.mean([np.mean(r.grad_cosine_sim_path[1:]) for r in method_results]),\n",
        "                'std_cosine_similarity': np.std([np.mean(r.grad_cosine_sim_path[1:]) for r in method_results]),\n",
        "                'mean_gradient_angle': np.mean([np.mean(r.grad_angle_path[1:]) for r in method_results]),\n",
        "                'std_gradient_angle': np.std([np.mean(r.grad_angle_path[1:]) for r in method_results]),\n",
        "\n",
        "                # Convergence metrics\n",
        "                'convergence_rate_mean': np.mean([(r.f_path[-1] - r.f_path[0]) / len(r.f_path) for r in method_results if r.f_path]),\n",
        "                'convergence_rate_std': np.std([(r.f_path[-1] - r.f_path[0]) / len(r.f_path) for r in method_results if r.f_path]),\n",
        "                'early_convergence_rate_mean': np.mean([\n",
        "                    (r.f_path[min(10, len(r.f_path)-1)] - r.f_path[0]) / min(10, len(r.f_path))\n",
        "                    for r in method_results if r.f_path\n",
        "                ]),\n",
        "                'time_to_improvement_mean': np.mean([\n",
        "                    next((i for i, v in enumerate(r.f_path) if v < 0.9 * r.f_initial), len(r.f_path))\n",
        "                    for r in method_results if r.f_path\n",
        "                ])\n",
        "            })\n",
        "\n",
        "        # Save dimension summary with means and stds\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_dir = os.path.join(self.base_dir, function_name, f\"{dimension}D\")\n",
        "        os.makedirs(summary_dir, exist_ok=True)\n",
        "        summary_df.to_csv(\n",
        "            os.path.join(summary_dir, f'dimension_summary_{self.timestamp}.csv'),\n",
        "            index=False\n",
        "        )\n",
        "\n",
        "    def create_method_summary(self, results: List[OptimizationResult]):\n",
        "        \"\"\"Create summary statistics for each optimization method across all dimensions\"\"\"\n",
        "        for method in set(r.method for r in results):\n",
        "            method_results = [r for r in results if r.method == method]\n",
        "\n",
        "            summary_data = []\n",
        "            for result in method_results:\n",
        "                summary_data.append({\n",
        "                    'function': result.function_name,\n",
        "                    'dimension': result.dimension,\n",
        "                    'final_value': result.f_final,\n",
        "                    'iterations': result.iterations,\n",
        "                    'runtime': result.runtime,\n",
        "                    'success': result.success,\n",
        "                    'distance_to_minimum': result.distance_to_minimum,\n",
        "                    'f_error': result.f_error,\n",
        "                    'mean_cosine_similarity': np.mean(result.grad_cosine_sim_path[1:]),\n",
        "                    'mean_gradient_angle': np.mean(result.grad_angle_path[1:]),\n",
        "                    'convergence_rate': (result.f_path[-1] - result.f_path[0]) / len(result.f_path) if result.f_path else None\n",
        "                })\n",
        "\n",
        "            # Save method summary\n",
        "            summary_df = pd.DataFrame(summary_data)\n",
        "            method_dir = os.path.join(self.base_dir, 'method_summaries')\n",
        "            os.makedirs(method_dir, exist_ok=True)\n",
        "            summary_df.to_csv(\n",
        "                os.path.join(method_dir, f'{method}_summary_{self.timestamp}.csv'),\n",
        "                index=False\n",
        "            )"
      ],
      "metadata": {
        "id": "LiMJFlqjGPQ1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Executions"
      ],
      "metadata": {
        "id": "w0OSkcfuGTRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_optimization(f: Callable,\n",
        "                    grad: Callable,\n",
        "                    hess: Callable,\n",
        "                    x0: np.ndarray,\n",
        "                    method: str,\n",
        "                    function_name: str) -> OptimizationResult:\n",
        "    \"\"\"Enhanced optimization runner with detailed metrics\"\"\"\n",
        "    start_time = time.time()\n",
        "    step_logger = StepLogger()  # Create new step logger\n",
        "\n",
        "    # Calculate initial metrics\n",
        "    f_initial = f(x0)\n",
        "    grad_initial = grad(x0)\n",
        "\n",
        "    # Log initial point\n",
        "    step_logger.log_iteration(x0, f_initial, grad_initial)\n",
        "\n",
        "    def callback(xk):\n",
        "        x_min, _ = TestFunctions.get_global_minimum(function_name, len(x0))\n",
        "        f_val = f(xk)\n",
        "        grad_val = grad(xk)\n",
        "        step_logger.log_iteration(xk, f_val, grad_val, global_minimum=x_min)\n",
        "\n",
        "    try:\n",
        "        # Run optimization with method-specific settings\n",
        "        if method == 'BFGS':\n",
        "            result = minimize(f, x0, method=method, jac=grad, callback=callback)\n",
        "        elif method == 'newton-cg':\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        elif method in ['trust-exact', 'trust-krylov']:\n",
        "            result = minimize(f, x0, method=method, jac=grad, hess=hess, callback=callback)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n",
        "        # Calculate final gradient\n",
        "        grad_final = grad(result.x)\n",
        "        runtime = time.time() - start_time\n",
        "\n",
        "        return OptimizationResult(\n",
        "            x_final=result.x,\n",
        "            f_final=result.fun,\n",
        "            success=result.success,\n",
        "            iterations=result.nit,\n",
        "            runtime=runtime,\n",
        "            path=step_logger.path,\n",
        "            f_path=step_logger.f_path,\n",
        "            grad_norm_path=step_logger.grad_norm_path,\n",
        "            grad_cosine_sim_path=step_logger.grad_cosine_sim_path,  # New\n",
        "            grad_angle_path=step_logger.grad_angle_path,            # New\n",
        "            timestamps=step_logger.timestamps,\n",
        "            method=method,\n",
        "            dimension=len(x0),\n",
        "            function_name=function_name,\n",
        "            x_initial=x0,\n",
        "            f_initial=f_initial,\n",
        "            grad_initial=grad_initial,\n",
        "            grad_final=grad_final,\n",
        "            step_sizes=step_logger.step_sizes,\n",
        "            improvements=step_logger.improvements,\n",
        "            best_so_far=step_logger.best_so_far,\n",
        "            relative_improvements=step_logger.relative_improvements,\n",
        "            distance_to_minimum_path=step_logger.distance_to_minimum\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Optimization failed: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "k_iehVE2FnKu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperimentManager:\n",
        "    \"\"\"Manages multiple optimization experiments\"\"\"\n",
        "    def __init__(self,\n",
        "                 test_functions: Dict[str, Tuple[Callable, Callable, Callable]],\n",
        "                 first_order_optimizers: Dict[str, object],\n",
        "                 second_order_methods: List[str],\n",
        "                 dimensions: List[int],\n",
        "                 n_experiments: int = 50,\n",
        "                 min_dist: float = 100,\n",
        "                 max_dist: float = 1000):\n",
        "        self.test_functions = test_functions\n",
        "        self.first_order_optimizers = first_order_optimizers\n",
        "        self.second_order_methods = second_order_methods\n",
        "        self.dimensions = dimensions\n",
        "        self.n_experiments = n_experiments\n",
        "        self.min_dist = min_dist\n",
        "        self.max_dist = max_dist\n",
        "\n",
        "    def generate_starting_points(self, dimension: int, seed: int = None) -> np.ndarray:\n",
        "        \"\"\"Generate random starting points with specified distance from origin\"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        starting_points = []\n",
        "\n",
        "        for _ in range(self.n_experiments):\n",
        "            direction = np.random.randn(dimension)\n",
        "            direction = direction / np.linalg.norm(direction)\n",
        "            distance = np.random.uniform(self.min_dist, self.max_dist)\n",
        "            point = direction * distance\n",
        "            starting_points.append(point)\n",
        "\n",
        "        return np.array(starting_points)\n",
        "\n",
        "    def run_experiments(self, base_dir: str = \"optimization_results\"):\n",
        "        \"\"\"Run all experiments with proper directory structure\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        experiment_dir = os.path.join(base_dir, f\"experiment_{timestamp}\")\n",
        "        experiment_logger = OptimizationLogger(experiment_dir)\n",
        "\n",
        "        # For storing all results (needed for final summary)\n",
        "        final_results = []\n",
        "\n",
        "        for func_name, (f, grad, hess) in self.test_functions.items():\n",
        "            print(f\"\\nTesting {func_name} function:\")\n",
        "\n",
        "            # Store function-level results\n",
        "            function_results = []\n",
        "\n",
        "            for dim in self.dimensions:\n",
        "                print(f\"\\nDimension: {dim}\")\n",
        "                # Store dimension-level results\n",
        "                dimension_results = []\n",
        "\n",
        "                # Generate starting points for this dimension\n",
        "                starting_points = self.generate_starting_points(dim)\n",
        "\n",
        "                # Create directory structure\n",
        "                func_dir = os.path.join(experiment_dir, func_name, f\"{dim}D\")\n",
        "                first_order_dir = os.path.join(func_dir, \"first_order\")\n",
        "                second_order_dir = os.path.join(func_dir, \"second_order\")\n",
        "                results_dir = os.path.join(func_dir, \"results\")\n",
        "\n",
        "                # Create all necessary directories\n",
        "                os.makedirs(results_dir, exist_ok=True)\n",
        "                os.makedirs(os.path.join(results_dir, \"gradient_metrics\"), exist_ok=True)\n",
        "\n",
        "                for method_name in self.first_order_optimizers.keys():\n",
        "                    method_dir = os.path.join(first_order_dir, method_name)\n",
        "                    os.makedirs(os.path.join(method_dir, \"results\"), exist_ok=True)\n",
        "                    os.makedirs(os.path.join(method_dir, \"trajectories\"), exist_ok=True)\n",
        "\n",
        "                for method_name in self.second_order_methods:\n",
        "                    method_dir = os.path.join(second_order_dir, method_name)\n",
        "                    os.makedirs(os.path.join(method_dir, \"results\"), exist_ok=True)\n",
        "                    os.makedirs(os.path.join(method_dir, \"trajectories\"), exist_ok=True)\n",
        "\n",
        "                # Run experiments for each starting point\n",
        "                for i, x0 in enumerate(starting_points):\n",
        "                    print(f\"\\nExperiment {i+1}/{self.n_experiments}\")\n",
        "\n",
        "                    # Store results for this experiment\n",
        "                    first_order_results = {}\n",
        "                    second_order_results = {}\n",
        "\n",
        "                    # First order methods\n",
        "                    print(\"\\nFirst-order methods:\")\n",
        "                    print(\"-\" * 50)\n",
        "                    for name, optimizer in self.first_order_optimizers.items():\n",
        "                        print(f\"Running {name}...\")\n",
        "                        try:\n",
        "                            step_logger = StepLogger()\n",
        "                            start_time = time.time()\n",
        "\n",
        "                            # Initialize with first gradient\n",
        "                            g_initial = grad(x0)\n",
        "                            x_min, _ = TestFunctions.get_global_minimum(func_name, dim)\n",
        "                            step_logger.log_iteration(x0, f(x0), g_initial, global_minimum=x_min)\n",
        "\n",
        "                            def callback(x):\n",
        "                                g = grad(x)\n",
        "                                step_logger.log_iteration(x, f(x), g, global_minimum=x_min)\n",
        "\n",
        "                            result = optimizer.optimize(f, grad, x0, func_name, callback=callback)\n",
        "                            runtime = time.time() - start_time\n",
        "\n",
        "                            opt_result = OptimizationResult(\n",
        "                                x_final=result['x'],\n",
        "                                f_final=result['fun'],\n",
        "                                success=result['success'],\n",
        "                                iterations=result['nit'],\n",
        "                                runtime=runtime,\n",
        "                                path=step_logger.path,\n",
        "                                f_path=step_logger.f_path,\n",
        "                                grad_norm_path=step_logger.grad_norm_path,\n",
        "                                grad_cosine_sim_path=step_logger.grad_cosine_sim_path,\n",
        "                                grad_angle_path=step_logger.grad_angle_path,\n",
        "                                timestamps=step_logger.timestamps,\n",
        "                                method=name,\n",
        "                                dimension=dim,\n",
        "                                function_name=func_name,\n",
        "                                x_initial=x0,\n",
        "                                f_initial=f(x0),\n",
        "                                grad_initial=g_initial,\n",
        "                                grad_final=grad(result['x']),\n",
        "                                step_sizes=step_logger.step_sizes,\n",
        "                                improvements=step_logger.improvements,\n",
        "                                best_so_far=step_logger.best_so_far,\n",
        "                                relative_improvements=step_logger.relative_improvements,\n",
        "                                distance_to_minimum_path=step_logger.distance_to_minimum\n",
        "                            )\n",
        "\n",
        "                            first_order_results[name] = opt_result\n",
        "                            dimension_results.append(opt_result)\n",
        "                            function_results.append(opt_result)\n",
        "                            final_results.append(opt_result)\n",
        "                            experiment_logger.log_run(opt_result, i)\n",
        "\n",
        "                            print(f\"  Runtime: {runtime:.3f} seconds\")\n",
        "                            print(f\"  Iterations: {result['nit']}\")\n",
        "                            print(f\"  Final value: {result['fun']:.6f}\")\n",
        "                            print(f\"  Success: {result['success']}\")\n",
        "                            print(\"Done\")\n",
        "                            print(\"-\" * 20)\n",
        "                            print()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"Failed: {str(e)}\")\n",
        "\n",
        "                    # Generate plots for first-order methods\n",
        "                    if first_order_results:\n",
        "                        for name, result in first_order_results.items():\n",
        "                            method_dir = os.path.join(first_order_dir, name)\n",
        "                            self._generate_plots({name: result}, method_dir, f, i, dim)\n",
        "\n",
        "                    # Second order methods\n",
        "                    print(\"\\nSecond-order methods:\")\n",
        "                    print(\"-\" * 50)\n",
        "                    for method in self.second_order_methods:\n",
        "                        print(f\"Running {method}...\")\n",
        "                        result = run_optimization(f, grad, hess, x0, method, func_name)\n",
        "                        if result:\n",
        "                            second_order_results[method] = result\n",
        "                            dimension_results.append(result)\n",
        "                            function_results.append(result)\n",
        "                            final_results.append(result)\n",
        "                            experiment_logger.log_run(result, i)\n",
        "                            print(f\"  Runtime: {result.runtime:.3f} seconds\")\n",
        "                            print(f\"  Iterations: {result.iterations}\")\n",
        "                            print(f\"  Final value: {result.f_final:.6f}\")\n",
        "                            print(\"Done\")\n",
        "                        else:\n",
        "                            print(\"Failed\")\n",
        "                        print(\"-\" * 20)\n",
        "                        print()\n",
        "\n",
        "                    # Generate plots for second-order methods\n",
        "                    if second_order_results:\n",
        "                        for name, result in second_order_results.items():\n",
        "                            method_dir = os.path.join(second_order_dir, name)\n",
        "                            self._generate_plots({name: result}, method_dir, f, i, dim)\n",
        "\n",
        "                    # Generate combined plots for this experiment\n",
        "                    all_results_this_batch = {**first_order_results, **second_order_results}\n",
        "                    if all_results_this_batch:\n",
        "                        # Convergence plot with experiment number\n",
        "                        convergence_filename = f'convergence_{func_name}_exp_{i}.png'\n",
        "                        Visualizer.plot_convergence(\n",
        "                            all_results_this_batch,\n",
        "                            os.path.join(results_dir),\n",
        "                            convergence_filename\n",
        "                        )\n",
        "\n",
        "                        # Gradient metrics plot with experiment number\n",
        "                        gradient_metrics_filename = f'gradient_metrics_{func_name}_exp_{i}.png'\n",
        "                        Visualizer.plot_gradient_metrics(\n",
        "                            all_results_this_batch,\n",
        "                            os.path.join(results_dir, \"gradient_metrics\"),\n",
        "                            gradient_metrics_filename\n",
        "                        )\n",
        "\n",
        "                    # Clear batch results to free memory\n",
        "                    first_order_results.clear()\n",
        "                    second_order_results.clear()\n",
        "\n",
        "                # After all experiments for this dimension\n",
        "                # Create dimension summary plots\n",
        "                dimension_results_combined = {}\n",
        "                for result in dimension_results:\n",
        "                    method = result.method\n",
        "                    if method not in dimension_results_combined:\n",
        "                        dimension_results_combined[method] = []\n",
        "                    dimension_results_combined[method].append(result)\n",
        "\n",
        "                # Create summary plots by averaging results for each method\n",
        "                if dimension_results_combined:\n",
        "                    summary_results = {}\n",
        "                    for method, results in dimension_results_combined.items():\n",
        "                        # Average the results for each method\n",
        "                        avg_result = OptimizationResult(\n",
        "                            x_final=np.mean([r.x_final for r in results], axis=0),\n",
        "                            f_final=np.mean([r.f_final for r in results]),\n",
        "                            success=np.mean([r.success for r in results]),\n",
        "                            iterations=int(np.mean([r.iterations for r in results])),\n",
        "                            runtime=np.mean([r.runtime for r in results]),\n",
        "                            path=[np.mean([r.path[i] for r in results if i < len(r.path)], axis=0)\n",
        "                                  for i in range(max(len(r.path) for r in results))],\n",
        "                            f_path=[np.mean([r.f_path[i] for r in results if i < len(r.f_path)])\n",
        "                                  for i in range(max(len(r.f_path) for r in results))],\n",
        "                            grad_norm_path=[np.mean([r.grad_norm_path[i] for r in results if i < len(r.grad_norm_path)])\n",
        "                                          for i in range(max(len(r.grad_norm_path) for r in results))],\n",
        "                            grad_cosine_sim_path=[np.mean([r.grad_cosine_sim_path[i] for r in results if i < len(r.grad_cosine_sim_path)])\n",
        "                                                for i in range(max(len(r.grad_cosine_sim_path) for r in results))],\n",
        "                            grad_angle_path=[np.mean([r.grad_angle_path[i] for r in results if i < len(r.grad_angle_path)])\n",
        "                                          for i in range(max(len(r.grad_angle_path) for r in results))],\n",
        "                            step_sizes=[np.mean([r.step_sizes[i] for r in results if i < len(r.step_sizes)])\n",
        "                                      for i in range(max(len(r.step_sizes) for r in results))],\n",
        "                            improvements=[np.mean([r.improvements[i] for r in results if i < len(r.improvements)])\n",
        "                                        for i in range(max(len(r.improvements) for r in results))],\n",
        "                            best_so_far=[np.mean([r.best_so_far[i] for r in results if i < len(r.best_so_far)])\n",
        "                                        for i in range(max(len(r.best_so_far) for r in results))],\n",
        "                            relative_improvements=[np.mean([r.relative_improvements[i] for r in results if i < len(r.relative_improvements)])\n",
        "                                                for i in range(max(len(r.relative_improvements) for r in results))],\n",
        "                            distance_to_minimum_path=[np.mean([r.distance_to_minimum_path[i] for r in results if i < len(r.distance_to_minimum_path)])\n",
        "                                                  for i in range(max(len(r.distance_to_minimum_path) for r in results))],\n",
        "                            method=method,\n",
        "                            dimension=results[0].dimension,\n",
        "                            function_name=results[0].function_name\n",
        "                        )\n",
        "                        summary_results[method] = avg_result\n",
        "\n",
        "                    # Create summary plots using averaged results\n",
        "                    Visualizer.plot_convergence(\n",
        "                        summary_results,\n",
        "                        results_dir,\n",
        "                        f'convergence_{func_name}_summary.png'\n",
        "                    )\n",
        "\n",
        "                    Visualizer.plot_gradient_metrics(\n",
        "                        summary_results,\n",
        "                        os.path.join(results_dir, \"gradient_metrics\"),\n",
        "                        f'gradient_metrics_{func_name}_summary.png'\n",
        "                    )\n",
        "\n",
        "                    Visualizer.plot_step_metrics(\n",
        "                        summary_results,\n",
        "                        results_dir,\n",
        "                        f'step_metrics_{func_name}_summary.png'\n",
        "                    )\n",
        "\n",
        "                    # Clear summary results\n",
        "                    summary_results.clear()\n",
        "\n",
        "                # Create dimension summary\n",
        "                experiment_logger.create_dimension_summary(func_name, dim, dimension_results)\n",
        "\n",
        "                # Save dimension results\n",
        "                results_df = pd.DataFrame([self._format_result(r, \"dimension\", i)\n",
        "                                        for i, r in enumerate(dimension_results)])\n",
        "                results_df.to_csv(\n",
        "                    os.path.join(results_dir, f'dimension_results_{timestamp}.csv'),\n",
        "                    index=False\n",
        "                )\n",
        "\n",
        "                # Clear dimension results after saving\n",
        "                dimension_results.clear()\n",
        "                dimension_results_combined.clear()\n",
        "\n",
        "            # After all dimensions for this function, save function results\n",
        "            results_df = pd.DataFrame([self._format_result(r, \"function\", i)\n",
        "                                    for i, r in enumerate(function_results)])\n",
        "            results_df.to_csv(\n",
        "                os.path.join(experiment_dir, func_name, f'function_results_{timestamp}.csv'),\n",
        "                index=False\n",
        "            )\n",
        "\n",
        "            # Clear function results after saving\n",
        "            function_results.clear()\n",
        "\n",
        "        # Save final results and generate statistics\n",
        "        results_df = pd.DataFrame([self._format_result(r, \"final\", i)\n",
        "                                for i, r in enumerate(final_results)])\n",
        "        results_df.to_csv(os.path.join(experiment_dir, \"all_results.csv\"), index=False)\n",
        "        generate_statistics(results_df, experiment_dir)\n",
        "\n",
        "        # Clear final results\n",
        "        final_results.clear()\n",
        "\n",
        "    def _generate_plots(self, results: Dict[str, OptimizationResult],\n",
        "                  base_dir: str, f: Callable, exp_num: int, dim: int):\n",
        "        \"\"\"Generate trajectory and convergence plots\"\"\"\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        results_dir = os.path.join(base_dir, \"results\")\n",
        "        trajectory_dir = os.path.join(base_dir, \"trajectories\")\n",
        "\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        os.makedirs(trajectory_dir, exist_ok=True)\n",
        "\n",
        "        # Get function name from first result\n",
        "        func_name = next(iter(results.values())).function_name\n",
        "\n",
        "        # Generate convergence plots\n",
        "        Visualizer.plot_convergence(results, results_dir, func_name)\n",
        "\n",
        "        # Generate step metrics plots\n",
        "        Visualizer.plot_step_metrics(results, results_dir, func_name, exp_num)\n",
        "\n",
        "        # Generate 2D trajectories if applicable\n",
        "        if dim == 2:\n",
        "            for method, result in results.items():\n",
        "                if result is not None:\n",
        "                    try:\n",
        "                        Visualizer.plot_2d_trajectory(f, result, trajectory_dir, exp_num)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error plotting trajectory for {method}: {e}\")\n",
        "\n",
        "    def _format_result(self, result: OptimizationResult, opt_type: str, exp_num: int) -> dict:\n",
        "        \"\"\"Format OptimizationResult for DataFrame with error handling\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'function': result.function_name,\n",
        "                'dimension': result.dimension,\n",
        "                'experiment': exp_num,\n",
        "                'method': result.method,\n",
        "                'optimizer_type': opt_type,\n",
        "                'start_distance': np.linalg.norm(result.x_initial),\n",
        "                'final_value': result.f_final,\n",
        "                'iterations': result.iterations,\n",
        "                'runtime': result.runtime,\n",
        "                'success': False if any(np.isinf(x) for x in result.f_path) else result.success,  # Mark as failed if inf values\n",
        "                'distance_to_minimum': result.distance_to_minimum,\n",
        "                'f_error': result.f_error,\n",
        "                'initial_gradient_norm': np.linalg.norm(result.grad_initial),\n",
        "                'final_gradient_norm': np.linalg.norm(result.grad_final),\n",
        "                'computation_error': False\n",
        "            }\n",
        "        except (OverflowError, ValueError, RuntimeError) as e:\n",
        "            # Return a failed result entry\n",
        "            return {\n",
        "                'function': result.function_name,\n",
        "                'dimension': result.dimension,\n",
        "                'experiment': exp_num,\n",
        "                'method': result.method,\n",
        "                'optimizer_type': opt_type,\n",
        "                'success': False,\n",
        "                'computation_error': True,\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "\n",
        "def generate_statistics(results_df: pd.DataFrame, save_dir: str):\n",
        "    \"\"\"Generate comprehensive statistical visualizations for optimization results\"\"\"\n",
        "    # Set style and color palette\n",
        "    plt.style.use('default')\n",
        "    colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC', '#99CCFF']\n",
        "\n",
        "    if len(results_df) == 0:\n",
        "        return\n",
        "\n",
        "    # Generate summary statistics table\n",
        "    summary_stats = results_df.groupby('method').agg({\n",
        "        'final_value': ['mean', 'std', 'min', 'max'],\n",
        "        'runtime': ['mean', 'std'],\n",
        "        'iterations': ['mean', 'std'],\n",
        "        'success': 'mean',\n",
        "        'distance_to_minimum': ['mean', 'std'] if 'distance_to_minimum' in results_df.columns else None,\n",
        "        'mean_step_size': ['mean', 'std'],\n",
        "        'mean_improvement_per_step': ['mean', 'std'],\n",
        "        'mean_plateau_percentage': ['mean', 'std'],\n",
        "        'time_to_10_percent_improvement': ['mean', 'std'],\n",
        "        'time_to_20_percent_improvement': ['mean', 'std']\n",
        "    }).round(4)\n",
        "\n",
        "    # Save summary statistics\n",
        "    summary_stats.to_csv(os.path.join(save_dir, 'summary_statistics.csv'))\n",
        "\n",
        "    return summary_stats"
      ],
      "metadata": {
        "id": "nd91nH3NiT5D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Github and main() function"
      ],
      "metadata": {
        "id": "fzdasrggF19C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from github import Github\n",
        "import git\n",
        "import json\n",
        "from pathlib import Path\n",
        "import time\n",
        "import random\n",
        "\n",
        "class AuthManager:\n",
        "    def __init__(self):\n",
        "        self.config_file = Path.home() / '.optimization_config'\n",
        "        self.credentials = self.load_credentials()\n",
        "\n",
        "    def load_credentials(self):\n",
        "        if self.config_file.exists():\n",
        "            try:\n",
        "                with open(self.config_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def save_credentials(self, credentials):\n",
        "        with open(self.config_file, 'w') as f:\n",
        "            json.dump(credentials, f)\n",
        "\n",
        "    def authenticate(self):\n",
        "        if self.credentials:\n",
        "            print(\"Found existing credentials. Would you like to use them? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                return self.credentials\n",
        "\n",
        "        print(\"\\n=== GitHub Authentication ===\")\n",
        "        print(\"Please provide your GitHub credentials to continue.\")\n",
        "        print(\"Note: Your personal access token needs repo and workflow permissions.\")\n",
        "        print(\"\\nDon't have a token? Create one at: https://github.com/settings/tokens\")\n",
        "\n",
        "        github_username = input(\"\\nGitHub Username: \")\n",
        "        github_token = getpass.getpass(\"Personal Access Token: \")\n",
        "        repo_name = input(\"Repository Name (format: username/repo): \")\n",
        "\n",
        "        # Verify credentials\n",
        "        try:\n",
        "            g = Github(github_token)\n",
        "            user = g.get_user()\n",
        "            _ = user.get_repo(repo_name.split('/')[1])\n",
        "\n",
        "            credentials = {\n",
        "                'username': github_username,\n",
        "                'token': github_token,\n",
        "                'repo': repo_name\n",
        "            }\n",
        "\n",
        "            print(\"\\nAuthentication successful!\")\n",
        "\n",
        "            # Ask to save credentials\n",
        "            print(\"Would you like to save these credentials for future use? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                self.save_credentials(credentials)\n",
        "                print(\"Credentials saved!\")\n",
        "\n",
        "            return credentials\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAuthentication failed: {str(e)}\")\n",
        "            print(\"Please try again.\")\n",
        "            return self.authenticate()\n",
        "\n",
        "class GitManager:\n",
        "    def __init__(self, credentials):\n",
        "        self.credentials = credentials\n",
        "        # Format the URL with the token in a different way\n",
        "        self.token = credentials['token']\n",
        "        self.repo_url = f\"https://x-access-token:{self.token}@github.com/{credentials['repo']}.git\"\n",
        "        self.instance_id = f\"optimization_{random.randint(1000, 9999)}\"\n",
        "        self.repo = None\n",
        "\n",
        "    def setup_repo(self, local_path):\n",
        "        \"\"\"Initialize or clone the repository\"\"\"\n",
        "        try:\n",
        "            print(f\"Creating directory: {local_path}\")\n",
        "            Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Configure git globally\n",
        "            with git.Git().custom_environment(GIT_SSL_NO_VERIFY='true'):\n",
        "                try:\n",
        "                    print(\"Checking if directory is a git repository...\")\n",
        "                    self.repo = git.Repo(local_path)\n",
        "                    print(\"Existing repository found\")\n",
        "\n",
        "                    # Update remote URL with credentials\n",
        "                    origin = self.repo.remote('origin')\n",
        "                    origin.set_url(self.repo_url)\n",
        "                    print(\"Remote URL updated\")\n",
        "\n",
        "                except git.exc.InvalidGitRepositoryError:\n",
        "                    print(\"Initializing new git repository...\")\n",
        "                    self.repo = git.Repo.init(local_path)\n",
        "                    print(\"Repository initialized\")\n",
        "\n",
        "                    print(\"Adding remote origin...\")\n",
        "                    origin = self.repo.create_remote('origin', self.repo_url)\n",
        "                    print(\"Remote added\")\n",
        "\n",
        "                    # Configure git credentials\n",
        "                    config_writer = self.repo.config_writer()\n",
        "                    config_writer.set_value(\"http\", \"sslVerify\", \"false\")\n",
        "                    config_writer.release()\n",
        "\n",
        "                    print(\"Fetching from remote...\")\n",
        "                    origin.fetch()\n",
        "                    print(\"Fetch completed\")\n",
        "\n",
        "                    print(\"Setting up main branch...\")\n",
        "                    if 'main' not in self.repo.refs:\n",
        "                        self.repo.create_head('main', origin.refs.main)\n",
        "                    self.repo.heads.main.set_tracking_branch(origin.refs.main)\n",
        "                    self.repo.heads.main.checkout()\n",
        "                    print(\"Main branch setup completed\")\n",
        "\n",
        "                    print(\"Pulling latest changes...\")\n",
        "                    origin.pull('main')\n",
        "                    print(\"Pull completed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up repository: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def push_results(self, local_path):\n",
        "        \"\"\"Push results to GitHub with conflict resolution\"\"\"\n",
        "        max_retries = 5\n",
        "        retry_count = 0\n",
        "\n",
        "        while retry_count < max_retries:\n",
        "            try:\n",
        "                print(f\"\\nAttempting to push results (attempt {retry_count + 1}/{max_retries})...\")\n",
        "\n",
        "                # Configure git environment for this operation\n",
        "                env = {\n",
        "                    'GIT_SSL_NO_VERIFY': 'true',\n",
        "                    'GIT_TERMINAL_PROMPT': '0',\n",
        "                    'GIT_USERNAME': 'x-access-token',\n",
        "                    'GIT_PASSWORD': self.token\n",
        "                }\n",
        "\n",
        "                with self.repo.git.custom_environment(**env):\n",
        "                    print(\"Creating new branch...\")\n",
        "                    current = self.repo.create_head(self.instance_id)\n",
        "                    current.checkout()\n",
        "                    print(f\"Created and checked out branch: {self.instance_id}\")\n",
        "\n",
        "                    print(\"Pulling latest changes from main...\")\n",
        "                    self.repo.remotes.origin.pull('main')\n",
        "                    print(\"Pull completed\")\n",
        "\n",
        "                    print(\"Adding new files...\")\n",
        "                    self.repo.index.add('*')\n",
        "                    print(\"Files added\")\n",
        "\n",
        "                    print(\"Committing changes...\")\n",
        "                    self.repo.index.commit(f\"Results update from {self.instance_id}\")\n",
        "                    print(\"Changes committed\")\n",
        "\n",
        "                    print(\"Pushing to remote...\")\n",
        "                    push_info = self.repo.remotes.origin.push(self.instance_id)\n",
        "                    print(\"Push completed\")\n",
        "\n",
        "                    print(\"Creating pull request...\")\n",
        "                    self.create_pull_request()\n",
        "\n",
        "                    print(f\"\\nResults successfully pushed to branch: {self.instance_id}\")\n",
        "                    print(\"Pull request created for review.\")\n",
        "                    break\n",
        "\n",
        "            except git.exc.GitCommandError as e:\n",
        "                print(f\"Git error: {e}\")\n",
        "                retry_count += 1\n",
        "                wait_time = random.uniform(1, 5)\n",
        "                print(f\"Waiting {wait_time:.2f} seconds before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error: {e}\")\n",
        "                print(f\"Error type: {type(e)}\")\n",
        "                break\n",
        "\n",
        "    def create_pull_request(self):\n",
        "        \"\"\"Create a pull request using GitHub API\"\"\"\n",
        "        try:\n",
        "            g = Github(self.token)\n",
        "            repo = g.get_repo(self.credentials['repo'])\n",
        "\n",
        "            pr = repo.create_pull(\n",
        "                title=f\"Results update from {self.instance_id}\",\n",
        "                body=\"Automated results update from optimization experiment\",\n",
        "                head=self.instance_id,\n",
        "                base=\"main\"\n",
        "            )\n",
        "            print(f\"Created PR: {pr.html_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create PR: {e}\")\n",
        "\n",
        "def setup_experiment():\n",
        "    \"\"\"Initial setup and authentication\"\"\"\n",
        "    print(\"\\n=== Optimization Experiment Setup ===\")\n",
        "\n",
        "    # Authenticate\n",
        "    auth_manager = AuthManager()\n",
        "    credentials = auth_manager.authenticate()\n",
        "\n",
        "    if not credentials:\n",
        "        print(\"Authentication failed. Cannot continue.\")\n",
        "        return None\n",
        "\n",
        "    return credentials\n"
      ],
      "metadata": {
        "id": "cvb6HG67Fp6u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Get credentials\n",
        "    credentials = setup_experiment()\n",
        "    if not credentials:\n",
        "        return\n",
        "\n",
        "    # Initialize Git manager\n",
        "    git_manager = GitManager(credentials)\n",
        "\n",
        "    # Setup local repository\n",
        "    local_path = \"optimization_results\"\n",
        "    try:\n",
        "        git_manager.setup_repo(local_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to setup repository: {e}\")\n",
        "        return\n",
        "\n",
        "    # Add all test functions\n",
        "    test_functions = {\n",
        "        'ackley': (\n",
        "            TestFunctions.ackley,\n",
        "            TestFunctions.ackley_gradient,\n",
        "            TestFunctions.ackley_hessian\n",
        "        ),\n",
        "        'rastrigin': (\n",
        "            TestFunctions.rastrigin,\n",
        "            TestFunctions.rastrigin_gradient,\n",
        "            TestFunctions.rastrigin_hessian\n",
        "        ),\n",
        "        'schwefel': (\n",
        "            TestFunctions.schwefel,\n",
        "            TestFunctions.schwefel_gradient,\n",
        "            TestFunctions.schwefel_hessian\n",
        "        ),\n",
        "        'sphere': (\n",
        "            TestFunctions.sphere,\n",
        "            TestFunctions.sphere_gradient,\n",
        "            TestFunctions.sphere_hessian\n",
        "        ),\n",
        "        'sum_squares': (\n",
        "            TestFunctions.sum_squares,\n",
        "            TestFunctions.sum_squares_gradient,\n",
        "            TestFunctions.sum_squares_hessian\n",
        "        ),\n",
        "        'rosenbrock': (\n",
        "            TestFunctions.rosenbrock,\n",
        "            TestFunctions.rosenbrock_gradient,\n",
        "            TestFunctions.rosenbrock_hessian\n",
        "        ),\n",
        "        'michalewicz': (\n",
        "            TestFunctions.michalewicz,\n",
        "            TestFunctions.michalewicz_gradient,\n",
        "            TestFunctions.michalewicz_hessian\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # All first-order optimizers with configurations\n",
        "    first_order_optimizers = {\n",
        "        'gradient_descent': GradientDescent(learning_rate=0.01),\n",
        "        'sgd': SGD(learning_rate=0.01, noise_scale=0.01),\n",
        "        'sgd_momentum': SGDMomentum(learning_rate=0.01, momentum=0.9, noise_scale=0.01),\n",
        "        'momentum': MomentumGD(learning_rate=0.01, momentum=0.9),\n",
        "        'rmsprop': RMSprop(learning_rate=0.01, decay_rate=0.9),\n",
        "        'adam': Adam(learning_rate=0.001)\n",
        "    }\n",
        "\n",
        "    # Second-order methods\n",
        "    second_order_methods = [\"trust-exact\", \"trust-krylov\"]\n",
        "\n",
        "    # Just test 2D for now\n",
        "    dimensions = [2]\n",
        "\n",
        "    # Create experiment manager\n",
        "    experiment = ExperimentManager(\n",
        "        test_functions=test_functions,\n",
        "        first_order_optimizers=first_order_optimizers,\n",
        "        second_order_methods=second_order_methods,\n",
        "        dimensions=dimensions,\n",
        "        n_experiments=2,  # Single experiment for quick testing\n",
        "        min_dist=2,  # Reduced distance for 2D visualization\n",
        "        max_dist=5   # Reduced distance for 2D visualization\n",
        "    )\n",
        "\n",
        "    # Run experiments\n",
        "    experiment.run_experiments()\n",
        "\n",
        "    # Push results to GitHub\n",
        "    print(\"\\nPushing results to GitHub...\")\n",
        "    git_manager.push_results(local_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqoSGTWxadj1",
        "outputId": "7079e818-91b5-4969-dca6-c0ecfd6dd612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Optimization Experiment Setup ===\n",
            "Found existing credentials. Would you like to use them? (y/n)\n",
            "y\n",
            "Creating directory: optimization_results\n",
            "Checking if directory is a git repository...\n",
            "Existing repository found\n",
            "Remote URL updated\n",
            "\n",
            "Testing ackley function:\n",
            "\n",
            "Dimension: 2\n",
            "\n",
            "Experiment 1/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.006 seconds\n",
            "  Iterations: 24\n",
            "  Final value: 6.559645\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.199 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 6.559646\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.191 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 6.559679\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.069 seconds\n",
            "  Iterations: 296\n",
            "  Final value: 6.559645\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.207 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 6.560969\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.164 seconds\n",
            "  Iterations: 795\n",
            "  Final value: 6.559645\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running trust-exact...\n",
            "  Runtime: 0.010 seconds\n",
            "  Iterations: 13\n",
            "  Final value: 6.559645\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.005 seconds\n",
            "  Iterations: 5\n",
            "  Final value: 6.559645\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Experiment 2/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.008 seconds\n",
            "  Iterations: 39\n",
            "  Final value: 9.001093\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.193 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 9.001094\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.193 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 9.001104\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.059 seconds\n",
            "  Iterations: 313\n",
            "  Final value: 9.001093\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.220 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 9.002421\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.203 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 9.001093\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running trust-exact...\n",
            "  Runtime: 0.018 seconds\n",
            "  Iterations: 19\n",
            "  Final value: 9.001093\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.023 seconds\n",
            "  Iterations: 28\n",
            "  Final value: 9.001093\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-2de361a0d02e>:153: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 15))\n",
            "<ipython-input-5-2de361a0d02e>:7: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  plt.figure(figsize=(20, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing rastrigin function:\n",
            "\n",
            "Dimension: 2\n",
            "\n",
            "Experiment 1/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.145 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 29.837598\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.154 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 14.885015\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.159 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 196.265317\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.146 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 197.251181\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.154 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 25.878549\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.157 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 25.868682\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running trust-exact...\n",
            "  Runtime: 0.003 seconds\n",
            "  Iterations: 5\n",
            "  Final value: 25.868682\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.006 seconds\n",
            "  Iterations: 5\n",
            "  Final value: 25.868682\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Experiment 2/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.140 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 38.400324\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.149 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 24.426091\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.149 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 65.216271\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.149 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 197.226510\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.014 seconds\n",
            "  Iterations: 72\n",
            "  Final value: 4.974790\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.163 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 4.974790\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running trust-exact...\n",
            "  Runtime: 0.007 seconds\n",
            "  Iterations: 7\n",
            "  Final value: 4.974790\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.009 seconds\n",
            "  Iterations: 9\n",
            "  Final value: 4.974790\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Testing schwefel function:\n",
            "\n",
            "Dimension: 2\n",
            "\n",
            "Experiment 1/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.143 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 831.170256\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.152 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 831.291009\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.157 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 830.075197\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.056 seconds\n",
            "  Iterations: 352\n",
            "  Final value: 830.075197\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.143 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 830.075202\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.153 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 840.583223\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running trust-exact...\n",
            "  Runtime: 0.018 seconds\n",
            "  Iterations: 35\n",
            "  Final value: 830.075197\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.226 seconds\n",
            "  Iterations: 400\n",
            "  Final value: 834.020498\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Experiment 2/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.138 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 830.392226\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.145 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 830.393543\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.145 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 830.075198\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.049 seconds\n",
            "  Iterations: 292\n",
            "  Final value: 830.075197\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.146 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 830.075207\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.157 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 835.767568\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running trust-exact...\n",
            "  Runtime: 0.036 seconds\n",
            "  Iterations: 61\n",
            "  Final value: 830.075197\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.221 seconds\n",
            "  Iterations: 400\n",
            "  Final value: 834.020498\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Testing sphere function:\n",
            "\n",
            "Dimension: 2\n",
            "\n",
            "Experiment 1/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.099 seconds\n",
            "  Iterations: 788\n",
            "  Final value: 0.000000\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.129 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.000001\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.131 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.000006\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.034 seconds\n",
            "  Iterations: 247\n",
            "  Final value: 0.000000\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.046 seconds\n",
            "  Iterations: 339\n",
            "  Final value: 0.000000\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.139 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 7.190017\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "\n",
            "Second-order methods:\n",
            "--------------------------------------------------\n",
            "Running trust-exact...\n",
            "  Runtime: 0.002 seconds\n",
            "  Iterations: 3\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running trust-krylov...\n",
            "  Runtime: 0.005 seconds\n",
            "  Iterations: 3\n",
            "  Final value: 0.000000\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment 2/2\n",
            "\n",
            "First-order methods:\n",
            "--------------------------------------------------\n",
            "Running gradient_descent...\n",
            "  Runtime: 0.095 seconds\n",
            "  Iterations: 767\n",
            "  Final value: 0.000000\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd...\n",
            "  Runtime: 0.135 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.000000\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running sgd_momentum...\n",
            "  Runtime: 0.128 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.000003\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running momentum...\n",
            "  Runtime: 0.033 seconds\n",
            "  Iterations: 247\n",
            "  Final value: 0.000000\n",
            "  Success: True\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running rmsprop...\n",
            "  Runtime: 0.129 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 0.000050\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n",
            "Running adam...\n",
            "  Runtime: 0.145 seconds\n",
            "  Iterations: 1000\n",
            "  Final value: 2.488112\n",
            "  Success: False\n",
            "Done\n",
            "--------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cN47rPnUH5hp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}