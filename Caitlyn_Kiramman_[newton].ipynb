{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJKOlUhvf3qS",
        "outputId": "02798f4d-a531-44b7-c7b5-757599a3c3ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch wandb huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeohHq1WZuJn",
        "outputId": "229b6ef2-2095-4b43-df0d-beb8e44bafc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set HF_TOKEN from colab secrets.\n",
            "Set wandb from colab secrets.\n",
            "hf_token: True\n",
            "wandb_key: True\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "# 1) Imports & Basic Setup\n",
        "##################################################\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import math\n",
        "\n",
        "# Attempt to load secrets from colab\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # 1) HF_TOKEN\n",
        "    hf_tok = userdata.get(\"HF_TOKEN\")\n",
        "    if hf_tok:\n",
        "        os.environ[\"HF_TOKEN\"] = hf_tok\n",
        "        print(\"Set HF_TOKEN from colab secrets.\")\n",
        "    # 2) W&B\n",
        "    wandb_key = userdata.get(\"wandb\")\n",
        "    if wandb_key:\n",
        "        os.environ[\"wandb\"] = wandb_key\n",
        "        print(\"Set wandb from colab secrets.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "hf_token  = os.environ.get(\"HF_TOKEN\", None)\n",
        "wandb_key = os.environ.get(\"wandb\", None)\n",
        "print(\"hf_token:\", bool(hf_token))\n",
        "print(\"wandb_key:\", bool(wandb_key))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "##################################################\n",
        "# 2) Larger MLP (~9.6k params)\n",
        "##################################################\n",
        "class LargeMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    We'll downsample MNIST to 16x16 => input dim=256\n",
        "    Then 2 hidden layers of 32 each => total ~9.6k parameters.\n",
        "\n",
        "    Calculation:\n",
        "      fc1: (256 * 32) + 32 = 8192 + 32 = 8224\n",
        "      fc2: (32 * 32) + 32  = 1024 + 32 = 1056\n",
        "      fc3: (32 * 10) + 10  = 320 + 10 = 330\n",
        "      total => 8224 + 1056 + 330 = 9610\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(256, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B,1,16,16]\n",
        "        x = x.view(x.size(0), -1)  # => [B,256]\n",
        "        x = torch.relu(self.fc1(x)) # => [B,32]\n",
        "        x = torch.relu(self.fc2(x)) # => [B,32]\n",
        "        x = self.fc3(x)             # => [B,10]\n",
        "        return x\n",
        "\n",
        "##################################################\n",
        "# 2.1) Compute Global Gradient Norm\n",
        "##################################################\n",
        "def compute_global_grad_norm(model):\n",
        "    \"\"\"\n",
        "    Sums the squared L2-norm of each param.grad, then sqrt.\n",
        "    This gives the 'global' gradient norm across all parameters.\n",
        "    \"\"\"\n",
        "    total_sq = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            g = p.grad.data\n",
        "            total_sq += g.norm(2).item()**2\n",
        "    return math.sqrt(total_sq)\n",
        "\n",
        "##################################################\n",
        "# 3) Naive Newton Optimizer\n",
        "##################################################\n",
        "class NaiveNewtonOptimizer(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Dense Hessian => O(N^3). Must call step(closure).\n",
        "    Great for demonstration with small networks only.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1.0, tol=1e-6):\n",
        "        defaults = dict(lr=lr, tol=tol)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        if closure is None:\n",
        "            raise RuntimeError(\"NaiveNewtonOptimizer needs a closure returning the loss (tensor).\")\n",
        "\n",
        "        loss = closure()\n",
        "        loss.backward(create_graph=True)\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            tol = group['tol']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.view(-1)\n",
        "                if grad.norm() < tol:\n",
        "                    continue\n",
        "\n",
        "                n = grad.numel()\n",
        "                # Build Hessian\n",
        "                H = []\n",
        "                for i in range(n):\n",
        "                    g_i = grad[i]\n",
        "                    p.grad = None\n",
        "                    g_i.backward(retain_graph=True)\n",
        "                    H_i = p.grad.view(-1).clone()\n",
        "                    H.append(H_i)\n",
        "                    p.grad = None\n",
        "                H = torch.stack(H, dim=1)  # => [n, n]\n",
        "\n",
        "                # Solve H dx = grad\n",
        "                try:\n",
        "                    dx, _ = torch.solve(grad.unsqueeze(1), H)\n",
        "                    dx = dx.squeeze(1)\n",
        "                except RuntimeError:\n",
        "                    dx = grad  # fallback\n",
        "\n",
        "                # Safely do in-place update using .data\n",
        "                p.data.sub_((lr * dx).view(p.shape))\n",
        "\n",
        "        return loss\n",
        "\n",
        "##################################################\n",
        "# 3) Naive Gradient Descent Optimizer\n",
        "##################################################\n",
        "class NaiveGradientDescent(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    A pure \"vanilla\" Gradient Descent:\n",
        "    p <- p - lr * grad(p)\n",
        "\n",
        "    Must call step(closure) where the closure:\n",
        "      - zeroes grads\n",
        "      - does forward pass\n",
        "      - returns the loss (tensor)\n",
        "    Then we do normal .backward() for the first derivative only.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=0.01):\n",
        "        defaults = dict(lr=lr)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        if closure is None:\n",
        "            raise RuntimeError(\"NaiveGradientDescent needs a closure returning the loss (tensor).\")\n",
        "\n",
        "        # 1) Recompute the forward pass\n",
        "        loss = closure()\n",
        "        # 2) Normal backward => first derivatives\n",
        "        loss.backward()\n",
        "\n",
        "        # 3) Update\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                p.data.sub_(lr * p.grad)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "##################################################\n",
        "# 4) DataLoaders with downsampling to 16x16\n",
        "##################################################\n",
        "def get_mnist_loaders(batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((16,16)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    train_ds = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
        "    test_ds  = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "##################################################\n",
        "# 5) Train function\n",
        "##################################################\n",
        "def train_model(\n",
        "    optimizer_name: str = \"\",\n",
        "    learning_rate: float = 1e-2,\n",
        "    epochs: int = 2,\n",
        "    batch_size: int = 64,\n",
        "    wandb_project: str = \"\"\n",
        "):\n",
        "    \"\"\"\n",
        "    We'll:\n",
        "      - Create ~9.6k param MLP (16x16 input => 2 hidden layers of 32)\n",
        "      - Use \"gd\", \"newton\", or \"sgd\"\n",
        "      - Possibly log each step's loss to wandb\n",
        "      - Also log:\n",
        "        1) global grad norm\n",
        "        2) step size\n",
        "        3) relative improvement\n",
        "    \"\"\"\n",
        "    model = LargeMLP().to(device)\n",
        "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # W&B\n",
        "    wandb_key = os.environ.get(\"wandb\", None)\n",
        "    do_wandb = (wandb_project != \"\") and (wandb_key is not None)\n",
        "    if do_wandb:\n",
        "        import wandb\n",
        "        wandb.login(key=wandb_key)\n",
        "        wandb.init(project=wandb_project, config={\n",
        "            \"optimizer_name\": optimizer_name,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": batch_size\n",
        "        })\n",
        "\n",
        "    # pick optimizer\n",
        "    if optimizer_name.lower() == \"gd\":\n",
        "        opt = NaiveGradientDescent(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name.lower() == \"newton\":\n",
        "        opt = NaiveNewtonOptimizer(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name.lower() == \"sgd\":\n",
        "        opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer {optimizer_name}\")\n",
        "\n",
        "    global_step = 0\n",
        "    initial_loss_val = None  # track initial for relative improvement\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            # 1) Save old parameters for step size calculation\n",
        "            old_params = [p.detach().clone() for p in model.parameters()]\n",
        "\n",
        "            def closure():\n",
        "                opt.zero_grad()\n",
        "                outputs = model(data)\n",
        "                loss_t = criterion(outputs, targets)\n",
        "                return loss_t\n",
        "\n",
        "            # 2) Step\n",
        "            loss_tensor = opt.step(closure=closure)\n",
        "            loss_val = loss_tensor.item()\n",
        "            total_loss += loss_val\n",
        "\n",
        "            # 3) If first time, store initial loss\n",
        "            if initial_loss_val is None:\n",
        "                initial_loss_val = loss_val\n",
        "\n",
        "            # 4) Compute global grad norm\n",
        "            grad_norm = compute_global_grad_norm(model)\n",
        "\n",
        "            # 5) Compute step size\n",
        "            sum_sq = 0.0\n",
        "            for p, old_p in zip(model.parameters(), old_params):\n",
        "                delta = (p.detach() - old_p).view(-1)\n",
        "                sum_sq += delta.norm(2).item()**2\n",
        "            step_size = math.sqrt(sum_sq)\n",
        "\n",
        "            # 6) Compute relative improvement from the initial iteration\n",
        "            # (if initial_loss_val was 2.3 and now it's 1.2, improvement = ~47.8%)\n",
        "            rel_improvement = 0.0\n",
        "            if abs(initial_loss_val) > 1e-12:  # avoid /0\n",
        "                rel_improvement = (initial_loss_val - loss_val) / abs(initial_loss_val)\n",
        "\n",
        "            global_step += 1\n",
        "            print(\n",
        "                f\"Epoch[{epoch}/{epochs}] Step[{global_step}] \"\n",
        "                f\"Loss={loss_val:.4f} GradNorm={grad_norm:.4f} \"\n",
        "                f\"StepSize={step_size:.4f} RelImp={rel_improvement*100:.2f}%\"\n",
        "            )\n",
        "\n",
        "            if do_wandb:\n",
        "                import wandb\n",
        "                wandb.log({\n",
        "                    \"train_loss\": loss_val,\n",
        "                    \"grad_norm\": grad_norm,\n",
        "                    \"step_size\": step_size,\n",
        "                    \"relative_improvement\": rel_improvement\n",
        "                }, step=global_step)\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data, targets in test_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                out = model(data)\n",
        "                l = criterion(out, targets)\n",
        "                test_loss += l.item() * data.size(0)\n",
        "                _, pred = out.max(1)\n",
        "                correct += pred.eq(targets).sum().item()\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        accuracy = 100.*correct / len(test_loader.dataset)\n",
        "        epoch_loss = total_loss / len(train_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} => TrainLoss={epoch_loss:.4f} TestLoss={test_loss:.4f} Acc={accuracy:.2f}%\")\n",
        "\n",
        "        if do_wandb:\n",
        "            import wandb\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss_epoch\": epoch_loss,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_accuracy\": accuracy\n",
        "            }, step=global_step)\n",
        "\n",
        "    if do_wandb:\n",
        "        import wandb\n",
        "        wandb.finish()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QBCCYTXFlrkV",
        "outputId": "462383dc-7184-48a9-90f5-3861080dce07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF token: True\n",
            "W&B key: True\n",
            "Using newton with ~9.6k param model. LR=0.0001, epochs=2, batch_size=16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250115_142427-y2ve4ttb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__/runs/y2ve4ttb' target=\"_blank\">radiant-glitter-15</a></strong> to <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__/runs/y2ve4ttb' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__/runs/y2ve4ttb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[0/2] Step[1] Loss=2.2950 GradNorm=6.0317 StepSize=0.0099 RelImp=0.00%\n",
            "Epoch[0/2] Step[2] Loss=2.3280 GradNorm=6.8848 StepSize=0.0132 RelImp=-1.44%\n",
            "Epoch[0/2] Step[3] Loss=2.2921 GradNorm=4.1236 StepSize=0.0113 RelImp=0.13%\n",
            "Epoch[0/2] Step[4] Loss=2.3568 GradNorm=4.2776 StepSize=0.0100 RelImp=-2.69%\n",
            "Epoch[0/2] Step[5] Loss=2.3371 GradNorm=6.0988 StepSize=0.0137 RelImp=-1.83%\n",
            "Epoch[0/2] Step[6] Loss=2.3081 GradNorm=8.6031 StepSize=0.0116 RelImp=-0.57%\n",
            "Epoch[0/2] Step[7] Loss=2.2685 GradNorm=4.5901 StepSize=0.0086 RelImp=1.15%\n",
            "Epoch[0/2] Step[8] Loss=2.2720 GradNorm=4.8771 StepSize=0.0107 RelImp=1.00%\n",
            "Epoch[0/2] Step[9] Loss=2.3163 GradNorm=7.5446 StepSize=0.0111 RelImp=-0.93%\n",
            "Epoch[0/2] Step[10] Loss=2.3125 GradNorm=4.7460 StepSize=0.0141 RelImp=-0.76%\n",
            "Epoch[0/2] Step[11] Loss=2.2926 GradNorm=4.7826 StepSize=0.0125 RelImp=0.11%\n",
            "Epoch[0/2] Step[12] Loss=2.3427 GradNorm=5.0377 StepSize=0.0111 RelImp=-2.08%\n",
            "Epoch[0/2] Step[13] Loss=2.4106 GradNorm=5.0801 StepSize=0.0178 RelImp=-5.03%\n",
            "Epoch[0/2] Step[14] Loss=2.3171 GradNorm=4.0391 StepSize=0.0098 RelImp=-0.96%\n",
            "Epoch[0/2] Step[15] Loss=2.2894 GradNorm=5.1240 StepSize=0.0122 RelImp=0.24%\n",
            "Epoch[0/2] Step[16] Loss=2.3466 GradNorm=6.4350 StepSize=0.0119 RelImp=-2.25%\n",
            "Epoch[0/2] Step[17] Loss=2.2376 GradNorm=5.6022 StepSize=0.0098 RelImp=2.50%\n",
            "Epoch[0/2] Step[18] Loss=2.2981 GradNorm=7.2502 StepSize=0.0095 RelImp=-0.14%\n",
            "Epoch[0/2] Step[19] Loss=2.3126 GradNorm=7.5933 StepSize=0.0134 RelImp=-0.77%\n",
            "Epoch[0/2] Step[20] Loss=2.2934 GradNorm=4.7969 StepSize=0.0126 RelImp=0.07%\n",
            "Epoch[0/2] Step[21] Loss=2.3512 GradNorm=8.1636 StepSize=0.0114 RelImp=-2.45%\n",
            "Epoch[0/2] Step[22] Loss=2.3651 GradNorm=7.0903 StepSize=0.0157 RelImp=-3.05%\n",
            "Epoch[0/2] Step[23] Loss=2.3187 GradNorm=12.0635 StepSize=0.0135 RelImp=-1.03%\n",
            "Epoch[0/2] Step[24] Loss=2.2795 GradNorm=9.3934 StepSize=0.0121 RelImp=0.68%\n",
            "Epoch[0/2] Step[25] Loss=2.3061 GradNorm=8.6913 StepSize=0.0118 RelImp=-0.48%\n",
            "Epoch[0/2] Step[26] Loss=2.3884 GradNorm=6.9973 StepSize=0.0144 RelImp=-4.07%\n",
            "Epoch[0/2] Step[27] Loss=2.3949 GradNorm=6.0865 StepSize=0.0167 RelImp=-4.35%\n",
            "Epoch[0/2] Step[28] Loss=2.3054 GradNorm=6.4096 StepSize=0.0090 RelImp=-0.45%\n",
            "Epoch[0/2] Step[29] Loss=2.2944 GradNorm=12.5748 StepSize=0.0162 RelImp=0.03%\n",
            "Epoch[0/2] Step[30] Loss=2.3491 GradNorm=16.9704 StepSize=0.0181 RelImp=-2.36%\n",
            "Epoch[0/2] Step[31] Loss=2.3503 GradNorm=5.5573 StepSize=0.0112 RelImp=-2.41%\n",
            "Epoch[0/2] Step[32] Loss=2.3460 GradNorm=6.3402 StepSize=0.0129 RelImp=-2.22%\n",
            "Epoch[0/2] Step[33] Loss=2.3185 GradNorm=12.5444 StepSize=0.0132 RelImp=-1.02%\n",
            "Epoch[0/2] Step[34] Loss=2.3486 GradNorm=5.3750 StepSize=0.0118 RelImp=-2.33%\n",
            "Epoch[0/2] Step[35] Loss=2.3353 GradNorm=7.2116 StepSize=0.0113 RelImp=-1.76%\n",
            "Epoch[0/2] Step[36] Loss=2.2752 GradNorm=7.2495 StepSize=0.0088 RelImp=0.86%\n",
            "Epoch[0/2] Step[37] Loss=2.3243 GradNorm=8.5965 StepSize=0.0152 RelImp=-1.28%\n",
            "Epoch[0/2] Step[38] Loss=2.4229 GradNorm=24.8321 StepSize=0.0193 RelImp=-5.57%\n",
            "Epoch[0/2] Step[39] Loss=2.3497 GradNorm=9.4935 StepSize=0.0105 RelImp=-2.38%\n",
            "Epoch[0/2] Step[40] Loss=2.2292 GradNorm=5.9612 StepSize=0.0090 RelImp=2.87%\n",
            "Epoch[0/2] Step[41] Loss=2.3956 GradNorm=10.4763 StepSize=0.0158 RelImp=-4.38%\n",
            "Epoch[0/2] Step[42] Loss=2.3888 GradNorm=12.5706 StepSize=0.0166 RelImp=-4.09%\n",
            "Epoch[0/2] Step[43] Loss=2.3866 GradNorm=14.7368 StepSize=0.0183 RelImp=-3.99%\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "# 6) Main\n",
        "##################################################\n",
        "def main(\n",
        "    optimizer_name=\"newton\",\n",
        "    lr=1.0,\n",
        "    epochs=2,\n",
        "    batch_size=16,\n",
        "    wandb_project=\"AAH-IA__newton-rhapson__\",\n",
        "):\n",
        "    hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "    wandb_key = os.environ.get(\"wandb\", None)\n",
        "\n",
        "    print(\"HF token:\", bool(hf_token))\n",
        "    print(\"W&B key:\", bool(wandb_key))\n",
        "    print(f\"Using {optimizer_name} with ~9.6k param model. LR={lr}, epochs={epochs}, batch_size={batch_size}\")\n",
        "\n",
        "    model = train_model(\n",
        "        optimizer_name=optimizer_name,\n",
        "        learning_rate=lr,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        wandb_project=wandb_project\n",
        "    )\n",
        "    print(\"Done with main(). Returning model.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Example if we want to run immediately:\n",
        "if __name__ == \"__main__\":\n",
        "    model = main(\n",
        "        optimizer_name=\"newton\",\n",
        "        lr=0.0001, # 0.0001 will be used for newton\n",
        "        epochs=2,\n",
        "        batch_size=16,\n",
        "        wandb_project=\"AAH-IA__newton-rhapson__\"  # or AAH-IA__gradient-descent__\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "tLDj-Q5RDMh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "00b155bc-910e-4006-ec41-bd30f0982e6a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▅▅█▆▆▅▄▂▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>7.34834</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dutiful-morning-13</strong> at: <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__/runs/cbhluv2k' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__/runs/cbhluv2k</a><br> View project at: <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250115_140143-cbhluv2k/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qcN7knzkFUjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}