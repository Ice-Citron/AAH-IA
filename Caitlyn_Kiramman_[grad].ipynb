{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQlvTfdkZvEO",
        "outputId": "38f5b737-d0e9-4dbb-a88d-6de90b37f620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch wandb huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeohHq1WZuJn",
        "outputId": "5e7730c4-6076-4aab-fb83-44c90915b2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set HF_TOKEN from colab secrets.\n",
            "Set wandb from colab secrets.\n",
            "hf_token: True\n",
            "wandb_key: True\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "# 1) Imports & Basic Setup\n",
        "##################################################\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import math\n",
        "\n",
        "# Attempt to load secrets from colab\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # 1) HF_TOKEN\n",
        "    hf_tok = userdata.get(\"HF_TOKEN\")\n",
        "    if hf_tok:\n",
        "        os.environ[\"HF_TOKEN\"] = hf_tok\n",
        "        print(\"Set HF_TOKEN from colab secrets.\")\n",
        "    # 2) W&B\n",
        "    wandb_key = userdata.get(\"wandb\")\n",
        "    if wandb_key:\n",
        "        os.environ[\"wandb\"] = wandb_key\n",
        "        print(\"Set wandb from colab secrets.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "hf_token  = os.environ.get(\"HF_TOKEN\", None)\n",
        "wandb_key = os.environ.get(\"wandb\", None)\n",
        "print(\"hf_token:\", bool(hf_token))\n",
        "print(\"wandb_key:\", bool(wandb_key))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "##################################################\n",
        "# 2) Larger MLP (~9.6k params)\n",
        "##################################################\n",
        "class LargeMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    We'll downsample MNIST to 16x16 => input dim=256\n",
        "    Then 2 hidden layers of 32 each => total ~9.6k parameters.\n",
        "\n",
        "    Calculation:\n",
        "      fc1: (256 * 32) + 32 = 8192 + 32 = 8224\n",
        "      fc2: (32 * 32) + 32  = 1024 + 32 = 1056\n",
        "      fc3: (32 * 10) + 10  = 320 + 10 = 330\n",
        "      total => 8224 + 1056 + 330 = 9610\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(256, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B,1,16,16]\n",
        "        x = x.view(x.size(0), -1)  # => [B,256]\n",
        "        x = torch.relu(self.fc1(x)) # => [B,32]\n",
        "        x = torch.relu(self.fc2(x)) # => [B,32]\n",
        "        x = self.fc3(x)             # => [B,10]\n",
        "        return x\n",
        "\n",
        "##################################################\n",
        "# 2.1) Compute Global Gradient Norm\n",
        "##################################################\n",
        "def compute_global_grad_norm(model):\n",
        "    \"\"\"\n",
        "    Sums the squared L2-norm of each param.grad, then sqrt.\n",
        "    This gives the 'global' gradient norm across all parameters.\n",
        "    \"\"\"\n",
        "    total_sq = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            g = p.grad.data\n",
        "            total_sq += g.norm(2).item()**2\n",
        "    return math.sqrt(total_sq)\n",
        "\n",
        "##################################################\n",
        "# 3) Naive Newton Optimizer\n",
        "##################################################\n",
        "class NaiveNewtonOptimizer(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Dense Hessian => O(N^3). Must call step(closure).\n",
        "    Great for demonstration with small networks only.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1.0, tol=1e-6):\n",
        "        defaults = dict(lr=lr, tol=tol)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        if closure is None:\n",
        "            raise RuntimeError(\"NaiveNewtonOptimizer needs a closure returning the loss (tensor).\")\n",
        "\n",
        "        loss = closure()\n",
        "        loss.backward(create_graph=True)\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            tol = group['tol']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.view(-1)\n",
        "                if grad.norm() < tol:\n",
        "                    continue\n",
        "\n",
        "                n = grad.numel()\n",
        "                # Build Hessian\n",
        "                H = []\n",
        "                for i in range(n):\n",
        "                    g_i = grad[i]\n",
        "                    p.grad = None\n",
        "                    g_i.backward(retain_graph=True)\n",
        "                    H_i = p.grad.view(-1).clone()\n",
        "                    H.append(H_i)\n",
        "                    p.grad = None\n",
        "                H = torch.stack(H, dim=1)  # => [n, n]\n",
        "\n",
        "                # Solve H dx = grad\n",
        "                try:\n",
        "                    dx, _ = torch.solve(grad.unsqueeze(1), H)\n",
        "                    dx = dx.squeeze(1)\n",
        "                except RuntimeError:\n",
        "                    dx = grad  # fallback\n",
        "\n",
        "                # Safely do in-place update using .data\n",
        "                p.data.sub_((lr * dx).view(p.shape))\n",
        "\n",
        "        return loss\n",
        "\n",
        "##################################################\n",
        "# 3) Naive Gradient Descent Optimizer\n",
        "##################################################\n",
        "class NaiveGradientDescent(torch.optim.Optimizer):\n",
        "    \"\"\"\n",
        "    A pure \"vanilla\" Gradient Descent:\n",
        "    p <- p - lr * grad(p)\n",
        "\n",
        "    Must call step(closure) where the closure:\n",
        "      - zeroes grads\n",
        "      - does forward pass\n",
        "      - returns the loss (tensor)\n",
        "    Then we do normal .backward() for the first derivative only.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=0.01):\n",
        "        defaults = dict(lr=lr)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        if closure is None:\n",
        "            raise RuntimeError(\"NaiveGradientDescent needs a closure returning the loss (tensor).\")\n",
        "\n",
        "        # 1) Recompute the forward pass\n",
        "        loss = closure()\n",
        "        # 2) Normal backward => first derivatives\n",
        "        loss.backward()\n",
        "\n",
        "        # 3) Update\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                p.data.sub_(lr * p.grad)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "##################################################\n",
        "# 4) DataLoaders with downsampling to 16x16\n",
        "##################################################\n",
        "def get_mnist_loaders(batch_size=64):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((16,16)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    train_ds = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
        "    test_ds  = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "##################################################\n",
        "# 5) Train function\n",
        "##################################################\n",
        "def train_model(\n",
        "    optimizer_name: str = \"\",\n",
        "    learning_rate: float = 1e-2,\n",
        "    epochs: int = 2,\n",
        "    batch_size: int = 64,\n",
        "    wandb_project: str = \"\"\n",
        "):\n",
        "    \"\"\"\n",
        "    We'll:\n",
        "      - Create ~9.6k param MLP (16x16 input => 2 hidden layers of 32)\n",
        "      - Use \"gd\", \"newton\", or \"sgd\"\n",
        "      - Possibly log each step's loss to wandb\n",
        "      - Also log:\n",
        "        1) global grad norm\n",
        "        2) step size\n",
        "        3) relative improvement\n",
        "    \"\"\"\n",
        "    model = LargeMLP().to(device)\n",
        "    train_loader, test_loader = get_mnist_loaders(batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # W&B\n",
        "    wandb_key = os.environ.get(\"wandb\", None)\n",
        "    do_wandb = (wandb_project != \"\") and (wandb_key is not None)\n",
        "    if do_wandb:\n",
        "        import wandb\n",
        "        wandb.login(key=wandb_key)\n",
        "        wandb.init(project=wandb_project, config={\n",
        "            \"optimizer_name\": optimizer_name,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": batch_size\n",
        "        })\n",
        "\n",
        "    # pick optimizer\n",
        "    if optimizer_name.lower() == \"gd\":\n",
        "        opt = NaiveGradientDescent(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name.lower() == \"newton\":\n",
        "        opt = NaiveNewtonOptimizer(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name.lower() == \"sgd\":\n",
        "        opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown optimizer {optimizer_name}\")\n",
        "\n",
        "    global_step = 0\n",
        "    initial_loss_val = None  # track initial for relative improvement\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            # 1) Save old parameters for step size calculation\n",
        "            old_params = [p.detach().clone() for p in model.parameters()]\n",
        "\n",
        "            def closure():\n",
        "                opt.zero_grad()\n",
        "                outputs = model(data)\n",
        "                loss_t = criterion(outputs, targets)\n",
        "                return loss_t\n",
        "\n",
        "            # 2) Step\n",
        "            loss_tensor = opt.step(closure=closure)\n",
        "            loss_val = loss_tensor.item()\n",
        "            total_loss += loss_val\n",
        "\n",
        "            # 3) If first time, store initial loss\n",
        "            if initial_loss_val is None:\n",
        "                initial_loss_val = loss_val\n",
        "\n",
        "            # 4) Compute global grad norm\n",
        "            grad_norm = compute_global_grad_norm(model)\n",
        "\n",
        "            # 5) Compute step size\n",
        "            sum_sq = 0.0\n",
        "            for p, old_p in zip(model.parameters(), old_params):\n",
        "                delta = (p.detach() - old_p).view(-1)\n",
        "                sum_sq += delta.norm(2).item()**2\n",
        "            step_size = math.sqrt(sum_sq)\n",
        "\n",
        "            # 6) Compute relative improvement from the initial iteration\n",
        "            # (if initial_loss_val was 2.3 and now it's 1.2, improvement = ~47.8%)\n",
        "            rel_improvement = 0.0\n",
        "            if abs(initial_loss_val) > 1e-12:  # avoid /0\n",
        "                rel_improvement = (initial_loss_val - loss_val) / abs(initial_loss_val)\n",
        "\n",
        "            global_step += 1\n",
        "            print(\n",
        "                f\"Epoch[{epoch}/{epochs}] Step[{global_step}] \"\n",
        "                f\"Loss={loss_val:.4f} GradNorm={grad_norm:.4f} \"\n",
        "                f\"StepSize={step_size:.4f} RelImp={rel_improvement*100:.2f}%\"\n",
        "            )\n",
        "\n",
        "            if do_wandb:\n",
        "                import wandb\n",
        "                wandb.log({\n",
        "                    \"train_loss\": loss_val,\n",
        "                    \"grad_norm\": grad_norm,\n",
        "                    \"step_size\": step_size,\n",
        "                    \"relative_improvement\": rel_improvement\n",
        "                }, step=global_step)\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data, targets in test_loader:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                out = model(data)\n",
        "                l = criterion(out, targets)\n",
        "                test_loss += l.item() * data.size(0)\n",
        "                _, pred = out.max(1)\n",
        "                correct += pred.eq(targets).sum().item()\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        accuracy = 100.*correct / len(test_loader.dataset)\n",
        "        epoch_loss = total_loss / len(train_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} => TrainLoss={epoch_loss:.4f} TestLoss={test_loss:.4f} Acc={accuracy:.2f}%\")\n",
        "\n",
        "        if do_wandb:\n",
        "            import wandb\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch,\n",
        "                \"train_loss_epoch\": epoch_loss,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_accuracy\": accuracy\n",
        "            }, step=global_step)\n",
        "\n",
        "    if do_wandb:\n",
        "        import wandb\n",
        "        wandb.finish()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QBCCYTXFlrkV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ace97b8-a4e0-4e1f-964b-ff9b495a7e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch[0/2] Step[2503] Loss=0.2428 GradNorm=2.0456 StepSize=0.0205 RelImp=89.43%\n",
            "Epoch[0/2] Step[2504] Loss=0.4461 GradNorm=4.7528 StepSize=0.0475 RelImp=80.57%\n",
            "Epoch[0/2] Step[2505] Loss=1.1773 GradNorm=7.4189 StepSize=0.0742 RelImp=48.74%\n",
            "Epoch[0/2] Step[2506] Loss=0.2105 GradNorm=2.7422 StepSize=0.0274 RelImp=90.83%\n",
            "Epoch[0/2] Step[2507] Loss=0.3602 GradNorm=2.0286 StepSize=0.0203 RelImp=84.32%\n",
            "Epoch[0/2] Step[2508] Loss=0.3029 GradNorm=3.5179 StepSize=0.0352 RelImp=86.81%\n",
            "Epoch[0/2] Step[2509] Loss=0.1977 GradNorm=2.4443 StepSize=0.0244 RelImp=91.39%\n",
            "Epoch[0/2] Step[2510] Loss=0.3946 GradNorm=3.8210 StepSize=0.0382 RelImp=82.82%\n",
            "Epoch[0/2] Step[2511] Loss=0.3696 GradNorm=3.1179 StepSize=0.0312 RelImp=83.91%\n",
            "Epoch[0/2] Step[2512] Loss=0.1339 GradNorm=1.8289 StepSize=0.0183 RelImp=94.17%\n",
            "Epoch[0/2] Step[2513] Loss=0.2462 GradNorm=2.4657 StepSize=0.0247 RelImp=89.28%\n",
            "Epoch[0/2] Step[2514] Loss=0.1511 GradNorm=1.8542 StepSize=0.0185 RelImp=93.42%\n",
            "Epoch[0/2] Step[2515] Loss=0.1142 GradNorm=1.3328 StepSize=0.0133 RelImp=95.03%\n",
            "Epoch[0/2] Step[2516] Loss=0.3235 GradNorm=2.3902 StepSize=0.0239 RelImp=85.92%\n",
            "Epoch[0/2] Step[2517] Loss=0.4815 GradNorm=4.1392 StepSize=0.0414 RelImp=79.03%\n",
            "Epoch[0/2] Step[2518] Loss=0.2923 GradNorm=2.4807 StepSize=0.0248 RelImp=87.27%\n",
            "Epoch[0/2] Step[2519] Loss=0.5658 GradNorm=4.1017 StepSize=0.0410 RelImp=75.36%\n",
            "Epoch[0/2] Step[2520] Loss=0.1877 GradNorm=2.0264 StepSize=0.0203 RelImp=91.83%\n",
            "Epoch[0/2] Step[2521] Loss=0.5369 GradNorm=3.4191 StepSize=0.0342 RelImp=76.62%\n",
            "Epoch[0/2] Step[2522] Loss=0.2424 GradNorm=2.4090 StepSize=0.0241 RelImp=89.45%\n",
            "Epoch[0/2] Step[2523] Loss=0.3187 GradNorm=3.6196 StepSize=0.0362 RelImp=86.12%\n",
            "Epoch[0/2] Step[2524] Loss=0.6131 GradNorm=3.3785 StepSize=0.0338 RelImp=73.31%\n",
            "Epoch[0/2] Step[2525] Loss=0.6259 GradNorm=3.5508 StepSize=0.0355 RelImp=72.75%\n",
            "Epoch[0/2] Step[2526] Loss=0.4863 GradNorm=2.7309 StepSize=0.0273 RelImp=78.83%\n",
            "Epoch[0/2] Step[2527] Loss=0.3070 GradNorm=2.6196 StepSize=0.0262 RelImp=86.63%\n",
            "Epoch[0/2] Step[2528] Loss=0.5290 GradNorm=2.5160 StepSize=0.0252 RelImp=76.97%\n",
            "Epoch[0/2] Step[2529] Loss=0.2237 GradNorm=1.9381 StepSize=0.0194 RelImp=90.26%\n",
            "Epoch[0/2] Step[2530] Loss=0.6796 GradNorm=3.6592 StepSize=0.0366 RelImp=70.41%\n",
            "Epoch[0/2] Step[2531] Loss=0.3801 GradNorm=3.6507 StepSize=0.0365 RelImp=83.45%\n",
            "Epoch[0/2] Step[2532] Loss=0.0772 GradNorm=0.8884 StepSize=0.0089 RelImp=96.64%\n",
            "Epoch[0/2] Step[2533] Loss=0.6254 GradNorm=3.3042 StepSize=0.0330 RelImp=72.77%\n",
            "Epoch[0/2] Step[2534] Loss=0.3732 GradNorm=2.6496 StepSize=0.0265 RelImp=83.75%\n",
            "Epoch[0/2] Step[2535] Loss=0.1122 GradNorm=1.6360 StepSize=0.0164 RelImp=95.12%\n",
            "Epoch[0/2] Step[2536] Loss=0.3156 GradNorm=2.7798 StepSize=0.0278 RelImp=86.26%\n",
            "Epoch[0/2] Step[2537] Loss=0.4378 GradNorm=3.3311 StepSize=0.0333 RelImp=80.94%\n",
            "Epoch[0/2] Step[2538] Loss=0.5393 GradNorm=4.4616 StepSize=0.0446 RelImp=76.52%\n",
            "Epoch[0/2] Step[2539] Loss=0.1946 GradNorm=2.3659 StepSize=0.0237 RelImp=91.53%\n",
            "Epoch[0/2] Step[2540] Loss=0.3043 GradNorm=3.3132 StepSize=0.0331 RelImp=86.75%\n",
            "Epoch[0/2] Step[2541] Loss=0.2981 GradNorm=2.3434 StepSize=0.0234 RelImp=87.02%\n",
            "Epoch[0/2] Step[2542] Loss=0.1411 GradNorm=1.5877 StepSize=0.0159 RelImp=93.86%\n",
            "Epoch[0/2] Step[2543] Loss=0.5623 GradNorm=2.7015 StepSize=0.0270 RelImp=75.52%\n",
            "Epoch[0/2] Step[2544] Loss=0.5843 GradNorm=5.2716 StepSize=0.0527 RelImp=74.56%\n",
            "Epoch[0/2] Step[2545] Loss=0.5662 GradNorm=3.4019 StepSize=0.0340 RelImp=75.34%\n",
            "Epoch[0/2] Step[2546] Loss=0.2609 GradNorm=2.6300 StepSize=0.0263 RelImp=88.64%\n",
            "Epoch[0/2] Step[2547] Loss=0.3158 GradNorm=2.7166 StepSize=0.0272 RelImp=86.25%\n",
            "Epoch[0/2] Step[2548] Loss=0.2906 GradNorm=2.6410 StepSize=0.0264 RelImp=87.34%\n",
            "Epoch[0/2] Step[2549] Loss=0.3965 GradNorm=2.7923 StepSize=0.0279 RelImp=82.73%\n",
            "Epoch[0/2] Step[2550] Loss=0.5082 GradNorm=4.4480 StepSize=0.0445 RelImp=77.87%\n",
            "Epoch[0/2] Step[2551] Loss=0.6707 GradNorm=3.1582 StepSize=0.0316 RelImp=70.80%\n",
            "Epoch[0/2] Step[2552] Loss=0.2728 GradNorm=2.3908 StepSize=0.0239 RelImp=88.12%\n",
            "Epoch[0/2] Step[2553] Loss=0.5313 GradNorm=5.0602 StepSize=0.0506 RelImp=76.87%\n",
            "Epoch[0/2] Step[2554] Loss=0.1685 GradNorm=1.4320 StepSize=0.0143 RelImp=92.66%\n",
            "Epoch[0/2] Step[2555] Loss=0.0814 GradNorm=1.3344 StepSize=0.0133 RelImp=96.46%\n",
            "Epoch[0/2] Step[2556] Loss=0.5960 GradNorm=2.6878 StepSize=0.0269 RelImp=74.05%\n",
            "Epoch[0/2] Step[2557] Loss=0.5708 GradNorm=4.1608 StepSize=0.0416 RelImp=75.14%\n",
            "Epoch[0/2] Step[2558] Loss=0.4324 GradNorm=3.2056 StepSize=0.0321 RelImp=81.17%\n",
            "Epoch[0/2] Step[2559] Loss=0.5752 GradNorm=4.0472 StepSize=0.0405 RelImp=74.95%\n",
            "Epoch[0/2] Step[2560] Loss=0.5375 GradNorm=5.3456 StepSize=0.0535 RelImp=76.60%\n",
            "Epoch[0/2] Step[2561] Loss=0.1932 GradNorm=1.8429 StepSize=0.0184 RelImp=91.59%\n",
            "Epoch[0/2] Step[2562] Loss=0.4490 GradNorm=5.1604 StepSize=0.0516 RelImp=80.45%\n",
            "Epoch[0/2] Step[2563] Loss=0.1644 GradNorm=1.8511 StepSize=0.0185 RelImp=92.84%\n",
            "Epoch[0/2] Step[2564] Loss=0.4455 GradNorm=3.8336 StepSize=0.0383 RelImp=80.60%\n",
            "Epoch[0/2] Step[2565] Loss=0.3354 GradNorm=2.3352 StepSize=0.0234 RelImp=85.40%\n",
            "Epoch[0/2] Step[2566] Loss=0.3763 GradNorm=2.1982 StepSize=0.0220 RelImp=83.62%\n",
            "Epoch[0/2] Step[2567] Loss=0.4985 GradNorm=4.1742 StepSize=0.0417 RelImp=78.29%\n",
            "Epoch[0/2] Step[2568] Loss=0.2043 GradNorm=2.5574 StepSize=0.0256 RelImp=91.11%\n",
            "Epoch[0/2] Step[2569] Loss=0.3335 GradNorm=4.6775 StepSize=0.0468 RelImp=85.48%\n",
            "Epoch[0/2] Step[2570] Loss=0.2590 GradNorm=3.1585 StepSize=0.0316 RelImp=88.72%\n",
            "Epoch[0/2] Step[2571] Loss=0.2696 GradNorm=3.8609 StepSize=0.0386 RelImp=88.26%\n",
            "Epoch[0/2] Step[2572] Loss=0.4102 GradNorm=2.2762 StepSize=0.0228 RelImp=82.14%\n",
            "Epoch[0/2] Step[2573] Loss=0.2711 GradNorm=2.3771 StepSize=0.0238 RelImp=88.20%\n",
            "Epoch[0/2] Step[2574] Loss=0.9849 GradNorm=6.6080 StepSize=0.0661 RelImp=57.11%\n",
            "Epoch[0/2] Step[2575] Loss=0.5093 GradNorm=4.8026 StepSize=0.0480 RelImp=77.82%\n",
            "Epoch[0/2] Step[2576] Loss=0.2849 GradNorm=2.2916 StepSize=0.0229 RelImp=87.59%\n",
            "Epoch[0/2] Step[2577] Loss=0.1543 GradNorm=1.9796 StepSize=0.0198 RelImp=93.28%\n",
            "Epoch[0/2] Step[2578] Loss=0.3785 GradNorm=3.0436 StepSize=0.0304 RelImp=83.52%\n",
            "Epoch[0/2] Step[2579] Loss=0.3960 GradNorm=3.2058 StepSize=0.0321 RelImp=82.76%\n",
            "Epoch[0/2] Step[2580] Loss=0.2118 GradNorm=2.4387 StepSize=0.0244 RelImp=90.78%\n",
            "Epoch[0/2] Step[2581] Loss=0.5052 GradNorm=4.6616 StepSize=0.0466 RelImp=78.00%\n",
            "Epoch[0/2] Step[2582] Loss=0.4395 GradNorm=4.1105 StepSize=0.0411 RelImp=80.86%\n",
            "Epoch[0/2] Step[2583] Loss=0.4797 GradNorm=3.8083 StepSize=0.0381 RelImp=79.11%\n",
            "Epoch[0/2] Step[2584] Loss=0.2706 GradNorm=2.9749 StepSize=0.0297 RelImp=88.22%\n",
            "Epoch[0/2] Step[2585] Loss=0.2549 GradNorm=2.3554 StepSize=0.0236 RelImp=88.90%\n",
            "Epoch[0/2] Step[2586] Loss=0.4154 GradNorm=3.7674 StepSize=0.0377 RelImp=81.91%\n",
            "Epoch[0/2] Step[2587] Loss=0.1840 GradNorm=1.8836 StepSize=0.0188 RelImp=91.99%\n",
            "Epoch[0/2] Step[2588] Loss=0.4691 GradNorm=2.5313 StepSize=0.0253 RelImp=79.58%\n",
            "Epoch[0/2] Step[2589] Loss=0.2843 GradNorm=3.3165 StepSize=0.0332 RelImp=87.62%\n",
            "Epoch[0/2] Step[2590] Loss=0.4346 GradNorm=3.4520 StepSize=0.0345 RelImp=81.08%\n",
            "Epoch[0/2] Step[2591] Loss=0.2004 GradNorm=1.9900 StepSize=0.0199 RelImp=91.27%\n",
            "Epoch[0/2] Step[2592] Loss=0.3752 GradNorm=2.8726 StepSize=0.0287 RelImp=83.66%\n",
            "Epoch[0/2] Step[2593] Loss=0.4375 GradNorm=3.5033 StepSize=0.0350 RelImp=80.95%\n",
            "Epoch[0/2] Step[2594] Loss=0.1469 GradNorm=1.7586 StepSize=0.0176 RelImp=93.61%\n",
            "Epoch[0/2] Step[2595] Loss=0.3030 GradNorm=3.0213 StepSize=0.0302 RelImp=86.81%\n",
            "Epoch[0/2] Step[2596] Loss=0.2108 GradNorm=2.8252 StepSize=0.0283 RelImp=90.82%\n",
            "Epoch[0/2] Step[2597] Loss=0.2796 GradNorm=2.6461 StepSize=0.0265 RelImp=87.83%\n",
            "Epoch[0/2] Step[2598] Loss=0.4320 GradNorm=2.7883 StepSize=0.0279 RelImp=81.19%\n",
            "Epoch[0/2] Step[2599] Loss=0.3123 GradNorm=4.0622 StepSize=0.0406 RelImp=86.40%\n",
            "Epoch[0/2] Step[2600] Loss=0.2582 GradNorm=2.8698 StepSize=0.0287 RelImp=88.76%\n",
            "Epoch[0/2] Step[2601] Loss=0.9981 GradNorm=3.2500 StepSize=0.0325 RelImp=56.54%\n",
            "Epoch[0/2] Step[2602] Loss=0.4501 GradNorm=4.5375 StepSize=0.0454 RelImp=80.40%\n",
            "Epoch[0/2] Step[2603] Loss=0.3453 GradNorm=3.0041 StepSize=0.0300 RelImp=84.96%\n",
            "Epoch[0/2] Step[2604] Loss=0.4434 GradNorm=2.9010 StepSize=0.0290 RelImp=80.69%\n",
            "Epoch[0/2] Step[2605] Loss=0.4042 GradNorm=2.3670 StepSize=0.0237 RelImp=82.40%\n",
            "Epoch[0/2] Step[2606] Loss=0.5702 GradNorm=5.0079 StepSize=0.0501 RelImp=75.17%\n",
            "Epoch[0/2] Step[2607] Loss=1.1699 GradNorm=3.4879 StepSize=0.0349 RelImp=49.06%\n",
            "Epoch[0/2] Step[2608] Loss=0.5450 GradNorm=4.4747 StepSize=0.0447 RelImp=76.27%\n",
            "Epoch[0/2] Step[2609] Loss=0.4857 GradNorm=2.3463 StepSize=0.0235 RelImp=78.85%\n",
            "Epoch[0/2] Step[2610] Loss=0.6302 GradNorm=3.4446 StepSize=0.0344 RelImp=72.56%\n",
            "Epoch[0/2] Step[2611] Loss=0.3895 GradNorm=3.0543 StepSize=0.0305 RelImp=83.04%\n",
            "Epoch[0/2] Step[2612] Loss=0.5397 GradNorm=4.0077 StepSize=0.0401 RelImp=76.50%\n",
            "Epoch[0/2] Step[2613] Loss=0.2500 GradNorm=2.4822 StepSize=0.0248 RelImp=89.11%\n",
            "Epoch[0/2] Step[2614] Loss=0.1517 GradNorm=1.6366 StepSize=0.0164 RelImp=93.39%\n",
            "Epoch[0/2] Step[2615] Loss=0.2959 GradNorm=2.6763 StepSize=0.0268 RelImp=87.12%\n",
            "Epoch[0/2] Step[2616] Loss=0.3933 GradNorm=4.6173 StepSize=0.0462 RelImp=82.88%\n",
            "Epoch[0/2] Step[2617] Loss=0.1786 GradNorm=1.2040 StepSize=0.0120 RelImp=92.22%\n",
            "Epoch[0/2] Step[2618] Loss=0.4482 GradNorm=2.2828 StepSize=0.0228 RelImp=80.48%\n",
            "Epoch[0/2] Step[2619] Loss=0.5989 GradNorm=3.1844 StepSize=0.0318 RelImp=73.92%\n",
            "Epoch[0/2] Step[2620] Loss=0.9422 GradNorm=5.7064 StepSize=0.0571 RelImp=58.97%\n",
            "Epoch[0/2] Step[2621] Loss=0.6747 GradNorm=4.1831 StepSize=0.0418 RelImp=70.62%\n",
            "Epoch[0/2] Step[2622] Loss=0.2657 GradNorm=2.4995 StepSize=0.0250 RelImp=88.43%\n",
            "Epoch[0/2] Step[2623] Loss=0.9134 GradNorm=3.1285 StepSize=0.0313 RelImp=60.23%\n",
            "Epoch[0/2] Step[2624] Loss=0.2684 GradNorm=1.8694 StepSize=0.0187 RelImp=88.31%\n",
            "Epoch[0/2] Step[2625] Loss=0.4025 GradNorm=2.0743 StepSize=0.0207 RelImp=82.48%\n",
            "Epoch[0/2] Step[2626] Loss=0.3637 GradNorm=3.2081 StepSize=0.0321 RelImp=84.17%\n",
            "Epoch[0/2] Step[2627] Loss=0.3027 GradNorm=2.7240 StepSize=0.0272 RelImp=86.82%\n",
            "Epoch[0/2] Step[2628] Loss=0.4970 GradNorm=3.7677 StepSize=0.0377 RelImp=78.36%\n",
            "Epoch[0/2] Step[2629] Loss=0.2579 GradNorm=2.2360 StepSize=0.0224 RelImp=88.77%\n",
            "Epoch[0/2] Step[2630] Loss=0.2147 GradNorm=3.5388 StepSize=0.0354 RelImp=90.65%\n",
            "Epoch[0/2] Step[2631] Loss=0.5883 GradNorm=3.5655 StepSize=0.0357 RelImp=74.38%\n",
            "Epoch[0/2] Step[2632] Loss=0.3077 GradNorm=3.7922 StepSize=0.0379 RelImp=86.60%\n",
            "Epoch[0/2] Step[2633] Loss=0.4522 GradNorm=4.7240 StepSize=0.0472 RelImp=80.31%\n",
            "Epoch[0/2] Step[2634] Loss=0.1376 GradNorm=1.2253 StepSize=0.0123 RelImp=94.01%\n",
            "Epoch[0/2] Step[2635] Loss=0.2675 GradNorm=3.0934 StepSize=0.0309 RelImp=88.35%\n",
            "Epoch[0/2] Step[2636] Loss=0.5441 GradNorm=3.4787 StepSize=0.0348 RelImp=76.31%\n",
            "Epoch[0/2] Step[2637] Loss=0.6778 GradNorm=4.0397 StepSize=0.0404 RelImp=70.49%\n",
            "Epoch[0/2] Step[2638] Loss=0.2716 GradNorm=2.4703 StepSize=0.0247 RelImp=88.17%\n",
            "Epoch[0/2] Step[2639] Loss=0.3281 GradNorm=2.0408 StepSize=0.0204 RelImp=85.71%\n",
            "Epoch[0/2] Step[2640] Loss=0.1794 GradNorm=2.9269 StepSize=0.0293 RelImp=92.19%\n",
            "Epoch[0/2] Step[2641] Loss=1.0060 GradNorm=6.4929 StepSize=0.0649 RelImp=56.20%\n",
            "Epoch[0/2] Step[2642] Loss=0.2752 GradNorm=2.5510 StepSize=0.0255 RelImp=88.02%\n",
            "Epoch[0/2] Step[2643] Loss=0.1627 GradNorm=1.9146 StepSize=0.0191 RelImp=92.91%\n",
            "Epoch[0/2] Step[2644] Loss=0.4544 GradNorm=4.5656 StepSize=0.0457 RelImp=80.21%\n",
            "Epoch[0/2] Step[2645] Loss=0.3238 GradNorm=4.1520 StepSize=0.0415 RelImp=85.90%\n",
            "Epoch[0/2] Step[2646] Loss=0.4592 GradNorm=4.7210 StepSize=0.0472 RelImp=80.01%\n",
            "Epoch[0/2] Step[2647] Loss=0.2948 GradNorm=2.5476 StepSize=0.0255 RelImp=87.16%\n",
            "Epoch[0/2] Step[2648] Loss=0.3246 GradNorm=2.7570 StepSize=0.0276 RelImp=85.87%\n",
            "Epoch[0/2] Step[2649] Loss=0.1902 GradNorm=2.5307 StepSize=0.0253 RelImp=91.72%\n",
            "Epoch[0/2] Step[2650] Loss=0.2836 GradNorm=3.1804 StepSize=0.0318 RelImp=87.65%\n",
            "Epoch[0/2] Step[2651] Loss=0.5473 GradNorm=4.6980 StepSize=0.0470 RelImp=76.17%\n",
            "Epoch[0/2] Step[2652] Loss=1.0231 GradNorm=5.9252 StepSize=0.0593 RelImp=55.45%\n",
            "Epoch[0/2] Step[2653] Loss=0.6203 GradNorm=4.6721 StepSize=0.0467 RelImp=72.99%\n",
            "Epoch[0/2] Step[2654] Loss=0.8199 GradNorm=3.2504 StepSize=0.0325 RelImp=64.30%\n",
            "Epoch[0/2] Step[2655] Loss=0.6935 GradNorm=8.2157 StepSize=0.0822 RelImp=69.80%\n",
            "Epoch[0/2] Step[2656] Loss=0.2412 GradNorm=2.6308 StepSize=0.0263 RelImp=89.50%\n",
            "Epoch[0/2] Step[2657] Loss=0.5151 GradNorm=3.4022 StepSize=0.0340 RelImp=77.57%\n",
            "Epoch[0/2] Step[2658] Loss=0.4039 GradNorm=2.9820 StepSize=0.0298 RelImp=82.41%\n",
            "Epoch[0/2] Step[2659] Loss=0.5025 GradNorm=3.4511 StepSize=0.0345 RelImp=78.12%\n",
            "Epoch[0/2] Step[2660] Loss=0.1344 GradNorm=1.9120 StepSize=0.0191 RelImp=94.15%\n",
            "Epoch[0/2] Step[2661] Loss=0.4817 GradNorm=4.7975 StepSize=0.0480 RelImp=79.02%\n",
            "Epoch[0/2] Step[2662] Loss=0.3311 GradNorm=3.8388 StepSize=0.0384 RelImp=85.58%\n",
            "Epoch[0/2] Step[2663] Loss=0.8136 GradNorm=4.3395 StepSize=0.0434 RelImp=64.57%\n",
            "Epoch[0/2] Step[2664] Loss=0.3176 GradNorm=3.0171 StepSize=0.0302 RelImp=86.17%\n",
            "Epoch[0/2] Step[2665] Loss=0.3093 GradNorm=2.6558 StepSize=0.0266 RelImp=86.53%\n",
            "Epoch[0/2] Step[2666] Loss=1.0396 GradNorm=5.4385 StepSize=0.0544 RelImp=54.73%\n",
            "Epoch[0/2] Step[2667] Loss=0.6256 GradNorm=4.8129 StepSize=0.0481 RelImp=72.76%\n",
            "Epoch[0/2] Step[2668] Loss=0.0836 GradNorm=0.9916 StepSize=0.0099 RelImp=96.36%\n",
            "Epoch[0/2] Step[2669] Loss=0.6606 GradNorm=3.9109 StepSize=0.0391 RelImp=71.24%\n",
            "Epoch[0/2] Step[2670] Loss=0.4348 GradNorm=4.1897 StepSize=0.0419 RelImp=81.07%\n",
            "Epoch[0/2] Step[2671] Loss=0.2301 GradNorm=1.7217 StepSize=0.0172 RelImp=89.98%\n",
            "Epoch[0/2] Step[2672] Loss=0.3827 GradNorm=4.0241 StepSize=0.0402 RelImp=83.34%\n",
            "Epoch[0/2] Step[2673] Loss=0.5902 GradNorm=3.2617 StepSize=0.0326 RelImp=74.30%\n",
            "Epoch[0/2] Step[2674] Loss=0.4337 GradNorm=2.6976 StepSize=0.0270 RelImp=81.12%\n",
            "Epoch[0/2] Step[2675] Loss=0.4912 GradNorm=3.2280 StepSize=0.0323 RelImp=78.61%\n",
            "Epoch[0/2] Step[2676] Loss=1.2513 GradNorm=6.6536 StepSize=0.0665 RelImp=45.51%\n",
            "Epoch[0/2] Step[2677] Loss=0.3853 GradNorm=4.8977 StepSize=0.0490 RelImp=83.22%\n",
            "Epoch[0/2] Step[2678] Loss=0.4382 GradNorm=4.6616 StepSize=0.0466 RelImp=80.92%\n",
            "Epoch[0/2] Step[2679] Loss=0.1477 GradNorm=2.1996 StepSize=0.0220 RelImp=93.57%\n",
            "Epoch[0/2] Step[2680] Loss=0.1637 GradNorm=1.5271 StepSize=0.0153 RelImp=92.87%\n",
            "Epoch[0/2] Step[2681] Loss=0.5305 GradNorm=4.8700 StepSize=0.0487 RelImp=76.90%\n",
            "Epoch[0/2] Step[2682] Loss=0.5625 GradNorm=2.8680 StepSize=0.0287 RelImp=75.51%\n",
            "Epoch[0/2] Step[2683] Loss=0.5745 GradNorm=3.9578 StepSize=0.0396 RelImp=74.99%\n",
            "Epoch[0/2] Step[2684] Loss=0.2643 GradNorm=2.2808 StepSize=0.0228 RelImp=88.49%\n",
            "Epoch[0/2] Step[2685] Loss=0.3524 GradNorm=4.6552 StepSize=0.0466 RelImp=84.66%\n",
            "Epoch[0/2] Step[2686] Loss=0.4468 GradNorm=3.9523 StepSize=0.0395 RelImp=80.54%\n",
            "Epoch[0/2] Step[2687] Loss=0.2049 GradNorm=2.2784 StepSize=0.0228 RelImp=91.08%\n",
            "Epoch[0/2] Step[2688] Loss=0.2518 GradNorm=2.5958 StepSize=0.0260 RelImp=89.04%\n",
            "Epoch[0/2] Step[2689] Loss=0.5507 GradNorm=3.2597 StepSize=0.0326 RelImp=76.02%\n",
            "Epoch[0/2] Step[2690] Loss=1.0998 GradNorm=3.3097 StepSize=0.0331 RelImp=52.11%\n",
            "Epoch[0/2] Step[2691] Loss=0.5079 GradNorm=3.0452 StepSize=0.0305 RelImp=77.88%\n",
            "Epoch[0/2] Step[2692] Loss=0.3701 GradNorm=3.3837 StepSize=0.0338 RelImp=83.88%\n",
            "Epoch[0/2] Step[2693] Loss=0.5049 GradNorm=3.9746 StepSize=0.0397 RelImp=78.02%\n",
            "Epoch[0/2] Step[2694] Loss=0.3483 GradNorm=3.4837 StepSize=0.0348 RelImp=84.83%\n",
            "Epoch[0/2] Step[2695] Loss=0.6826 GradNorm=3.0133 StepSize=0.0301 RelImp=70.28%\n",
            "Epoch[0/2] Step[2696] Loss=0.4424 GradNorm=3.3290 StepSize=0.0333 RelImp=80.74%\n",
            "Epoch[0/2] Step[2697] Loss=0.7179 GradNorm=4.2238 StepSize=0.0422 RelImp=68.74%\n",
            "Epoch[0/2] Step[2698] Loss=0.1012 GradNorm=1.7292 StepSize=0.0173 RelImp=95.59%\n",
            "Epoch[0/2] Step[2699] Loss=0.3752 GradNorm=3.6696 StepSize=0.0367 RelImp=83.66%\n",
            "Epoch[0/2] Step[2700] Loss=0.2579 GradNorm=2.7776 StepSize=0.0278 RelImp=88.77%\n",
            "Epoch[0/2] Step[2701] Loss=0.5263 GradNorm=2.5108 StepSize=0.0251 RelImp=77.08%\n",
            "Epoch[0/2] Step[2702] Loss=0.3392 GradNorm=2.9669 StepSize=0.0297 RelImp=85.23%\n",
            "Epoch[0/2] Step[2703] Loss=0.2029 GradNorm=1.5834 StepSize=0.0158 RelImp=91.16%\n",
            "Epoch[0/2] Step[2704] Loss=0.7932 GradNorm=3.6641 StepSize=0.0366 RelImp=65.46%\n",
            "Epoch[0/2] Step[2705] Loss=0.6020 GradNorm=4.5605 StepSize=0.0456 RelImp=73.79%\n",
            "Epoch[0/2] Step[2706] Loss=0.6050 GradNorm=3.4037 StepSize=0.0340 RelImp=73.66%\n",
            "Epoch[0/2] Step[2707] Loss=0.2054 GradNorm=2.6315 StepSize=0.0263 RelImp=91.06%\n",
            "Epoch[0/2] Step[2708] Loss=0.6057 GradNorm=3.0684 StepSize=0.0307 RelImp=73.63%\n",
            "Epoch[0/2] Step[2709] Loss=0.3229 GradNorm=2.5791 StepSize=0.0258 RelImp=85.94%\n",
            "Epoch[0/2] Step[2710] Loss=0.2550 GradNorm=2.0588 StepSize=0.0206 RelImp=88.90%\n",
            "Epoch[0/2] Step[2711] Loss=0.7758 GradNorm=5.8968 StepSize=0.0590 RelImp=66.22%\n",
            "Epoch[0/2] Step[2712] Loss=0.2580 GradNorm=2.3692 StepSize=0.0237 RelImp=88.77%\n",
            "Epoch[0/2] Step[2713] Loss=0.6409 GradNorm=3.5554 StepSize=0.0356 RelImp=72.09%\n",
            "Epoch[0/2] Step[2714] Loss=0.3739 GradNorm=4.7520 StepSize=0.0475 RelImp=83.72%\n",
            "Epoch[0/2] Step[2715] Loss=0.9073 GradNorm=5.0426 StepSize=0.0504 RelImp=60.49%\n",
            "Epoch[0/2] Step[2716] Loss=0.4852 GradNorm=3.9951 StepSize=0.0400 RelImp=78.87%\n",
            "Epoch[0/2] Step[2717] Loss=0.5957 GradNorm=5.4004 StepSize=0.0540 RelImp=74.06%\n",
            "Epoch[0/2] Step[2718] Loss=0.2109 GradNorm=3.1040 StepSize=0.0310 RelImp=90.82%\n",
            "Epoch[0/2] Step[2719] Loss=0.1307 GradNorm=1.2596 StepSize=0.0126 RelImp=94.31%\n",
            "Epoch[0/2] Step[2720] Loss=0.1389 GradNorm=1.3128 StepSize=0.0131 RelImp=93.95%\n",
            "Epoch[0/2] Step[2721] Loss=0.2751 GradNorm=2.3728 StepSize=0.0237 RelImp=88.02%\n",
            "Epoch[0/2] Step[2722] Loss=1.3120 GradNorm=4.0706 StepSize=0.0407 RelImp=42.87%\n",
            "Epoch[0/2] Step[2723] Loss=0.1086 GradNorm=1.5248 StepSize=0.0152 RelImp=95.27%\n",
            "Epoch[0/2] Step[2724] Loss=0.4781 GradNorm=3.5745 StepSize=0.0357 RelImp=79.18%\n",
            "Epoch[0/2] Step[2725] Loss=0.6527 GradNorm=5.8278 StepSize=0.0583 RelImp=71.58%\n",
            "Epoch[0/2] Step[2726] Loss=0.2109 GradNorm=1.6121 StepSize=0.0161 RelImp=90.82%\n",
            "Epoch[0/2] Step[2727] Loss=0.6226 GradNorm=5.0934 StepSize=0.0509 RelImp=72.89%\n",
            "Epoch[0/2] Step[2728] Loss=0.2955 GradNorm=2.4676 StepSize=0.0247 RelImp=87.13%\n",
            "Epoch[0/2] Step[2729] Loss=0.3019 GradNorm=2.8365 StepSize=0.0284 RelImp=86.86%\n",
            "Epoch[0/2] Step[2730] Loss=0.1216 GradNorm=1.1655 StepSize=0.0117 RelImp=94.71%\n",
            "Epoch[0/2] Step[2731] Loss=0.3344 GradNorm=2.0675 StepSize=0.0207 RelImp=85.44%\n",
            "Epoch[0/2] Step[2732] Loss=0.3062 GradNorm=2.9009 StepSize=0.0290 RelImp=86.67%\n",
            "Epoch[0/2] Step[2733] Loss=0.6231 GradNorm=3.3465 StepSize=0.0335 RelImp=72.87%\n",
            "Epoch[0/2] Step[2734] Loss=0.3465 GradNorm=3.3177 StepSize=0.0332 RelImp=84.91%\n",
            "Epoch[0/2] Step[2735] Loss=0.3842 GradNorm=3.7037 StepSize=0.0370 RelImp=83.27%\n",
            "Epoch[0/2] Step[2736] Loss=0.4457 GradNorm=2.3539 StepSize=0.0235 RelImp=80.59%\n",
            "Epoch[0/2] Step[2737] Loss=0.3131 GradNorm=2.4552 StepSize=0.0246 RelImp=86.37%\n",
            "Epoch[0/2] Step[2738] Loss=0.3071 GradNorm=2.4038 StepSize=0.0240 RelImp=86.63%\n",
            "Epoch[0/2] Step[2739] Loss=0.4448 GradNorm=4.6357 StepSize=0.0464 RelImp=80.63%\n",
            "Epoch[0/2] Step[2740] Loss=0.3827 GradNorm=2.2119 StepSize=0.0221 RelImp=83.34%\n",
            "Epoch[0/2] Step[2741] Loss=0.4070 GradNorm=3.2436 StepSize=0.0324 RelImp=82.28%\n",
            "Epoch[0/2] Step[2742] Loss=0.2345 GradNorm=2.4983 StepSize=0.0250 RelImp=89.79%\n",
            "Epoch[0/2] Step[2743] Loss=0.1956 GradNorm=1.8636 StepSize=0.0186 RelImp=91.48%\n",
            "Epoch[0/2] Step[2744] Loss=0.7517 GradNorm=5.5700 StepSize=0.0557 RelImp=67.27%\n",
            "Epoch[0/2] Step[2745] Loss=0.5196 GradNorm=4.6793 StepSize=0.0468 RelImp=77.37%\n",
            "Epoch[0/2] Step[2746] Loss=0.1539 GradNorm=1.3413 StepSize=0.0134 RelImp=93.30%\n",
            "Epoch[0/2] Step[2747] Loss=0.3436 GradNorm=3.0909 StepSize=0.0309 RelImp=85.04%\n",
            "Epoch[0/2] Step[2748] Loss=0.6800 GradNorm=4.0967 StepSize=0.0410 RelImp=70.39%\n",
            "Epoch[0/2] Step[2749] Loss=0.2326 GradNorm=2.5711 StepSize=0.0257 RelImp=89.87%\n",
            "Epoch[0/2] Step[2750] Loss=0.4446 GradNorm=3.0805 StepSize=0.0308 RelImp=80.64%\n",
            "Epoch[0/2] Step[2751] Loss=0.1728 GradNorm=2.6525 StepSize=0.0265 RelImp=92.48%\n",
            "Epoch[0/2] Step[2752] Loss=0.5289 GradNorm=3.8824 StepSize=0.0388 RelImp=76.97%\n",
            "Epoch[0/2] Step[2753] Loss=0.2465 GradNorm=2.2317 StepSize=0.0223 RelImp=89.26%\n",
            "Epoch[0/2] Step[2754] Loss=0.1895 GradNorm=2.4359 StepSize=0.0244 RelImp=91.75%\n",
            "Epoch[0/2] Step[2755] Loss=0.2456 GradNorm=2.9607 StepSize=0.0296 RelImp=89.31%\n",
            "Epoch[0/2] Step[2756] Loss=0.1889 GradNorm=2.7482 StepSize=0.0275 RelImp=91.78%\n",
            "Epoch[0/2] Step[2757] Loss=0.2103 GradNorm=2.8028 StepSize=0.0280 RelImp=90.84%\n",
            "Epoch[0/2] Step[2758] Loss=0.3027 GradNorm=3.1686 StepSize=0.0317 RelImp=86.82%\n",
            "Epoch[0/2] Step[2759] Loss=0.1847 GradNorm=1.3002 StepSize=0.0130 RelImp=91.96%\n",
            "Epoch[0/2] Step[2760] Loss=0.4533 GradNorm=3.3023 StepSize=0.0330 RelImp=80.26%\n",
            "Epoch[0/2] Step[2761] Loss=0.7480 GradNorm=4.4564 StepSize=0.0446 RelImp=67.43%\n",
            "Epoch[0/2] Step[2762] Loss=0.7194 GradNorm=6.3854 StepSize=0.0639 RelImp=68.67%\n",
            "Epoch[0/2] Step[2763] Loss=0.6320 GradNorm=4.5072 StepSize=0.0451 RelImp=72.48%\n",
            "Epoch[0/2] Step[2764] Loss=0.1229 GradNorm=1.6107 StepSize=0.0161 RelImp=94.65%\n",
            "Epoch[0/2] Step[2765] Loss=0.8381 GradNorm=3.7985 StepSize=0.0380 RelImp=63.51%\n",
            "Epoch[0/2] Step[2766] Loss=0.3402 GradNorm=3.3597 StepSize=0.0336 RelImp=85.19%\n",
            "Epoch[0/2] Step[2767] Loss=0.3360 GradNorm=3.0013 StepSize=0.0300 RelImp=85.37%\n",
            "Epoch[0/2] Step[2768] Loss=0.3678 GradNorm=2.6789 StepSize=0.0268 RelImp=83.98%\n",
            "Epoch[0/2] Step[2769] Loss=0.4740 GradNorm=5.2752 StepSize=0.0528 RelImp=79.36%\n",
            "Epoch[0/2] Step[2770] Loss=0.2072 GradNorm=2.5990 StepSize=0.0260 RelImp=90.98%\n",
            "Epoch[0/2] Step[2771] Loss=0.3578 GradNorm=3.5096 StepSize=0.0351 RelImp=84.42%\n",
            "Epoch[0/2] Step[2772] Loss=0.3456 GradNorm=3.2561 StepSize=0.0326 RelImp=84.95%\n",
            "Epoch[0/2] Step[2773] Loss=0.4250 GradNorm=3.4043 StepSize=0.0340 RelImp=81.50%\n",
            "Epoch[0/2] Step[2774] Loss=0.2346 GradNorm=2.5454 StepSize=0.0255 RelImp=89.78%\n",
            "Epoch[0/2] Step[2775] Loss=0.2190 GradNorm=2.5856 StepSize=0.0259 RelImp=90.47%\n",
            "Epoch[0/2] Step[2776] Loss=0.2677 GradNorm=3.5316 StepSize=0.0353 RelImp=88.35%\n",
            "Epoch[0/2] Step[2777] Loss=0.1727 GradNorm=1.7797 StepSize=0.0178 RelImp=92.48%\n",
            "Epoch[0/2] Step[2778] Loss=0.3440 GradNorm=2.7241 StepSize=0.0272 RelImp=85.02%\n",
            "Epoch[0/2] Step[2779] Loss=0.6452 GradNorm=4.4371 StepSize=0.0444 RelImp=71.91%\n",
            "Epoch[0/2] Step[2780] Loss=0.2855 GradNorm=3.9525 StepSize=0.0395 RelImp=87.57%\n",
            "Epoch[0/2] Step[2781] Loss=0.1525 GradNorm=1.8066 StepSize=0.0181 RelImp=93.36%\n",
            "Epoch[0/2] Step[2782] Loss=0.1823 GradNorm=2.0571 StepSize=0.0206 RelImp=92.06%\n",
            "Epoch[0/2] Step[2783] Loss=0.1491 GradNorm=2.1511 StepSize=0.0215 RelImp=93.51%\n",
            "Epoch[0/2] Step[2784] Loss=0.7522 GradNorm=3.3650 StepSize=0.0336 RelImp=67.25%\n",
            "Epoch[0/2] Step[2785] Loss=0.2376 GradNorm=2.5939 StepSize=0.0259 RelImp=89.65%\n",
            "Epoch[0/2] Step[2786] Loss=0.5857 GradNorm=4.8277 StepSize=0.0483 RelImp=74.50%\n",
            "Epoch[0/2] Step[2787] Loss=0.3853 GradNorm=2.9512 StepSize=0.0295 RelImp=83.22%\n",
            "Epoch[0/2] Step[2788] Loss=0.1454 GradNorm=1.7218 StepSize=0.0172 RelImp=93.67%\n",
            "Epoch[0/2] Step[2789] Loss=0.1532 GradNorm=1.8110 StepSize=0.0181 RelImp=93.33%\n",
            "Epoch[0/2] Step[2790] Loss=0.5921 GradNorm=4.9577 StepSize=0.0496 RelImp=74.22%\n",
            "Epoch[0/2] Step[2791] Loss=0.4016 GradNorm=3.2214 StepSize=0.0322 RelImp=82.51%\n",
            "Epoch[0/2] Step[2792] Loss=0.1355 GradNorm=1.1281 StepSize=0.0113 RelImp=94.10%\n",
            "Epoch[0/2] Step[2793] Loss=0.7517 GradNorm=4.8289 StepSize=0.0483 RelImp=67.27%\n",
            "Epoch[0/2] Step[2794] Loss=0.4206 GradNorm=2.8888 StepSize=0.0289 RelImp=81.69%\n",
            "Epoch[0/2] Step[2795] Loss=0.3897 GradNorm=3.8550 StepSize=0.0386 RelImp=83.03%\n",
            "Epoch[0/2] Step[2796] Loss=0.4177 GradNorm=3.8214 StepSize=0.0382 RelImp=81.81%\n",
            "Epoch[0/2] Step[2797] Loss=0.1959 GradNorm=1.9339 StepSize=0.0193 RelImp=91.47%\n",
            "Epoch[0/2] Step[2798] Loss=0.2091 GradNorm=2.8297 StepSize=0.0283 RelImp=90.90%\n",
            "Epoch[0/2] Step[2799] Loss=0.6160 GradNorm=5.1952 StepSize=0.0520 RelImp=73.18%\n",
            "Epoch[0/2] Step[2800] Loss=0.3992 GradNorm=5.1974 StepSize=0.0520 RelImp=82.62%\n",
            "Epoch[0/2] Step[2801] Loss=1.0126 GradNorm=5.1911 StepSize=0.0519 RelImp=55.91%\n",
            "Epoch[0/2] Step[2802] Loss=1.0467 GradNorm=2.4812 StepSize=0.0248 RelImp=54.42%\n",
            "Epoch[0/2] Step[2803] Loss=0.6675 GradNorm=4.5754 StepSize=0.0458 RelImp=70.93%\n",
            "Epoch[0/2] Step[2804] Loss=0.2141 GradNorm=2.4089 StepSize=0.0241 RelImp=90.68%\n",
            "Epoch[0/2] Step[2805] Loss=0.2905 GradNorm=2.7591 StepSize=0.0276 RelImp=87.35%\n",
            "Epoch[0/2] Step[2806] Loss=0.5480 GradNorm=3.7509 StepSize=0.0375 RelImp=76.14%\n",
            "Epoch[0/2] Step[2807] Loss=0.1680 GradNorm=1.6559 StepSize=0.0166 RelImp=92.69%\n",
            "Epoch[0/2] Step[2808] Loss=0.2171 GradNorm=1.7974 StepSize=0.0180 RelImp=90.55%\n",
            "Epoch[0/2] Step[2809] Loss=0.3151 GradNorm=2.7038 StepSize=0.0270 RelImp=86.28%\n",
            "Epoch[0/2] Step[2810] Loss=0.3542 GradNorm=2.4874 StepSize=0.0249 RelImp=84.58%\n",
            "Epoch[0/2] Step[2811] Loss=0.2472 GradNorm=2.5591 StepSize=0.0256 RelImp=89.24%\n",
            "Epoch[0/2] Step[2812] Loss=0.3479 GradNorm=2.6654 StepSize=0.0267 RelImp=84.85%\n",
            "Epoch[0/2] Step[2813] Loss=0.7079 GradNorm=4.6363 StepSize=0.0464 RelImp=69.18%\n",
            "Epoch[0/2] Step[2814] Loss=0.9779 GradNorm=4.1525 StepSize=0.0415 RelImp=57.42%\n",
            "Epoch[0/2] Step[2815] Loss=0.1430 GradNorm=2.1272 StepSize=0.0213 RelImp=93.77%\n",
            "Epoch[0/2] Step[2816] Loss=0.3744 GradNorm=3.4131 StepSize=0.0341 RelImp=83.70%\n",
            "Epoch[0/2] Step[2817] Loss=0.1635 GradNorm=2.2304 StepSize=0.0223 RelImp=92.88%\n",
            "Epoch[0/2] Step[2818] Loss=0.5396 GradNorm=3.4393 StepSize=0.0344 RelImp=76.50%\n",
            "Epoch[0/2] Step[2819] Loss=1.0104 GradNorm=4.6940 StepSize=0.0469 RelImp=56.00%\n",
            "Epoch[0/2] Step[2820] Loss=0.1320 GradNorm=1.9793 StepSize=0.0198 RelImp=94.25%\n",
            "Epoch[0/2] Step[2821] Loss=0.2490 GradNorm=3.2778 StepSize=0.0328 RelImp=89.16%\n",
            "Epoch[0/2] Step[2822] Loss=0.2388 GradNorm=2.2198 StepSize=0.0222 RelImp=89.60%\n",
            "Epoch[0/2] Step[2823] Loss=0.5517 GradNorm=3.5514 StepSize=0.0355 RelImp=75.98%\n",
            "Epoch[0/2] Step[2824] Loss=0.5304 GradNorm=4.1159 StepSize=0.0412 RelImp=76.91%\n",
            "Epoch[0/2] Step[2825] Loss=0.0665 GradNorm=0.8468 StepSize=0.0085 RelImp=97.11%\n",
            "Epoch[0/2] Step[2826] Loss=0.2917 GradNorm=3.3012 StepSize=0.0330 RelImp=87.30%\n",
            "Epoch[0/2] Step[2827] Loss=0.9038 GradNorm=4.1682 StepSize=0.0417 RelImp=60.65%\n",
            "Epoch[0/2] Step[2828] Loss=0.5855 GradNorm=3.0861 StepSize=0.0309 RelImp=74.51%\n",
            "Epoch[0/2] Step[2829] Loss=0.2608 GradNorm=2.1532 StepSize=0.0215 RelImp=88.64%\n",
            "Epoch[0/2] Step[2830] Loss=0.4858 GradNorm=3.8269 StepSize=0.0383 RelImp=78.85%\n",
            "Epoch[0/2] Step[2831] Loss=0.1731 GradNorm=1.1348 StepSize=0.0113 RelImp=92.46%\n",
            "Epoch[0/2] Step[2832] Loss=0.3464 GradNorm=2.8542 StepSize=0.0285 RelImp=84.92%\n",
            "Epoch[0/2] Step[2833] Loss=0.2203 GradNorm=2.2797 StepSize=0.0228 RelImp=90.41%\n",
            "Epoch[0/2] Step[2834] Loss=1.0683 GradNorm=8.6663 StepSize=0.0867 RelImp=53.48%\n",
            "Epoch[0/2] Step[2835] Loss=1.0740 GradNorm=7.7327 StepSize=0.0773 RelImp=53.23%\n",
            "Epoch[0/2] Step[2836] Loss=0.5543 GradNorm=3.6273 StepSize=0.0363 RelImp=75.87%\n",
            "Epoch[0/2] Step[2837] Loss=0.6265 GradNorm=5.2473 StepSize=0.0525 RelImp=72.72%\n",
            "Epoch[0/2] Step[2838] Loss=0.3201 GradNorm=3.1497 StepSize=0.0315 RelImp=86.06%\n",
            "Epoch[0/2] Step[2839] Loss=0.0747 GradNorm=0.8964 StepSize=0.0090 RelImp=96.75%\n",
            "Epoch[0/2] Step[2840] Loss=0.1441 GradNorm=1.3450 StepSize=0.0134 RelImp=93.73%\n",
            "Epoch[0/2] Step[2841] Loss=0.1274 GradNorm=1.1564 StepSize=0.0116 RelImp=94.45%\n",
            "Epoch[0/2] Step[2842] Loss=0.2559 GradNorm=2.0370 StepSize=0.0204 RelImp=88.86%\n",
            "Epoch[0/2] Step[2843] Loss=0.3944 GradNorm=1.6474 StepSize=0.0165 RelImp=82.83%\n",
            "Epoch[0/2] Step[2844] Loss=0.5639 GradNorm=3.6177 StepSize=0.0362 RelImp=75.45%\n",
            "Epoch[0/2] Step[2845] Loss=0.5166 GradNorm=4.2094 StepSize=0.0421 RelImp=77.51%\n",
            "Epoch[0/2] Step[2846] Loss=0.2643 GradNorm=2.0975 StepSize=0.0210 RelImp=88.49%\n",
            "Epoch[0/2] Step[2847] Loss=0.2651 GradNorm=2.4331 StepSize=0.0243 RelImp=88.46%\n",
            "Epoch[0/2] Step[2848] Loss=0.5080 GradNorm=3.0182 StepSize=0.0302 RelImp=77.88%\n",
            "Epoch[0/2] Step[2849] Loss=0.2212 GradNorm=1.9024 StepSize=0.0190 RelImp=90.37%\n",
            "Epoch[0/2] Step[2850] Loss=0.2665 GradNorm=2.4135 StepSize=0.0241 RelImp=88.39%\n",
            "Epoch[0/2] Step[2851] Loss=0.1471 GradNorm=1.6209 StepSize=0.0162 RelImp=93.60%\n",
            "Epoch[0/2] Step[2852] Loss=0.4352 GradNorm=3.1677 StepSize=0.0317 RelImp=81.05%\n",
            "Epoch[0/2] Step[2853] Loss=0.8498 GradNorm=4.4154 StepSize=0.0442 RelImp=63.00%\n",
            "Epoch[0/2] Step[2854] Loss=0.7422 GradNorm=4.2003 StepSize=0.0420 RelImp=67.68%\n",
            "Epoch[0/2] Step[2855] Loss=0.1665 GradNorm=1.9025 StepSize=0.0190 RelImp=92.75%\n",
            "Epoch[0/2] Step[2856] Loss=0.6147 GradNorm=3.5352 StepSize=0.0354 RelImp=73.23%\n",
            "Epoch[0/2] Step[2857] Loss=0.2949 GradNorm=2.2062 StepSize=0.0221 RelImp=87.16%\n",
            "Epoch[0/2] Step[2858] Loss=0.2596 GradNorm=3.3076 StepSize=0.0331 RelImp=88.70%\n",
            "Epoch[0/2] Step[2859] Loss=0.5488 GradNorm=4.2874 StepSize=0.0429 RelImp=76.10%\n",
            "Epoch[0/2] Step[2860] Loss=0.5765 GradNorm=3.0506 StepSize=0.0305 RelImp=74.90%\n",
            "Epoch[0/2] Step[2861] Loss=0.0973 GradNorm=1.0778 StepSize=0.0108 RelImp=95.76%\n",
            "Epoch[0/2] Step[2862] Loss=0.3456 GradNorm=2.6894 StepSize=0.0269 RelImp=84.95%\n",
            "Epoch[0/2] Step[2863] Loss=0.2699 GradNorm=1.7076 StepSize=0.0171 RelImp=88.25%\n",
            "Epoch[0/2] Step[2864] Loss=0.5367 GradNorm=3.5393 StepSize=0.0354 RelImp=76.63%\n",
            "Epoch[0/2] Step[2865] Loss=0.4047 GradNorm=4.5111 StepSize=0.0451 RelImp=82.38%\n",
            "Epoch[0/2] Step[2866] Loss=0.3404 GradNorm=3.0970 StepSize=0.0310 RelImp=85.18%\n",
            "Epoch[0/2] Step[2867] Loss=0.3125 GradNorm=3.4733 StepSize=0.0347 RelImp=86.39%\n",
            "Epoch[0/2] Step[2868] Loss=1.0013 GradNorm=4.7257 StepSize=0.0473 RelImp=56.40%\n",
            "Epoch[0/2] Step[2869] Loss=0.6034 GradNorm=3.9205 StepSize=0.0392 RelImp=73.72%\n",
            "Epoch[0/2] Step[2870] Loss=0.3905 GradNorm=3.3818 StepSize=0.0338 RelImp=82.99%\n",
            "Epoch[0/2] Step[2871] Loss=0.9657 GradNorm=3.8202 StepSize=0.0382 RelImp=57.95%\n",
            "Epoch[0/2] Step[2872] Loss=0.4536 GradNorm=3.1390 StepSize=0.0314 RelImp=80.25%\n",
            "Epoch[0/2] Step[2873] Loss=0.2535 GradNorm=2.1453 StepSize=0.0215 RelImp=88.96%\n",
            "Epoch[0/2] Step[2874] Loss=0.4151 GradNorm=2.8081 StepSize=0.0281 RelImp=81.93%\n",
            "Epoch[0/2] Step[2875] Loss=0.1864 GradNorm=1.9287 StepSize=0.0193 RelImp=91.88%\n",
            "Epoch[0/2] Step[2876] Loss=1.0136 GradNorm=4.3850 StepSize=0.0438 RelImp=55.86%\n",
            "Epoch[0/2] Step[2877] Loss=0.5487 GradNorm=3.7867 StepSize=0.0379 RelImp=76.11%\n",
            "Epoch[0/2] Step[2878] Loss=0.6320 GradNorm=5.0668 StepSize=0.0507 RelImp=72.48%\n",
            "Epoch[0/2] Step[2879] Loss=0.8054 GradNorm=4.4767 StepSize=0.0448 RelImp=64.93%\n",
            "Epoch[0/2] Step[2880] Loss=0.3908 GradNorm=5.2254 StepSize=0.0523 RelImp=82.98%\n",
            "Epoch[0/2] Step[2881] Loss=0.4476 GradNorm=4.2722 StepSize=0.0427 RelImp=80.51%\n",
            "Epoch[0/2] Step[2882] Loss=0.2345 GradNorm=2.5162 StepSize=0.0252 RelImp=89.79%\n",
            "Epoch[0/2] Step[2883] Loss=0.5084 GradNorm=4.5751 StepSize=0.0458 RelImp=77.86%\n",
            "Epoch[0/2] Step[2884] Loss=0.6459 GradNorm=6.2821 StepSize=0.0628 RelImp=71.88%\n",
            "Epoch[0/2] Step[2885] Loss=0.4763 GradNorm=3.0235 StepSize=0.0302 RelImp=79.26%\n",
            "Epoch[0/2] Step[2886] Loss=0.1868 GradNorm=2.2228 StepSize=0.0222 RelImp=91.87%\n",
            "Epoch[0/2] Step[2887] Loss=0.3686 GradNorm=3.6097 StepSize=0.0361 RelImp=83.95%\n",
            "Epoch[0/2] Step[2888] Loss=0.3298 GradNorm=3.1817 StepSize=0.0318 RelImp=85.64%\n",
            "Epoch[0/2] Step[2889] Loss=0.4711 GradNorm=3.2314 StepSize=0.0323 RelImp=79.49%\n",
            "Epoch[0/2] Step[2890] Loss=0.1924 GradNorm=2.6191 StepSize=0.0262 RelImp=91.62%\n",
            "Epoch[0/2] Step[2891] Loss=0.2478 GradNorm=2.7495 StepSize=0.0275 RelImp=89.21%\n",
            "Epoch[0/2] Step[2892] Loss=0.7423 GradNorm=4.5126 StepSize=0.0451 RelImp=67.68%\n",
            "Epoch[0/2] Step[2893] Loss=0.3652 GradNorm=2.6744 StepSize=0.0267 RelImp=84.10%\n",
            "Epoch[0/2] Step[2894] Loss=0.1644 GradNorm=2.0264 StepSize=0.0203 RelImp=92.84%\n",
            "Epoch[0/2] Step[2895] Loss=0.8629 GradNorm=5.1176 StepSize=0.0512 RelImp=62.43%\n",
            "Epoch[0/2] Step[2896] Loss=0.1325 GradNorm=1.4706 StepSize=0.0147 RelImp=94.23%\n",
            "Epoch[0/2] Step[2897] Loss=0.3486 GradNorm=4.4589 StepSize=0.0446 RelImp=84.82%\n",
            "Epoch[0/2] Step[2898] Loss=0.8102 GradNorm=5.4761 StepSize=0.0548 RelImp=64.72%\n",
            "Epoch[0/2] Step[2899] Loss=0.7939 GradNorm=5.0061 StepSize=0.0501 RelImp=65.43%\n",
            "Epoch[0/2] Step[2900] Loss=0.1170 GradNorm=1.9218 StepSize=0.0192 RelImp=94.90%\n",
            "Epoch[0/2] Step[2901] Loss=0.1421 GradNorm=1.3712 StepSize=0.0137 RelImp=93.81%\n",
            "Epoch[0/2] Step[2902] Loss=0.2152 GradNorm=1.8374 StepSize=0.0184 RelImp=90.63%\n",
            "Epoch[0/2] Step[2903] Loss=0.1703 GradNorm=2.3483 StepSize=0.0235 RelImp=92.58%\n",
            "Epoch[0/2] Step[2904] Loss=0.1492 GradNorm=2.6577 StepSize=0.0266 RelImp=93.50%\n",
            "Epoch[0/2] Step[2905] Loss=0.2811 GradNorm=2.0450 StepSize=0.0204 RelImp=87.76%\n",
            "Epoch[0/2] Step[2906] Loss=0.2022 GradNorm=2.2044 StepSize=0.0220 RelImp=91.19%\n",
            "Epoch[0/2] Step[2907] Loss=0.1167 GradNorm=1.8170 StepSize=0.0182 RelImp=94.92%\n",
            "Epoch[0/2] Step[2908] Loss=0.2424 GradNorm=2.3709 StepSize=0.0237 RelImp=89.45%\n",
            "Epoch[0/2] Step[2909] Loss=0.4557 GradNorm=3.7800 StepSize=0.0378 RelImp=80.16%\n",
            "Epoch[0/2] Step[2910] Loss=0.4444 GradNorm=4.7542 StepSize=0.0475 RelImp=80.65%\n",
            "Epoch[0/2] Step[2911] Loss=0.1212 GradNorm=1.5666 StepSize=0.0157 RelImp=94.72%\n",
            "Epoch[0/2] Step[2912] Loss=0.0630 GradNorm=0.9194 StepSize=0.0092 RelImp=97.26%\n",
            "Epoch[0/2] Step[2913] Loss=0.3760 GradNorm=3.7183 StepSize=0.0372 RelImp=83.63%\n",
            "Epoch[0/2] Step[2914] Loss=0.4347 GradNorm=4.2380 StepSize=0.0424 RelImp=81.07%\n",
            "Epoch[0/2] Step[2915] Loss=0.4552 GradNorm=3.1465 StepSize=0.0315 RelImp=80.18%\n",
            "Epoch[0/2] Step[2916] Loss=0.0999 GradNorm=1.2740 StepSize=0.0127 RelImp=95.65%\n",
            "Epoch[0/2] Step[2917] Loss=0.4421 GradNorm=3.2668 StepSize=0.0327 RelImp=80.75%\n",
            "Epoch[0/2] Step[2918] Loss=0.2759 GradNorm=2.1805 StepSize=0.0218 RelImp=87.98%\n",
            "Epoch[0/2] Step[2919] Loss=0.6107 GradNorm=2.3969 StepSize=0.0240 RelImp=73.41%\n",
            "Epoch[0/2] Step[2920] Loss=0.1900 GradNorm=1.9485 StepSize=0.0195 RelImp=91.73%\n",
            "Epoch[0/2] Step[2921] Loss=0.1520 GradNorm=1.6752 StepSize=0.0168 RelImp=93.38%\n",
            "Epoch[0/2] Step[2922] Loss=0.4027 GradNorm=3.7402 StepSize=0.0374 RelImp=82.47%\n",
            "Epoch[0/2] Step[2923] Loss=0.1720 GradNorm=2.1479 StepSize=0.0215 RelImp=92.51%\n",
            "Epoch[0/2] Step[2924] Loss=0.4024 GradNorm=2.9987 StepSize=0.0300 RelImp=82.48%\n",
            "Epoch[0/2] Step[2925] Loss=0.3624 GradNorm=2.8702 StepSize=0.0287 RelImp=84.22%\n",
            "Epoch[0/2] Step[2926] Loss=0.2976 GradNorm=2.5363 StepSize=0.0254 RelImp=87.04%\n",
            "Epoch[0/2] Step[2927] Loss=0.2871 GradNorm=3.5886 StepSize=0.0359 RelImp=87.50%\n",
            "Epoch[0/2] Step[2928] Loss=0.1829 GradNorm=2.3526 StepSize=0.0235 RelImp=92.04%\n",
            "Epoch[0/2] Step[2929] Loss=0.3920 GradNorm=1.8277 StepSize=0.0183 RelImp=82.93%\n",
            "Epoch[0/2] Step[2930] Loss=0.2724 GradNorm=2.3889 StepSize=0.0239 RelImp=88.14%\n",
            "Epoch[0/2] Step[2931] Loss=0.2250 GradNorm=1.9701 StepSize=0.0197 RelImp=90.20%\n",
            "Epoch[0/2] Step[2932] Loss=0.4767 GradNorm=4.0745 StepSize=0.0407 RelImp=79.24%\n",
            "Epoch[0/2] Step[2933] Loss=0.2169 GradNorm=3.1611 StepSize=0.0316 RelImp=90.56%\n",
            "Epoch[0/2] Step[2934] Loss=0.6451 GradNorm=5.2986 StepSize=0.0530 RelImp=71.91%\n",
            "Epoch[0/2] Step[2935] Loss=0.3371 GradNorm=4.1352 StepSize=0.0414 RelImp=85.32%\n",
            "Epoch[0/2] Step[2936] Loss=0.3242 GradNorm=2.8353 StepSize=0.0284 RelImp=85.88%\n",
            "Epoch[0/2] Step[2937] Loss=0.6604 GradNorm=3.4490 StepSize=0.0345 RelImp=71.24%\n",
            "Epoch[0/2] Step[2938] Loss=0.6650 GradNorm=4.2471 StepSize=0.0425 RelImp=71.05%\n",
            "Epoch[0/2] Step[2939] Loss=0.1691 GradNorm=1.7931 StepSize=0.0179 RelImp=92.64%\n",
            "Epoch[0/2] Step[2940] Loss=0.2789 GradNorm=3.0407 StepSize=0.0304 RelImp=87.86%\n",
            "Epoch[0/2] Step[2941] Loss=0.3888 GradNorm=3.7980 StepSize=0.0380 RelImp=83.07%\n",
            "Epoch[0/2] Step[2942] Loss=0.2244 GradNorm=1.8453 StepSize=0.0185 RelImp=90.23%\n",
            "Epoch[0/2] Step[2943] Loss=0.6326 GradNorm=4.7651 StepSize=0.0477 RelImp=72.45%\n",
            "Epoch[0/2] Step[2944] Loss=0.2055 GradNorm=2.2419 StepSize=0.0224 RelImp=91.05%\n",
            "Epoch[0/2] Step[2945] Loss=0.1919 GradNorm=2.8305 StepSize=0.0283 RelImp=91.64%\n",
            "Epoch[0/2] Step[2946] Loss=0.2552 GradNorm=2.7625 StepSize=0.0276 RelImp=88.89%\n",
            "Epoch[0/2] Step[2947] Loss=0.4032 GradNorm=3.1228 StepSize=0.0312 RelImp=82.44%\n",
            "Epoch[0/2] Step[2948] Loss=0.3596 GradNorm=3.7182 StepSize=0.0372 RelImp=84.34%\n",
            "Epoch[0/2] Step[2949] Loss=0.4223 GradNorm=3.5411 StepSize=0.0354 RelImp=81.61%\n",
            "Epoch[0/2] Step[2950] Loss=0.3686 GradNorm=2.8705 StepSize=0.0287 RelImp=83.95%\n",
            "Epoch[0/2] Step[2951] Loss=0.7034 GradNorm=5.2114 StepSize=0.0521 RelImp=69.37%\n",
            "Epoch[0/2] Step[2952] Loss=0.1693 GradNorm=2.2109 StepSize=0.0221 RelImp=92.63%\n",
            "Epoch[0/2] Step[2953] Loss=0.3874 GradNorm=4.1070 StepSize=0.0411 RelImp=83.13%\n",
            "Epoch[0/2] Step[2954] Loss=0.2570 GradNorm=2.6064 StepSize=0.0261 RelImp=88.81%\n",
            "Epoch[0/2] Step[2955] Loss=0.4443 GradNorm=3.9173 StepSize=0.0392 RelImp=80.65%\n",
            "Epoch[0/2] Step[2956] Loss=0.2455 GradNorm=3.8050 StepSize=0.0381 RelImp=89.31%\n",
            "Epoch[0/2] Step[2957] Loss=0.3361 GradNorm=3.0580 StepSize=0.0306 RelImp=85.37%\n",
            "Epoch[0/2] Step[2958] Loss=0.2482 GradNorm=1.9224 StepSize=0.0192 RelImp=89.19%\n",
            "Epoch[0/2] Step[2959] Loss=0.5696 GradNorm=3.4985 StepSize=0.0350 RelImp=75.20%\n",
            "Epoch[0/2] Step[2960] Loss=0.1499 GradNorm=1.3086 StepSize=0.0131 RelImp=93.48%\n",
            "Epoch[0/2] Step[2961] Loss=0.5539 GradNorm=3.8239 StepSize=0.0382 RelImp=75.88%\n",
            "Epoch[0/2] Step[2962] Loss=0.6724 GradNorm=5.3529 StepSize=0.0535 RelImp=70.72%\n",
            "Epoch[0/2] Step[2963] Loss=0.2392 GradNorm=2.5265 StepSize=0.0253 RelImp=89.59%\n",
            "Epoch[0/2] Step[2964] Loss=0.1828 GradNorm=2.4863 StepSize=0.0249 RelImp=92.04%\n",
            "Epoch[0/2] Step[2965] Loss=0.3311 GradNorm=3.5989 StepSize=0.0360 RelImp=85.58%\n",
            "Epoch[0/2] Step[2966] Loss=0.3215 GradNorm=4.7461 StepSize=0.0475 RelImp=86.00%\n",
            "Epoch[0/2] Step[2967] Loss=0.2516 GradNorm=3.0173 StepSize=0.0302 RelImp=89.05%\n",
            "Epoch[0/2] Step[2968] Loss=0.3527 GradNorm=4.4099 StepSize=0.0441 RelImp=84.64%\n",
            "Epoch[0/2] Step[2969] Loss=0.1702 GradNorm=1.8523 StepSize=0.0185 RelImp=92.59%\n",
            "Epoch[0/2] Step[2970] Loss=0.2286 GradNorm=1.8851 StepSize=0.0189 RelImp=90.05%\n",
            "Epoch[0/2] Step[2971] Loss=0.5306 GradNorm=4.4570 StepSize=0.0446 RelImp=76.90%\n",
            "Epoch[0/2] Step[2972] Loss=0.2264 GradNorm=1.8165 StepSize=0.0182 RelImp=90.14%\n",
            "Epoch[0/2] Step[2973] Loss=0.5756 GradNorm=3.1714 StepSize=0.0317 RelImp=74.94%\n",
            "Epoch[0/2] Step[2974] Loss=0.2133 GradNorm=2.2358 StepSize=0.0224 RelImp=90.71%\n",
            "Epoch[0/2] Step[2975] Loss=0.5199 GradNorm=4.4654 StepSize=0.0447 RelImp=77.36%\n",
            "Epoch[0/2] Step[2976] Loss=0.5265 GradNorm=3.8689 StepSize=0.0387 RelImp=77.08%\n",
            "Epoch[0/2] Step[2977] Loss=0.2038 GradNorm=2.5104 StepSize=0.0251 RelImp=91.13%\n",
            "Epoch[0/2] Step[2978] Loss=0.3092 GradNorm=2.6480 StepSize=0.0265 RelImp=86.54%\n",
            "Epoch[0/2] Step[2979] Loss=0.1088 GradNorm=1.2970 StepSize=0.0130 RelImp=95.26%\n",
            "Epoch[0/2] Step[2980] Loss=0.2125 GradNorm=3.6761 StepSize=0.0368 RelImp=90.75%\n",
            "Epoch[0/2] Step[2981] Loss=0.7808 GradNorm=5.9886 StepSize=0.0599 RelImp=66.00%\n",
            "Epoch[0/2] Step[2982] Loss=0.2527 GradNorm=2.1823 StepSize=0.0218 RelImp=89.00%\n",
            "Epoch[0/2] Step[2983] Loss=1.0850 GradNorm=5.3526 StepSize=0.0535 RelImp=52.76%\n",
            "Epoch[0/2] Step[2984] Loss=0.4014 GradNorm=2.9826 StepSize=0.0298 RelImp=82.52%\n",
            "Epoch[0/2] Step[2985] Loss=0.2237 GradNorm=2.8854 StepSize=0.0289 RelImp=90.26%\n",
            "Epoch[0/2] Step[2986] Loss=0.2409 GradNorm=2.4627 StepSize=0.0246 RelImp=89.51%\n",
            "Epoch[0/2] Step[2987] Loss=0.3695 GradNorm=3.5406 StepSize=0.0354 RelImp=83.91%\n",
            "Epoch[0/2] Step[2988] Loss=0.7674 GradNorm=7.0391 StepSize=0.0704 RelImp=66.59%\n",
            "Epoch[0/2] Step[2989] Loss=0.4792 GradNorm=3.6440 StepSize=0.0364 RelImp=79.13%\n",
            "Epoch[0/2] Step[2990] Loss=1.1737 GradNorm=7.4172 StepSize=0.0742 RelImp=48.90%\n",
            "Epoch[0/2] Step[2991] Loss=0.7046 GradNorm=2.2008 StepSize=0.0220 RelImp=69.32%\n",
            "Epoch[0/2] Step[2992] Loss=0.3423 GradNorm=5.4082 StepSize=0.0541 RelImp=85.09%\n",
            "Epoch[0/2] Step[2993] Loss=0.4770 GradNorm=3.6170 StepSize=0.0362 RelImp=79.23%\n",
            "Epoch[0/2] Step[2994] Loss=0.4902 GradNorm=3.1381 StepSize=0.0314 RelImp=78.66%\n",
            "Epoch[0/2] Step[2995] Loss=0.2347 GradNorm=3.2077 StepSize=0.0321 RelImp=89.78%\n",
            "Epoch[0/2] Step[2996] Loss=0.5460 GradNorm=5.0670 StepSize=0.0507 RelImp=76.22%\n",
            "Epoch[0/2] Step[2997] Loss=0.2322 GradNorm=2.7864 StepSize=0.0279 RelImp=89.89%\n",
            "Epoch[0/2] Step[2998] Loss=0.4022 GradNorm=4.0115 StepSize=0.0401 RelImp=82.49%\n",
            "Epoch[0/2] Step[2999] Loss=0.1193 GradNorm=0.9975 StepSize=0.0100 RelImp=94.80%\n",
            "Epoch[0/2] Step[3000] Loss=0.4655 GradNorm=3.6773 StepSize=0.0368 RelImp=79.73%\n",
            "Epoch[0/2] Step[3001] Loss=0.6701 GradNorm=5.9294 StepSize=0.0593 RelImp=70.82%\n",
            "Epoch[0/2] Step[3002] Loss=0.3488 GradNorm=3.3305 StepSize=0.0333 RelImp=84.81%\n",
            "Epoch[0/2] Step[3003] Loss=0.1173 GradNorm=1.9320 StepSize=0.0193 RelImp=94.89%\n",
            "Epoch[0/2] Step[3004] Loss=0.2520 GradNorm=2.7973 StepSize=0.0280 RelImp=89.03%\n",
            "Epoch[0/2] Step[3005] Loss=0.4581 GradNorm=4.3076 StepSize=0.0431 RelImp=80.05%\n",
            "Epoch[0/2] Step[3006] Loss=0.6599 GradNorm=4.7607 StepSize=0.0476 RelImp=71.27%\n",
            "Epoch[0/2] Step[3007] Loss=0.6154 GradNorm=2.3904 StepSize=0.0239 RelImp=73.20%\n",
            "Epoch[0/2] Step[3008] Loss=0.1491 GradNorm=1.4869 StepSize=0.0149 RelImp=93.51%\n",
            "Epoch[0/2] Step[3009] Loss=0.3438 GradNorm=3.6007 StepSize=0.0360 RelImp=85.03%\n",
            "Epoch[0/2] Step[3010] Loss=0.4327 GradNorm=3.5743 StepSize=0.0357 RelImp=81.16%\n",
            "Epoch[0/2] Step[3011] Loss=0.2597 GradNorm=3.6577 StepSize=0.0366 RelImp=88.69%\n",
            "Epoch[0/2] Step[3012] Loss=0.5195 GradNorm=6.5610 StepSize=0.0656 RelImp=77.38%\n",
            "Epoch[0/2] Step[3013] Loss=0.7624 GradNorm=7.1052 StepSize=0.0711 RelImp=66.80%\n",
            "Epoch[0/2] Step[3014] Loss=0.1191 GradNorm=0.6672 StepSize=0.0067 RelImp=94.81%\n",
            "Epoch[0/2] Step[3015] Loss=0.8137 GradNorm=5.0073 StepSize=0.0501 RelImp=64.57%\n",
            "Epoch[0/2] Step[3016] Loss=0.2093 GradNorm=2.2187 StepSize=0.0222 RelImp=90.89%\n",
            "Epoch[0/2] Step[3017] Loss=0.3300 GradNorm=2.2993 StepSize=0.0230 RelImp=85.63%\n",
            "Epoch[0/2] Step[3018] Loss=1.0663 GradNorm=5.1692 StepSize=0.0517 RelImp=53.57%\n",
            "Epoch[0/2] Step[3019] Loss=0.8299 GradNorm=5.3752 StepSize=0.0538 RelImp=63.86%\n",
            "Epoch[0/2] Step[3020] Loss=0.5243 GradNorm=2.2329 StepSize=0.0223 RelImp=77.17%\n",
            "Epoch[0/2] Step[3021] Loss=0.2146 GradNorm=2.6256 StepSize=0.0263 RelImp=90.66%\n",
            "Epoch[0/2] Step[3022] Loss=1.2399 GradNorm=6.6757 StepSize=0.0668 RelImp=46.01%\n",
            "Epoch[0/2] Step[3023] Loss=0.5849 GradNorm=3.4784 StepSize=0.0348 RelImp=74.53%\n",
            "Epoch[0/2] Step[3024] Loss=0.5289 GradNorm=4.1852 StepSize=0.0419 RelImp=76.97%\n",
            "Epoch[0/2] Step[3025] Loss=0.3824 GradNorm=3.6697 StepSize=0.0367 RelImp=83.35%\n",
            "Epoch[0/2] Step[3026] Loss=0.0710 GradNorm=0.6748 StepSize=0.0067 RelImp=96.91%\n",
            "Epoch[0/2] Step[3027] Loss=0.6660 GradNorm=3.9637 StepSize=0.0396 RelImp=71.00%\n",
            "Epoch[0/2] Step[3028] Loss=0.1706 GradNorm=1.3813 StepSize=0.0138 RelImp=92.57%\n",
            "Epoch[0/2] Step[3029] Loss=0.6046 GradNorm=4.4591 StepSize=0.0446 RelImp=73.68%\n",
            "Epoch[0/2] Step[3030] Loss=0.3621 GradNorm=2.5844 StepSize=0.0258 RelImp=84.23%\n",
            "Epoch[0/2] Step[3031] Loss=0.2866 GradNorm=2.7560 StepSize=0.0276 RelImp=87.52%\n",
            "Epoch[0/2] Step[3032] Loss=0.2943 GradNorm=2.6482 StepSize=0.0265 RelImp=87.19%\n",
            "Epoch[0/2] Step[3033] Loss=0.4169 GradNorm=2.8046 StepSize=0.0280 RelImp=81.85%\n",
            "Epoch[0/2] Step[3034] Loss=0.5347 GradNorm=5.2516 StepSize=0.0525 RelImp=76.72%\n",
            "Epoch[0/2] Step[3035] Loss=0.5208 GradNorm=4.3319 StepSize=0.0433 RelImp=77.32%\n",
            "Epoch[0/2] Step[3036] Loss=0.2525 GradNorm=2.7800 StepSize=0.0278 RelImp=89.01%\n",
            "Epoch[0/2] Step[3037] Loss=0.3854 GradNorm=3.4733 StepSize=0.0347 RelImp=83.22%\n",
            "Epoch[0/2] Step[3038] Loss=0.3845 GradNorm=4.1038 StepSize=0.0410 RelImp=83.26%\n",
            "Epoch[0/2] Step[3039] Loss=0.1939 GradNorm=2.3007 StepSize=0.0230 RelImp=91.56%\n",
            "Epoch[0/2] Step[3040] Loss=0.0948 GradNorm=1.2581 StepSize=0.0126 RelImp=95.87%\n",
            "Epoch[0/2] Step[3041] Loss=0.1247 GradNorm=1.6875 StepSize=0.0169 RelImp=94.57%\n",
            "Epoch[0/2] Step[3042] Loss=0.2631 GradNorm=2.8097 StepSize=0.0281 RelImp=88.54%\n",
            "Epoch[0/2] Step[3043] Loss=0.3601 GradNorm=3.2514 StepSize=0.0325 RelImp=84.32%\n",
            "Epoch[0/2] Step[3044] Loss=0.4772 GradNorm=5.6113 StepSize=0.0561 RelImp=79.22%\n",
            "Epoch[0/2] Step[3045] Loss=0.5893 GradNorm=3.0221 StepSize=0.0302 RelImp=74.34%\n",
            "Epoch[0/2] Step[3046] Loss=0.9151 GradNorm=4.5611 StepSize=0.0456 RelImp=60.16%\n",
            "Epoch[0/2] Step[3047] Loss=0.6932 GradNorm=4.5307 StepSize=0.0453 RelImp=69.82%\n",
            "Epoch[0/2] Step[3048] Loss=0.2158 GradNorm=2.7618 StepSize=0.0276 RelImp=90.60%\n",
            "Epoch[0/2] Step[3049] Loss=0.0986 GradNorm=1.0022 StepSize=0.0100 RelImp=95.71%\n",
            "Epoch[0/2] Step[3050] Loss=0.3822 GradNorm=3.2496 StepSize=0.0325 RelImp=83.36%\n",
            "Epoch[0/2] Step[3051] Loss=0.2881 GradNorm=3.8320 StepSize=0.0383 RelImp=87.46%\n",
            "Epoch[0/2] Step[3052] Loss=0.2890 GradNorm=2.6201 StepSize=0.0262 RelImp=87.42%\n",
            "Epoch[0/2] Step[3053] Loss=0.3472 GradNorm=4.1946 StepSize=0.0419 RelImp=84.88%\n",
            "Epoch[0/2] Step[3054] Loss=0.3537 GradNorm=3.4346 StepSize=0.0343 RelImp=84.60%\n",
            "Epoch[0/2] Step[3055] Loss=0.9725 GradNorm=7.2527 StepSize=0.0725 RelImp=57.65%\n",
            "Epoch[0/2] Step[3056] Loss=0.1895 GradNorm=2.3633 StepSize=0.0236 RelImp=91.75%\n",
            "Epoch[0/2] Step[3057] Loss=0.6173 GradNorm=4.9565 StepSize=0.0496 RelImp=73.12%\n",
            "Epoch[0/2] Step[3058] Loss=0.1152 GradNorm=0.9855 StepSize=0.0099 RelImp=94.98%\n",
            "Epoch[0/2] Step[3059] Loss=0.3897 GradNorm=3.9149 StepSize=0.0391 RelImp=83.03%\n",
            "Epoch[0/2] Step[3060] Loss=0.6430 GradNorm=3.1808 StepSize=0.0318 RelImp=72.00%\n",
            "Epoch[0/2] Step[3061] Loss=0.4665 GradNorm=4.2273 StepSize=0.0423 RelImp=79.69%\n",
            "Epoch[0/2] Step[3062] Loss=0.1741 GradNorm=2.0432 StepSize=0.0204 RelImp=92.42%\n",
            "Epoch[0/2] Step[3063] Loss=0.6051 GradNorm=4.2014 StepSize=0.0420 RelImp=73.65%\n",
            "Epoch[0/2] Step[3064] Loss=0.3815 GradNorm=4.8672 StepSize=0.0487 RelImp=83.39%\n",
            "Epoch[0/2] Step[3065] Loss=0.2182 GradNorm=3.2002 StepSize=0.0320 RelImp=90.50%\n",
            "Epoch[0/2] Step[3066] Loss=0.3537 GradNorm=2.5392 StepSize=0.0254 RelImp=84.60%\n",
            "Epoch[0/2] Step[3067] Loss=0.3483 GradNorm=1.8598 StepSize=0.0186 RelImp=84.84%\n",
            "Epoch[0/2] Step[3068] Loss=0.4114 GradNorm=2.6514 StepSize=0.0265 RelImp=82.09%\n",
            "Epoch[0/2] Step[3069] Loss=0.3164 GradNorm=3.1015 StepSize=0.0310 RelImp=86.22%\n",
            "Epoch[0/2] Step[3070] Loss=0.2097 GradNorm=2.3408 StepSize=0.0234 RelImp=90.87%\n",
            "Epoch[0/2] Step[3071] Loss=0.2402 GradNorm=3.3376 StepSize=0.0334 RelImp=89.54%\n",
            "Epoch[0/2] Step[3072] Loss=0.1267 GradNorm=1.2777 StepSize=0.0128 RelImp=94.48%\n",
            "Epoch[0/2] Step[3073] Loss=0.1493 GradNorm=1.5074 StepSize=0.0151 RelImp=93.50%\n",
            "Epoch[0/2] Step[3074] Loss=0.3128 GradNorm=2.9855 StepSize=0.0299 RelImp=86.38%\n",
            "Epoch[0/2] Step[3075] Loss=0.4874 GradNorm=4.1327 StepSize=0.0413 RelImp=78.78%\n",
            "Epoch[0/2] Step[3076] Loss=0.1328 GradNorm=0.7941 StepSize=0.0079 RelImp=94.22%\n",
            "Epoch[0/2] Step[3077] Loss=0.3656 GradNorm=2.7880 StepSize=0.0279 RelImp=84.08%\n",
            "Epoch[0/2] Step[3078] Loss=0.5409 GradNorm=3.2268 StepSize=0.0323 RelImp=76.45%\n",
            "Epoch[0/2] Step[3079] Loss=0.4975 GradNorm=4.1948 StepSize=0.0419 RelImp=78.34%\n",
            "Epoch[0/2] Step[3080] Loss=0.2843 GradNorm=2.6208 StepSize=0.0262 RelImp=87.62%\n",
            "Epoch[0/2] Step[3081] Loss=0.6551 GradNorm=5.2307 StepSize=0.0523 RelImp=71.47%\n",
            "Epoch[0/2] Step[3082] Loss=0.3030 GradNorm=3.7971 StepSize=0.0380 RelImp=86.81%\n",
            "Epoch[0/2] Step[3083] Loss=0.0902 GradNorm=1.3764 StepSize=0.0138 RelImp=96.07%\n",
            "Epoch[0/2] Step[3084] Loss=0.1327 GradNorm=1.3076 StepSize=0.0131 RelImp=94.22%\n",
            "Epoch[0/2] Step[3085] Loss=0.1497 GradNorm=2.2138 StepSize=0.0221 RelImp=93.48%\n",
            "Epoch[0/2] Step[3086] Loss=0.2267 GradNorm=1.6442 StepSize=0.0164 RelImp=90.13%\n",
            "Epoch[0/2] Step[3087] Loss=0.6487 GradNorm=4.3469 StepSize=0.0435 RelImp=71.75%\n",
            "Epoch[0/2] Step[3088] Loss=0.1018 GradNorm=1.9375 StepSize=0.0194 RelImp=95.57%\n",
            "Epoch[0/2] Step[3089] Loss=0.2870 GradNorm=2.9348 StepSize=0.0293 RelImp=87.50%\n",
            "Epoch[0/2] Step[3090] Loss=0.3168 GradNorm=4.2484 StepSize=0.0425 RelImp=86.21%\n",
            "Epoch[0/2] Step[3091] Loss=0.2735 GradNorm=3.6262 StepSize=0.0363 RelImp=88.09%\n",
            "Epoch[0/2] Step[3092] Loss=0.1336 GradNorm=1.7252 StepSize=0.0173 RelImp=94.18%\n",
            "Epoch[0/2] Step[3093] Loss=0.5546 GradNorm=4.1553 StepSize=0.0416 RelImp=75.85%\n",
            "Epoch[0/2] Step[3094] Loss=0.3964 GradNorm=4.1392 StepSize=0.0414 RelImp=82.74%\n",
            "Epoch[0/2] Step[3095] Loss=0.0781 GradNorm=1.2385 StepSize=0.0124 RelImp=96.60%\n",
            "Epoch[0/2] Step[3096] Loss=0.2508 GradNorm=1.6630 StepSize=0.0166 RelImp=89.08%\n",
            "Epoch[0/2] Step[3097] Loss=0.9100 GradNorm=4.9489 StepSize=0.0495 RelImp=60.37%\n",
            "Epoch[0/2] Step[3098] Loss=0.0769 GradNorm=1.1919 StepSize=0.0119 RelImp=96.65%\n",
            "Epoch[0/2] Step[3099] Loss=0.8751 GradNorm=6.7910 StepSize=0.0679 RelImp=61.90%\n",
            "Epoch[0/2] Step[3100] Loss=0.1952 GradNorm=2.6350 StepSize=0.0264 RelImp=91.50%\n",
            "Epoch[0/2] Step[3101] Loss=0.5251 GradNorm=3.1294 StepSize=0.0313 RelImp=77.13%\n",
            "Epoch[0/2] Step[3102] Loss=0.2553 GradNorm=3.3060 StepSize=0.0331 RelImp=88.88%\n",
            "Epoch[0/2] Step[3103] Loss=0.3828 GradNorm=2.8318 StepSize=0.0283 RelImp=83.33%\n",
            "Epoch[0/2] Step[3104] Loss=0.3247 GradNorm=3.3906 StepSize=0.0339 RelImp=85.86%\n",
            "Epoch[0/2] Step[3105] Loss=0.2012 GradNorm=2.3447 StepSize=0.0234 RelImp=91.24%\n",
            "Epoch[0/2] Step[3106] Loss=0.3715 GradNorm=3.0117 StepSize=0.0301 RelImp=83.82%\n",
            "Epoch[0/2] Step[3107] Loss=0.7370 GradNorm=4.7197 StepSize=0.0472 RelImp=67.91%\n",
            "Epoch[0/2] Step[3108] Loss=0.0757 GradNorm=1.0905 StepSize=0.0109 RelImp=96.70%\n",
            "Epoch[0/2] Step[3109] Loss=0.4402 GradNorm=3.0663 StepSize=0.0307 RelImp=80.83%\n",
            "Epoch[0/2] Step[3110] Loss=0.5708 GradNorm=4.2626 StepSize=0.0426 RelImp=75.14%\n",
            "Epoch[0/2] Step[3111] Loss=0.2344 GradNorm=2.7739 StepSize=0.0277 RelImp=89.79%\n",
            "Epoch[0/2] Step[3112] Loss=0.4225 GradNorm=2.7715 StepSize=0.0277 RelImp=81.60%\n",
            "Epoch[0/2] Step[3113] Loss=0.1812 GradNorm=2.2348 StepSize=0.0223 RelImp=92.11%\n",
            "Epoch[0/2] Step[3114] Loss=0.1269 GradNorm=1.8342 StepSize=0.0183 RelImp=94.47%\n",
            "Epoch[0/2] Step[3115] Loss=0.4543 GradNorm=3.2674 StepSize=0.0327 RelImp=80.22%\n",
            "Epoch[0/2] Step[3116] Loss=0.7993 GradNorm=4.7524 StepSize=0.0475 RelImp=65.19%\n",
            "Epoch[0/2] Step[3117] Loss=0.4430 GradNorm=3.9303 StepSize=0.0393 RelImp=80.71%\n",
            "Epoch[0/2] Step[3118] Loss=0.2678 GradNorm=1.6900 StepSize=0.0169 RelImp=88.34%\n",
            "Epoch[0/2] Step[3119] Loss=0.1798 GradNorm=1.5926 StepSize=0.0159 RelImp=92.17%\n",
            "Epoch[0/2] Step[3120] Loss=0.4539 GradNorm=3.2734 StepSize=0.0327 RelImp=80.24%\n",
            "Epoch[0/2] Step[3121] Loss=0.2840 GradNorm=2.2055 StepSize=0.0221 RelImp=87.63%\n",
            "Epoch[0/2] Step[3122] Loss=0.3068 GradNorm=3.2568 StepSize=0.0326 RelImp=86.64%\n",
            "Epoch[0/2] Step[3123] Loss=0.1662 GradNorm=2.1863 StepSize=0.0219 RelImp=92.76%\n",
            "Epoch[0/2] Step[3124] Loss=0.1857 GradNorm=1.4781 StepSize=0.0148 RelImp=91.91%\n",
            "Epoch[0/2] Step[3125] Loss=0.4938 GradNorm=2.6340 StepSize=0.0263 RelImp=78.50%\n",
            "Epoch[0/2] Step[3126] Loss=0.4434 GradNorm=4.3971 StepSize=0.0440 RelImp=80.69%\n",
            "Epoch[0/2] Step[3127] Loss=0.4589 GradNorm=3.0218 StepSize=0.0302 RelImp=80.02%\n",
            "Epoch[0/2] Step[3128] Loss=0.4335 GradNorm=2.2010 StepSize=0.0220 RelImp=81.12%\n",
            "Epoch[0/2] Step[3129] Loss=0.4017 GradNorm=3.5038 StepSize=0.0350 RelImp=82.51%\n",
            "Epoch[0/2] Step[3130] Loss=0.4205 GradNorm=3.6559 StepSize=0.0366 RelImp=81.69%\n",
            "Epoch[0/2] Step[3131] Loss=0.1000 GradNorm=1.0807 StepSize=0.0108 RelImp=95.65%\n",
            "Epoch[0/2] Step[3132] Loss=0.5213 GradNorm=4.0540 StepSize=0.0405 RelImp=77.30%\n",
            "Epoch[0/2] Step[3133] Loss=0.4099 GradNorm=2.9438 StepSize=0.0294 RelImp=82.15%\n",
            "Epoch[0/2] Step[3134] Loss=0.5029 GradNorm=4.0749 StepSize=0.0407 RelImp=78.10%\n",
            "Epoch[0/2] Step[3135] Loss=0.4298 GradNorm=5.3674 StepSize=0.0537 RelImp=81.28%\n",
            "Epoch[0/2] Step[3136] Loss=0.5129 GradNorm=3.1606 StepSize=0.0316 RelImp=77.67%\n",
            "Epoch[0/2] Step[3137] Loss=0.1475 GradNorm=1.6301 StepSize=0.0163 RelImp=93.58%\n",
            "Epoch[0/2] Step[3138] Loss=0.3653 GradNorm=3.3509 StepSize=0.0335 RelImp=84.09%\n",
            "Epoch[0/2] Step[3139] Loss=0.7076 GradNorm=3.5287 StepSize=0.0353 RelImp=69.19%\n",
            "Epoch[0/2] Step[3140] Loss=0.5675 GradNorm=2.7883 StepSize=0.0279 RelImp=75.29%\n",
            "Epoch[0/2] Step[3141] Loss=0.3404 GradNorm=3.0866 StepSize=0.0309 RelImp=85.18%\n",
            "Epoch[0/2] Step[3142] Loss=0.5243 GradNorm=3.9711 StepSize=0.0397 RelImp=77.17%\n",
            "Epoch[0/2] Step[3143] Loss=0.4832 GradNorm=4.4692 StepSize=0.0447 RelImp=78.96%\n",
            "Epoch[0/2] Step[3144] Loss=0.1702 GradNorm=1.6015 StepSize=0.0160 RelImp=92.59%\n",
            "Epoch[0/2] Step[3145] Loss=0.2496 GradNorm=2.4448 StepSize=0.0244 RelImp=89.13%\n",
            "Epoch[0/2] Step[3146] Loss=1.1935 GradNorm=5.3784 StepSize=0.0538 RelImp=48.03%\n",
            "Epoch[0/2] Step[3147] Loss=0.3925 GradNorm=2.4267 StepSize=0.0243 RelImp=82.91%\n",
            "Epoch[0/2] Step[3148] Loss=0.9135 GradNorm=5.0099 StepSize=0.0501 RelImp=60.22%\n",
            "Epoch[0/2] Step[3149] Loss=0.1992 GradNorm=2.5286 StepSize=0.0253 RelImp=91.33%\n",
            "Epoch[0/2] Step[3150] Loss=0.4534 GradNorm=3.5895 StepSize=0.0359 RelImp=80.26%\n",
            "Epoch[0/2] Step[3151] Loss=0.1764 GradNorm=1.6776 StepSize=0.0168 RelImp=92.32%\n",
            "Epoch[0/2] Step[3152] Loss=0.3831 GradNorm=3.6157 StepSize=0.0362 RelImp=83.32%\n",
            "Epoch[0/2] Step[3153] Loss=0.6788 GradNorm=4.3225 StepSize=0.0432 RelImp=70.44%\n",
            "Epoch[0/2] Step[3154] Loss=0.4527 GradNorm=3.0268 StepSize=0.0303 RelImp=80.29%\n",
            "Epoch[0/2] Step[3155] Loss=0.6657 GradNorm=5.3732 StepSize=0.0537 RelImp=71.01%\n",
            "Epoch[0/2] Step[3156] Loss=0.3660 GradNorm=3.5084 StepSize=0.0351 RelImp=84.06%\n",
            "Epoch[0/2] Step[3157] Loss=0.4193 GradNorm=4.5988 StepSize=0.0460 RelImp=81.74%\n",
            "Epoch[0/2] Step[3158] Loss=0.3503 GradNorm=4.1955 StepSize=0.0420 RelImp=84.75%\n",
            "Epoch[0/2] Step[3159] Loss=0.4893 GradNorm=5.1510 StepSize=0.0515 RelImp=78.69%\n",
            "Epoch[0/2] Step[3160] Loss=0.3634 GradNorm=2.7446 StepSize=0.0274 RelImp=84.18%\n",
            "Epoch[0/2] Step[3161] Loss=0.1239 GradNorm=1.8861 StepSize=0.0189 RelImp=94.61%\n",
            "Epoch[0/2] Step[3162] Loss=0.2463 GradNorm=3.0616 StepSize=0.0306 RelImp=89.28%\n",
            "Epoch[0/2] Step[3163] Loss=0.3191 GradNorm=3.0367 StepSize=0.0304 RelImp=86.10%\n",
            "Epoch[0/2] Step[3164] Loss=0.3027 GradNorm=2.4227 StepSize=0.0242 RelImp=86.82%\n",
            "Epoch[0/2] Step[3165] Loss=0.4595 GradNorm=4.8319 StepSize=0.0483 RelImp=79.99%\n",
            "Epoch[0/2] Step[3166] Loss=0.1921 GradNorm=2.5582 StepSize=0.0256 RelImp=91.64%\n",
            "Epoch[0/2] Step[3167] Loss=0.5592 GradNorm=4.2895 StepSize=0.0429 RelImp=75.65%\n",
            "Epoch[0/2] Step[3168] Loss=0.8222 GradNorm=5.2920 StepSize=0.0529 RelImp=64.20%\n",
            "Epoch[0/2] Step[3169] Loss=0.3203 GradNorm=3.3010 StepSize=0.0330 RelImp=86.05%\n",
            "Epoch[0/2] Step[3170] Loss=0.1675 GradNorm=1.9806 StepSize=0.0198 RelImp=92.70%\n",
            "Epoch[0/2] Step[3171] Loss=0.3473 GradNorm=2.2460 StepSize=0.0225 RelImp=84.88%\n",
            "Epoch[0/2] Step[3172] Loss=0.2888 GradNorm=3.7627 StepSize=0.0376 RelImp=87.42%\n",
            "Epoch[0/2] Step[3173] Loss=0.2420 GradNorm=2.7496 StepSize=0.0275 RelImp=89.46%\n",
            "Epoch[0/2] Step[3174] Loss=0.4200 GradNorm=3.3910 StepSize=0.0339 RelImp=81.71%\n",
            "Epoch[0/2] Step[3175] Loss=0.8787 GradNorm=5.5807 StepSize=0.0558 RelImp=61.74%\n",
            "Epoch[0/2] Step[3176] Loss=0.4027 GradNorm=3.0216 StepSize=0.0302 RelImp=82.46%\n",
            "Epoch[0/2] Step[3177] Loss=0.2521 GradNorm=3.1198 StepSize=0.0312 RelImp=89.02%\n",
            "Epoch[0/2] Step[3178] Loss=0.7204 GradNorm=4.1481 StepSize=0.0415 RelImp=68.63%\n",
            "Epoch[0/2] Step[3179] Loss=0.7026 GradNorm=4.1022 StepSize=0.0410 RelImp=69.41%\n",
            "Epoch[0/2] Step[3180] Loss=0.2484 GradNorm=2.4107 StepSize=0.0241 RelImp=89.18%\n",
            "Epoch[0/2] Step[3181] Loss=0.3395 GradNorm=3.5841 StepSize=0.0358 RelImp=85.22%\n",
            "Epoch[0/2] Step[3182] Loss=0.2272 GradNorm=2.8698 StepSize=0.0287 RelImp=90.11%\n",
            "Epoch[0/2] Step[3183] Loss=0.9608 GradNorm=5.9069 StepSize=0.0591 RelImp=58.16%\n",
            "Epoch[0/2] Step[3184] Loss=0.3942 GradNorm=5.1739 StepSize=0.0517 RelImp=82.83%\n",
            "Epoch[0/2] Step[3185] Loss=0.3528 GradNorm=4.6517 StepSize=0.0465 RelImp=84.64%\n",
            "Epoch[0/2] Step[3186] Loss=0.4556 GradNorm=2.5158 StepSize=0.0252 RelImp=80.16%\n",
            "Epoch[0/2] Step[3187] Loss=0.3944 GradNorm=4.3353 StepSize=0.0434 RelImp=82.83%\n",
            "Epoch[0/2] Step[3188] Loss=0.3277 GradNorm=4.1818 StepSize=0.0418 RelImp=85.73%\n",
            "Epoch[0/2] Step[3189] Loss=0.2561 GradNorm=3.0503 StepSize=0.0305 RelImp=88.85%\n",
            "Epoch[0/2] Step[3190] Loss=0.3076 GradNorm=2.6443 StepSize=0.0264 RelImp=86.61%\n",
            "Epoch[0/2] Step[3191] Loss=0.3250 GradNorm=2.0762 StepSize=0.0208 RelImp=85.85%\n",
            "Epoch[0/2] Step[3192] Loss=0.4939 GradNorm=5.0754 StepSize=0.0508 RelImp=78.50%\n",
            "Epoch[0/2] Step[3193] Loss=0.5835 GradNorm=2.7412 StepSize=0.0274 RelImp=74.59%\n",
            "Epoch[0/2] Step[3194] Loss=0.4888 GradNorm=3.3541 StepSize=0.0335 RelImp=78.71%\n",
            "Epoch[0/2] Step[3195] Loss=0.2771 GradNorm=2.3086 StepSize=0.0231 RelImp=87.94%\n",
            "Epoch[0/2] Step[3196] Loss=0.1756 GradNorm=1.5962 StepSize=0.0160 RelImp=92.35%\n",
            "Epoch[0/2] Step[3197] Loss=0.4893 GradNorm=5.4591 StepSize=0.0546 RelImp=78.70%\n",
            "Epoch[0/2] Step[3198] Loss=0.5968 GradNorm=3.0535 StepSize=0.0305 RelImp=74.02%\n",
            "Epoch[0/2] Step[3199] Loss=0.1547 GradNorm=2.1797 StepSize=0.0218 RelImp=93.27%\n",
            "Epoch[0/2] Step[3200] Loss=0.3408 GradNorm=3.9449 StepSize=0.0394 RelImp=85.16%\n",
            "Epoch[0/2] Step[3201] Loss=0.5773 GradNorm=3.8288 StepSize=0.0383 RelImp=74.86%\n",
            "Epoch[0/2] Step[3202] Loss=0.1591 GradNorm=2.1193 StepSize=0.0212 RelImp=93.07%\n",
            "Epoch[0/2] Step[3203] Loss=0.3674 GradNorm=2.6979 StepSize=0.0270 RelImp=84.00%\n",
            "Epoch[0/2] Step[3204] Loss=1.3183 GradNorm=6.5253 StepSize=0.0653 RelImp=42.60%\n",
            "Epoch[0/2] Step[3205] Loss=1.0214 GradNorm=4.1060 StepSize=0.0411 RelImp=55.53%\n",
            "Epoch[0/2] Step[3206] Loss=0.6477 GradNorm=5.3169 StepSize=0.0532 RelImp=71.80%\n",
            "Epoch[0/2] Step[3207] Loss=0.4475 GradNorm=5.2767 StepSize=0.0528 RelImp=80.51%\n",
            "Epoch[0/2] Step[3208] Loss=0.2421 GradNorm=3.3549 StepSize=0.0335 RelImp=89.46%\n",
            "Epoch[0/2] Step[3209] Loss=0.2981 GradNorm=3.4853 StepSize=0.0349 RelImp=87.02%\n",
            "Epoch[0/2] Step[3210] Loss=0.3713 GradNorm=3.5019 StepSize=0.0350 RelImp=83.83%\n",
            "Epoch[0/2] Step[3211] Loss=0.4146 GradNorm=3.2904 StepSize=0.0329 RelImp=81.95%\n",
            "Epoch[0/2] Step[3212] Loss=0.2235 GradNorm=2.9018 StepSize=0.0290 RelImp=90.27%\n",
            "Epoch[0/2] Step[3213] Loss=0.1715 GradNorm=1.6615 StepSize=0.0166 RelImp=92.53%\n",
            "Epoch[0/2] Step[3214] Loss=0.1554 GradNorm=2.5899 StepSize=0.0259 RelImp=93.24%\n",
            "Epoch[0/2] Step[3215] Loss=0.2240 GradNorm=2.9926 StepSize=0.0299 RelImp=90.25%\n",
            "Epoch[0/2] Step[3216] Loss=0.8953 GradNorm=4.5227 StepSize=0.0452 RelImp=61.01%\n",
            "Epoch[0/2] Step[3217] Loss=0.3623 GradNorm=2.4768 StepSize=0.0248 RelImp=84.23%\n",
            "Epoch[0/2] Step[3218] Loss=0.4031 GradNorm=5.0700 StepSize=0.0507 RelImp=82.45%\n",
            "Epoch[0/2] Step[3219] Loss=0.6261 GradNorm=3.6602 StepSize=0.0366 RelImp=72.74%\n",
            "Epoch[0/2] Step[3220] Loss=0.4859 GradNorm=3.1524 StepSize=0.0315 RelImp=78.84%\n",
            "Epoch[0/2] Step[3221] Loss=0.2634 GradNorm=1.6439 StepSize=0.0164 RelImp=88.53%\n",
            "Epoch[0/2] Step[3222] Loss=0.4023 GradNorm=2.8971 StepSize=0.0290 RelImp=82.48%\n",
            "Epoch[0/2] Step[3223] Loss=0.4341 GradNorm=3.4470 StepSize=0.0345 RelImp=81.10%\n",
            "Epoch[0/2] Step[3224] Loss=0.2444 GradNorm=2.7403 StepSize=0.0274 RelImp=89.36%\n",
            "Epoch[0/2] Step[3225] Loss=0.3779 GradNorm=2.1578 StepSize=0.0216 RelImp=83.55%\n",
            "Epoch[0/2] Step[3226] Loss=0.3975 GradNorm=4.4523 StepSize=0.0445 RelImp=82.69%\n",
            "Epoch[0/2] Step[3227] Loss=0.1735 GradNorm=2.0694 StepSize=0.0207 RelImp=92.45%\n",
            "Epoch[0/2] Step[3228] Loss=0.5354 GradNorm=3.2245 StepSize=0.0322 RelImp=76.69%\n",
            "Epoch[0/2] Step[3229] Loss=0.4096 GradNorm=3.0098 StepSize=0.0301 RelImp=82.16%\n",
            "Epoch[0/2] Step[3230] Loss=0.3093 GradNorm=3.0724 StepSize=0.0307 RelImp=86.53%\n",
            "Epoch[0/2] Step[3231] Loss=0.5388 GradNorm=3.0575 StepSize=0.0306 RelImp=76.54%\n",
            "Epoch[0/2] Step[3232] Loss=0.4819 GradNorm=2.9366 StepSize=0.0294 RelImp=79.02%\n",
            "Epoch[0/2] Step[3233] Loss=0.7539 GradNorm=4.0561 StepSize=0.0406 RelImp=67.17%\n",
            "Epoch[0/2] Step[3234] Loss=0.5993 GradNorm=4.3918 StepSize=0.0439 RelImp=73.91%\n",
            "Epoch[0/2] Step[3235] Loss=0.3090 GradNorm=3.0831 StepSize=0.0308 RelImp=86.54%\n",
            "Epoch[0/2] Step[3236] Loss=0.3710 GradNorm=3.2565 StepSize=0.0326 RelImp=83.85%\n",
            "Epoch[0/2] Step[3237] Loss=0.3658 GradNorm=3.2326 StepSize=0.0323 RelImp=84.07%\n",
            "Epoch[0/2] Step[3238] Loss=0.1328 GradNorm=1.0043 StepSize=0.0100 RelImp=94.22%\n",
            "Epoch[0/2] Step[3239] Loss=0.6820 GradNorm=2.8337 StepSize=0.0283 RelImp=70.30%\n",
            "Epoch[0/2] Step[3240] Loss=0.3265 GradNorm=2.8114 StepSize=0.0281 RelImp=85.78%\n",
            "Epoch[0/2] Step[3241] Loss=0.3388 GradNorm=2.9441 StepSize=0.0294 RelImp=85.25%\n",
            "Epoch[0/2] Step[3242] Loss=0.2178 GradNorm=2.3065 StepSize=0.0231 RelImp=90.52%\n",
            "Epoch[0/2] Step[3243] Loss=0.2189 GradNorm=3.5416 StepSize=0.0354 RelImp=90.47%\n",
            "Epoch[0/2] Step[3244] Loss=0.4300 GradNorm=3.5492 StepSize=0.0355 RelImp=81.27%\n",
            "Epoch[0/2] Step[3245] Loss=0.4848 GradNorm=3.0275 StepSize=0.0303 RelImp=78.89%\n",
            "Epoch[0/2] Step[3246] Loss=0.2299 GradNorm=3.3864 StepSize=0.0339 RelImp=89.99%\n",
            "Epoch[0/2] Step[3247] Loss=0.3509 GradNorm=3.2210 StepSize=0.0322 RelImp=84.72%\n",
            "Epoch[0/2] Step[3248] Loss=0.4041 GradNorm=2.6305 StepSize=0.0263 RelImp=82.40%\n",
            "Epoch[0/2] Step[3249] Loss=0.3545 GradNorm=2.8217 StepSize=0.0282 RelImp=84.57%\n",
            "Epoch[0/2] Step[3250] Loss=0.5813 GradNorm=4.4346 StepSize=0.0443 RelImp=74.69%\n",
            "Epoch[0/2] Step[3251] Loss=0.1552 GradNorm=1.7655 StepSize=0.0177 RelImp=93.24%\n",
            "Epoch[0/2] Step[3252] Loss=0.5025 GradNorm=4.6308 StepSize=0.0463 RelImp=78.12%\n",
            "Epoch[0/2] Step[3253] Loss=0.2596 GradNorm=1.6765 StepSize=0.0168 RelImp=88.70%\n",
            "Epoch[0/2] Step[3254] Loss=0.5851 GradNorm=3.4970 StepSize=0.0350 RelImp=74.52%\n",
            "Epoch[0/2] Step[3255] Loss=0.2928 GradNorm=3.2485 StepSize=0.0325 RelImp=87.25%\n",
            "Epoch[0/2] Step[3256] Loss=0.2038 GradNorm=2.2688 StepSize=0.0227 RelImp=91.13%\n",
            "Epoch[0/2] Step[3257] Loss=0.4882 GradNorm=4.3636 StepSize=0.0436 RelImp=78.74%\n",
            "Epoch[0/2] Step[3258] Loss=0.3654 GradNorm=2.1247 StepSize=0.0212 RelImp=84.09%\n",
            "Epoch[0/2] Step[3259] Loss=0.3154 GradNorm=2.3604 StepSize=0.0236 RelImp=86.27%\n",
            "Epoch[0/2] Step[3260] Loss=0.3525 GradNorm=3.6558 StepSize=0.0366 RelImp=84.65%\n",
            "Epoch[0/2] Step[3261] Loss=0.3783 GradNorm=3.7840 StepSize=0.0378 RelImp=83.53%\n",
            "Epoch[0/2] Step[3262] Loss=0.1668 GradNorm=1.7474 StepSize=0.0175 RelImp=92.74%\n",
            "Epoch[0/2] Step[3263] Loss=0.4494 GradNorm=2.4463 StepSize=0.0245 RelImp=80.43%\n",
            "Epoch[0/2] Step[3264] Loss=0.6891 GradNorm=5.7049 StepSize=0.0570 RelImp=70.00%\n",
            "Epoch[0/2] Step[3265] Loss=0.0725 GradNorm=1.0106 StepSize=0.0101 RelImp=96.84%\n",
            "Epoch[0/2] Step[3266] Loss=0.6282 GradNorm=5.2858 StepSize=0.0529 RelImp=72.64%\n",
            "Epoch[0/2] Step[3267] Loss=0.8050 GradNorm=3.5578 StepSize=0.0356 RelImp=64.95%\n",
            "Epoch[0/2] Step[3268] Loss=0.4061 GradNorm=4.2912 StepSize=0.0429 RelImp=82.32%\n",
            "Epoch[0/2] Step[3269] Loss=0.7563 GradNorm=5.0705 StepSize=0.0507 RelImp=67.07%\n",
            "Epoch[0/2] Step[3270] Loss=0.4900 GradNorm=4.2018 StepSize=0.0420 RelImp=78.67%\n",
            "Epoch[0/2] Step[3271] Loss=0.2017 GradNorm=2.5491 StepSize=0.0255 RelImp=91.22%\n",
            "Epoch[0/2] Step[3272] Loss=0.5665 GradNorm=3.0045 StepSize=0.0300 RelImp=75.33%\n",
            "Epoch[0/2] Step[3273] Loss=0.0904 GradNorm=1.3818 StepSize=0.0138 RelImp=96.06%\n",
            "Epoch[0/2] Step[3274] Loss=0.5025 GradNorm=4.8634 StepSize=0.0486 RelImp=78.12%\n",
            "Epoch[0/2] Step[3275] Loss=0.0767 GradNorm=1.0294 StepSize=0.0103 RelImp=96.66%\n",
            "Epoch[0/2] Step[3276] Loss=0.2642 GradNorm=3.7029 StepSize=0.0370 RelImp=88.50%\n",
            "Epoch[0/2] Step[3277] Loss=0.4357 GradNorm=4.3478 StepSize=0.0435 RelImp=81.03%\n",
            "Epoch[0/2] Step[3278] Loss=0.7112 GradNorm=3.9139 StepSize=0.0391 RelImp=69.03%\n",
            "Epoch[0/2] Step[3279] Loss=0.8658 GradNorm=5.2265 StepSize=0.0523 RelImp=62.30%\n",
            "Epoch[0/2] Step[3280] Loss=0.5161 GradNorm=2.8960 StepSize=0.0290 RelImp=77.53%\n",
            "Epoch[0/2] Step[3281] Loss=0.2275 GradNorm=2.1622 StepSize=0.0216 RelImp=90.10%\n",
            "Epoch[0/2] Step[3282] Loss=0.4218 GradNorm=4.2695 StepSize=0.0427 RelImp=81.63%\n",
            "Epoch[0/2] Step[3283] Loss=0.1855 GradNorm=2.9465 StepSize=0.0295 RelImp=91.92%\n",
            "Epoch[0/2] Step[3284] Loss=0.1572 GradNorm=1.7021 StepSize=0.0170 RelImp=93.16%\n",
            "Epoch[0/2] Step[3285] Loss=0.3058 GradNorm=2.6256 StepSize=0.0263 RelImp=86.69%\n",
            "Epoch[0/2] Step[3286] Loss=0.0992 GradNorm=1.4973 StepSize=0.0150 RelImp=95.68%\n",
            "Epoch[0/2] Step[3287] Loss=0.3840 GradNorm=2.7995 StepSize=0.0280 RelImp=83.28%\n",
            "Epoch[0/2] Step[3288] Loss=0.6522 GradNorm=3.9705 StepSize=0.0397 RelImp=71.60%\n",
            "Epoch[0/2] Step[3289] Loss=0.3345 GradNorm=3.6927 StepSize=0.0369 RelImp=85.44%\n",
            "Epoch[0/2] Step[3290] Loss=0.6849 GradNorm=3.9115 StepSize=0.0391 RelImp=70.18%\n",
            "Epoch[0/2] Step[3291] Loss=0.6198 GradNorm=4.0284 StepSize=0.0403 RelImp=73.01%\n",
            "Epoch[0/2] Step[3292] Loss=0.4655 GradNorm=5.1007 StepSize=0.0510 RelImp=79.73%\n",
            "Epoch[0/2] Step[3293] Loss=0.3050 GradNorm=2.4928 StepSize=0.0249 RelImp=86.72%\n",
            "Epoch[0/2] Step[3294] Loss=0.4070 GradNorm=4.6114 StepSize=0.0461 RelImp=82.28%\n",
            "Epoch[0/2] Step[3295] Loss=0.2412 GradNorm=1.5370 StepSize=0.0154 RelImp=89.50%\n",
            "Epoch[0/2] Step[3296] Loss=0.4497 GradNorm=4.1471 StepSize=0.0415 RelImp=80.42%\n",
            "Epoch[0/2] Step[3297] Loss=0.6992 GradNorm=4.7434 StepSize=0.0474 RelImp=69.56%\n",
            "Epoch[0/2] Step[3298] Loss=0.4769 GradNorm=4.1002 StepSize=0.0410 RelImp=79.24%\n",
            "Epoch[0/2] Step[3299] Loss=0.1096 GradNorm=1.2687 StepSize=0.0127 RelImp=95.23%\n",
            "Epoch[0/2] Step[3300] Loss=0.1366 GradNorm=1.6469 StepSize=0.0165 RelImp=94.05%\n",
            "Epoch[0/2] Step[3301] Loss=0.5221 GradNorm=5.1581 StepSize=0.0516 RelImp=77.27%\n",
            "Epoch[0/2] Step[3302] Loss=0.4124 GradNorm=4.9399 StepSize=0.0494 RelImp=82.05%\n",
            "Epoch[0/2] Step[3303] Loss=0.2540 GradNorm=3.2387 StepSize=0.0324 RelImp=88.94%\n",
            "Epoch[0/2] Step[3304] Loss=0.5621 GradNorm=2.8953 StepSize=0.0290 RelImp=75.52%\n",
            "Epoch[0/2] Step[3305] Loss=0.9533 GradNorm=6.2304 StepSize=0.0623 RelImp=58.49%\n",
            "Epoch[0/2] Step[3306] Loss=0.3640 GradNorm=3.8445 StepSize=0.0384 RelImp=84.15%\n",
            "Epoch[0/2] Step[3307] Loss=0.2777 GradNorm=3.4200 StepSize=0.0342 RelImp=87.91%\n",
            "Epoch[0/2] Step[3308] Loss=0.9811 GradNorm=5.6896 StepSize=0.0569 RelImp=57.28%\n",
            "Epoch[0/2] Step[3309] Loss=0.3996 GradNorm=3.8796 StepSize=0.0388 RelImp=82.60%\n",
            "Epoch[0/2] Step[3310] Loss=0.1254 GradNorm=1.1836 StepSize=0.0118 RelImp=94.54%\n",
            "Epoch[0/2] Step[3311] Loss=0.2927 GradNorm=2.5686 StepSize=0.0257 RelImp=87.25%\n",
            "Epoch[0/2] Step[3312] Loss=0.3976 GradNorm=4.6311 StepSize=0.0463 RelImp=82.69%\n",
            "Epoch[0/2] Step[3313] Loss=0.3443 GradNorm=3.5980 StepSize=0.0360 RelImp=85.01%\n",
            "Epoch[0/2] Step[3314] Loss=0.5061 GradNorm=4.3939 StepSize=0.0439 RelImp=77.96%\n",
            "Epoch[0/2] Step[3315] Loss=0.1819 GradNorm=2.1302 StepSize=0.0213 RelImp=92.08%\n",
            "Epoch[0/2] Step[3316] Loss=0.2970 GradNorm=2.5161 StepSize=0.0252 RelImp=87.07%\n",
            "Epoch[0/2] Step[3317] Loss=0.1687 GradNorm=1.7004 StepSize=0.0170 RelImp=92.65%\n",
            "Epoch[0/2] Step[3318] Loss=0.0982 GradNorm=1.1498 StepSize=0.0115 RelImp=95.73%\n",
            "Epoch[0/2] Step[3319] Loss=0.2462 GradNorm=3.1471 StepSize=0.0315 RelImp=89.28%\n",
            "Epoch[0/2] Step[3320] Loss=0.3892 GradNorm=2.9597 StepSize=0.0296 RelImp=83.05%\n",
            "Epoch[0/2] Step[3321] Loss=0.5893 GradNorm=3.7550 StepSize=0.0375 RelImp=74.34%\n",
            "Epoch[0/2] Step[3322] Loss=0.1450 GradNorm=1.4283 StepSize=0.0143 RelImp=93.69%\n",
            "Epoch[0/2] Step[3323] Loss=0.1448 GradNorm=2.3909 StepSize=0.0239 RelImp=93.69%\n",
            "Epoch[0/2] Step[3324] Loss=0.3303 GradNorm=2.4323 StepSize=0.0243 RelImp=85.62%\n",
            "Epoch[0/2] Step[3325] Loss=0.1322 GradNorm=1.6596 StepSize=0.0166 RelImp=94.24%\n",
            "Epoch[0/2] Step[3326] Loss=0.6393 GradNorm=3.9449 StepSize=0.0394 RelImp=72.16%\n",
            "Epoch[0/2] Step[3327] Loss=0.4532 GradNorm=3.2174 StepSize=0.0322 RelImp=80.27%\n",
            "Epoch[0/2] Step[3328] Loss=0.2195 GradNorm=2.0565 StepSize=0.0206 RelImp=90.44%\n",
            "Epoch[0/2] Step[3329] Loss=0.4064 GradNorm=2.9691 StepSize=0.0297 RelImp=82.31%\n",
            "Epoch[0/2] Step[3330] Loss=0.2121 GradNorm=2.3463 StepSize=0.0235 RelImp=90.76%\n",
            "Epoch[0/2] Step[3331] Loss=0.1056 GradNorm=1.1658 StepSize=0.0117 RelImp=95.40%\n",
            "Epoch[0/2] Step[3332] Loss=0.4898 GradNorm=3.7903 StepSize=0.0379 RelImp=78.67%\n",
            "Epoch[0/2] Step[3333] Loss=0.2036 GradNorm=2.6564 StepSize=0.0266 RelImp=91.14%\n",
            "Epoch[0/2] Step[3334] Loss=0.4172 GradNorm=3.9000 StepSize=0.0390 RelImp=81.83%\n",
            "Epoch[0/2] Step[3335] Loss=0.2971 GradNorm=2.1941 StepSize=0.0219 RelImp=87.06%\n",
            "Epoch[0/2] Step[3336] Loss=0.2650 GradNorm=2.5737 StepSize=0.0257 RelImp=88.46%\n",
            "Epoch[0/2] Step[3337] Loss=0.2662 GradNorm=2.1252 StepSize=0.0213 RelImp=88.41%\n",
            "Epoch[0/2] Step[3338] Loss=0.9573 GradNorm=3.5347 StepSize=0.0353 RelImp=58.32%\n",
            "Epoch[0/2] Step[3339] Loss=0.3384 GradNorm=4.3585 StepSize=0.0436 RelImp=85.26%\n",
            "Epoch[0/2] Step[3340] Loss=0.7707 GradNorm=5.2945 StepSize=0.0529 RelImp=66.44%\n",
            "Epoch[0/2] Step[3341] Loss=1.0488 GradNorm=4.6910 StepSize=0.0469 RelImp=54.33%\n",
            "Epoch[0/2] Step[3342] Loss=0.1481 GradNorm=1.4116 StepSize=0.0141 RelImp=93.55%\n",
            "Epoch[0/2] Step[3343] Loss=0.7680 GradNorm=3.8522 StepSize=0.0385 RelImp=66.56%\n",
            "Epoch[0/2] Step[3344] Loss=0.2172 GradNorm=2.7420 StepSize=0.0274 RelImp=90.54%\n",
            "Epoch[0/2] Step[3345] Loss=0.4726 GradNorm=4.1113 StepSize=0.0411 RelImp=79.42%\n",
            "Epoch[0/2] Step[3346] Loss=0.7719 GradNorm=3.5322 StepSize=0.0353 RelImp=66.39%\n",
            "Epoch[0/2] Step[3347] Loss=0.1880 GradNorm=2.4767 StepSize=0.0248 RelImp=91.82%\n",
            "Epoch[0/2] Step[3348] Loss=0.3970 GradNorm=4.3217 StepSize=0.0432 RelImp=82.71%\n",
            "Epoch[0/2] Step[3349] Loss=1.0638 GradNorm=4.0330 StepSize=0.0403 RelImp=53.68%\n",
            "Epoch[0/2] Step[3350] Loss=0.2658 GradNorm=2.8164 StepSize=0.0282 RelImp=88.43%\n",
            "Epoch[0/2] Step[3351] Loss=0.2472 GradNorm=3.2289 StepSize=0.0323 RelImp=89.24%\n",
            "Epoch[0/2] Step[3352] Loss=0.2161 GradNorm=3.4964 StepSize=0.0350 RelImp=90.59%\n",
            "Epoch[0/2] Step[3353] Loss=0.2316 GradNorm=2.6193 StepSize=0.0262 RelImp=89.92%\n",
            "Epoch[0/2] Step[3354] Loss=0.3055 GradNorm=3.3151 StepSize=0.0332 RelImp=86.70%\n",
            "Epoch[0/2] Step[3355] Loss=0.5110 GradNorm=3.7748 StepSize=0.0377 RelImp=77.75%\n",
            "Epoch[0/2] Step[3356] Loss=0.6805 GradNorm=4.6019 StepSize=0.0460 RelImp=70.37%\n",
            "Epoch[0/2] Step[3357] Loss=0.6376 GradNorm=3.7827 StepSize=0.0378 RelImp=72.24%\n",
            "Epoch[0/2] Step[3358] Loss=0.4224 GradNorm=3.1978 StepSize=0.0320 RelImp=81.61%\n",
            "Epoch[0/2] Step[3359] Loss=0.8249 GradNorm=5.0391 StepSize=0.0504 RelImp=64.08%\n",
            "Epoch[0/2] Step[3360] Loss=0.2251 GradNorm=3.0817 StepSize=0.0308 RelImp=90.20%\n",
            "Epoch[0/2] Step[3361] Loss=0.4088 GradNorm=2.4606 StepSize=0.0246 RelImp=82.20%\n",
            "Epoch[0/2] Step[3362] Loss=0.3316 GradNorm=3.5202 StepSize=0.0352 RelImp=85.56%\n",
            "Epoch[0/2] Step[3363] Loss=0.2186 GradNorm=3.0448 StepSize=0.0304 RelImp=90.48%\n",
            "Epoch[0/2] Step[3364] Loss=0.2185 GradNorm=3.0646 StepSize=0.0306 RelImp=90.49%\n",
            "Epoch[0/2] Step[3365] Loss=0.4422 GradNorm=3.1850 StepSize=0.0318 RelImp=80.75%\n",
            "Epoch[0/2] Step[3366] Loss=0.3730 GradNorm=3.5974 StepSize=0.0360 RelImp=83.76%\n",
            "Epoch[0/2] Step[3367] Loss=0.2425 GradNorm=3.4637 StepSize=0.0346 RelImp=89.44%\n",
            "Epoch[0/2] Step[3368] Loss=0.5347 GradNorm=2.5241 StepSize=0.0252 RelImp=76.72%\n",
            "Epoch[0/2] Step[3369] Loss=0.2121 GradNorm=2.2350 StepSize=0.0224 RelImp=90.76%\n",
            "Epoch[0/2] Step[3370] Loss=0.3199 GradNorm=2.3050 StepSize=0.0231 RelImp=86.07%\n",
            "Epoch[0/2] Step[3371] Loss=0.2073 GradNorm=2.5271 StepSize=0.0253 RelImp=90.98%\n",
            "Epoch[0/2] Step[3372] Loss=0.0701 GradNorm=1.0485 StepSize=0.0105 RelImp=96.95%\n",
            "Epoch[0/2] Step[3373] Loss=0.2334 GradNorm=2.6031 StepSize=0.0260 RelImp=89.84%\n",
            "Epoch[0/2] Step[3374] Loss=0.2058 GradNorm=2.7476 StepSize=0.0275 RelImp=91.04%\n",
            "Epoch[0/2] Step[3375] Loss=0.4084 GradNorm=4.5601 StepSize=0.0456 RelImp=82.22%\n",
            "Epoch[0/2] Step[3376] Loss=0.8009 GradNorm=3.2099 StepSize=0.0321 RelImp=65.12%\n",
            "Epoch[0/2] Step[3377] Loss=0.1572 GradNorm=1.3123 StepSize=0.0131 RelImp=93.16%\n",
            "Epoch[0/2] Step[3378] Loss=0.5861 GradNorm=5.0279 StepSize=0.0503 RelImp=74.48%\n",
            "Epoch[0/2] Step[3379] Loss=0.3722 GradNorm=3.6039 StepSize=0.0360 RelImp=83.79%\n",
            "Epoch[0/2] Step[3380] Loss=0.4577 GradNorm=3.4741 StepSize=0.0347 RelImp=80.07%\n",
            "Epoch[0/2] Step[3381] Loss=0.2513 GradNorm=2.6434 StepSize=0.0264 RelImp=89.06%\n",
            "Epoch[0/2] Step[3382] Loss=0.3299 GradNorm=3.8984 StepSize=0.0390 RelImp=85.64%\n",
            "Epoch[0/2] Step[3383] Loss=0.5913 GradNorm=3.9491 StepSize=0.0395 RelImp=74.25%\n",
            "Epoch[0/2] Step[3384] Loss=1.0832 GradNorm=5.7180 StepSize=0.0572 RelImp=52.83%\n",
            "Epoch[0/2] Step[3385] Loss=0.3014 GradNorm=2.5573 StepSize=0.0256 RelImp=86.88%\n",
            "Epoch[0/2] Step[3386] Loss=0.2357 GradNorm=2.8599 StepSize=0.0286 RelImp=89.74%\n",
            "Epoch[0/2] Step[3387] Loss=0.3717 GradNorm=3.1329 StepSize=0.0313 RelImp=83.82%\n",
            "Epoch[0/2] Step[3388] Loss=0.2927 GradNorm=3.3826 StepSize=0.0338 RelImp=87.26%\n",
            "Epoch[0/2] Step[3389] Loss=0.0649 GradNorm=1.0677 StepSize=0.0107 RelImp=97.18%\n",
            "Epoch[0/2] Step[3390] Loss=0.1959 GradNorm=2.7094 StepSize=0.0271 RelImp=91.47%\n",
            "Epoch[0/2] Step[3391] Loss=0.3055 GradNorm=3.5284 StepSize=0.0353 RelImp=86.70%\n",
            "Epoch[0/2] Step[3392] Loss=0.5302 GradNorm=3.4883 StepSize=0.0349 RelImp=76.91%\n",
            "Epoch[0/2] Step[3393] Loss=0.1714 GradNorm=2.5424 StepSize=0.0254 RelImp=92.54%\n",
            "Epoch[0/2] Step[3394] Loss=0.1975 GradNorm=1.8121 StepSize=0.0181 RelImp=91.40%\n",
            "Epoch[0/2] Step[3395] Loss=0.2888 GradNorm=2.4766 StepSize=0.0248 RelImp=87.43%\n",
            "Epoch[0/2] Step[3396] Loss=0.3275 GradNorm=3.1918 StepSize=0.0319 RelImp=85.74%\n",
            "Epoch[0/2] Step[3397] Loss=0.4021 GradNorm=3.3999 StepSize=0.0340 RelImp=82.49%\n",
            "Epoch[0/2] Step[3398] Loss=0.4491 GradNorm=3.6315 StepSize=0.0363 RelImp=80.44%\n",
            "Epoch[0/2] Step[3399] Loss=0.4059 GradNorm=3.4053 StepSize=0.0341 RelImp=82.33%\n",
            "Epoch[0/2] Step[3400] Loss=0.7077 GradNorm=4.0104 StepSize=0.0401 RelImp=69.18%\n",
            "Epoch[0/2] Step[3401] Loss=0.9435 GradNorm=4.3101 StepSize=0.0431 RelImp=58.92%\n",
            "Epoch[0/2] Step[3402] Loss=0.4487 GradNorm=3.9001 StepSize=0.0390 RelImp=80.46%\n",
            "Epoch[0/2] Step[3403] Loss=0.9723 GradNorm=5.8908 StepSize=0.0589 RelImp=57.67%\n",
            "Epoch[0/2] Step[3404] Loss=0.4821 GradNorm=5.1039 StepSize=0.0510 RelImp=79.01%\n",
            "Epoch[0/2] Step[3405] Loss=0.5759 GradNorm=4.1912 StepSize=0.0419 RelImp=74.92%\n",
            "Epoch[0/2] Step[3406] Loss=0.2489 GradNorm=3.1250 StepSize=0.0312 RelImp=89.16%\n",
            "Epoch[0/2] Step[3407] Loss=0.1395 GradNorm=1.1043 StepSize=0.0110 RelImp=93.93%\n",
            "Epoch[0/2] Step[3408] Loss=0.3786 GradNorm=4.4129 StepSize=0.0441 RelImp=83.51%\n",
            "Epoch[0/2] Step[3409] Loss=0.4267 GradNorm=3.7573 StepSize=0.0376 RelImp=81.42%\n",
            "Epoch[0/2] Step[3410] Loss=0.3968 GradNorm=3.7150 StepSize=0.0372 RelImp=82.72%\n",
            "Epoch[0/2] Step[3411] Loss=0.1515 GradNorm=1.5829 StepSize=0.0158 RelImp=93.40%\n",
            "Epoch[0/2] Step[3412] Loss=0.8423 GradNorm=4.2383 StepSize=0.0424 RelImp=63.33%\n",
            "Epoch[0/2] Step[3413] Loss=0.5497 GradNorm=4.8124 StepSize=0.0481 RelImp=76.06%\n",
            "Epoch[0/2] Step[3414] Loss=0.4256 GradNorm=3.4628 StepSize=0.0346 RelImp=81.47%\n",
            "Epoch[0/2] Step[3415] Loss=0.5710 GradNorm=2.3258 StepSize=0.0233 RelImp=75.14%\n",
            "Epoch[0/2] Step[3416] Loss=0.1380 GradNorm=1.4811 StepSize=0.0148 RelImp=93.99%\n",
            "Epoch[0/2] Step[3417] Loss=0.3606 GradNorm=4.5914 StepSize=0.0459 RelImp=84.30%\n",
            "Epoch[0/2] Step[3418] Loss=0.6073 GradNorm=3.7680 StepSize=0.0377 RelImp=73.56%\n",
            "Epoch[0/2] Step[3419] Loss=0.6244 GradNorm=3.4501 StepSize=0.0345 RelImp=72.81%\n",
            "Epoch[0/2] Step[3420] Loss=0.2774 GradNorm=2.8316 StepSize=0.0283 RelImp=87.92%\n",
            "Epoch[0/2] Step[3421] Loss=0.1965 GradNorm=2.3921 StepSize=0.0239 RelImp=91.44%\n",
            "Epoch[0/2] Step[3422] Loss=0.2969 GradNorm=2.8363 StepSize=0.0284 RelImp=87.07%\n",
            "Epoch[0/2] Step[3423] Loss=0.3503 GradNorm=5.8617 StepSize=0.0586 RelImp=84.75%\n",
            "Epoch[0/2] Step[3424] Loss=0.2731 GradNorm=2.6221 StepSize=0.0262 RelImp=88.11%\n",
            "Epoch[0/2] Step[3425] Loss=0.4377 GradNorm=3.3010 StepSize=0.0330 RelImp=80.94%\n",
            "Epoch[0/2] Step[3426] Loss=0.5103 GradNorm=4.1183 StepSize=0.0412 RelImp=77.78%\n",
            "Epoch[0/2] Step[3427] Loss=0.1354 GradNorm=1.4700 StepSize=0.0147 RelImp=94.11%\n",
            "Epoch[0/2] Step[3428] Loss=0.8567 GradNorm=6.2534 StepSize=0.0625 RelImp=62.70%\n",
            "Epoch[0/2] Step[3429] Loss=0.2987 GradNorm=3.8994 StepSize=0.0390 RelImp=87.00%\n",
            "Epoch[0/2] Step[3430] Loss=0.6180 GradNorm=4.1691 StepSize=0.0417 RelImp=73.09%\n",
            "Epoch[0/2] Step[3431] Loss=0.7954 GradNorm=6.7950 StepSize=0.0679 RelImp=65.37%\n",
            "Epoch[0/2] Step[3432] Loss=0.6914 GradNorm=6.0078 StepSize=0.0601 RelImp=69.90%\n",
            "Epoch[0/2] Step[3433] Loss=0.8533 GradNorm=5.0095 StepSize=0.0501 RelImp=62.84%\n",
            "Epoch[0/2] Step[3434] Loss=0.4072 GradNorm=4.2351 StepSize=0.0424 RelImp=82.27%\n",
            "Epoch[0/2] Step[3435] Loss=0.5024 GradNorm=3.7142 StepSize=0.0371 RelImp=78.12%\n",
            "Epoch[0/2] Step[3436] Loss=0.7051 GradNorm=5.4337 StepSize=0.0543 RelImp=69.30%\n",
            "Epoch[0/2] Step[3437] Loss=0.6148 GradNorm=4.1160 StepSize=0.0412 RelImp=73.23%\n",
            "Epoch[0/2] Step[3438] Loss=0.2826 GradNorm=3.5253 StepSize=0.0353 RelImp=87.70%\n",
            "Epoch[0/2] Step[3439] Loss=0.5892 GradNorm=5.4483 StepSize=0.0545 RelImp=74.34%\n",
            "Epoch[0/2] Step[3440] Loss=0.2536 GradNorm=2.4773 StepSize=0.0248 RelImp=88.96%\n",
            "Epoch[0/2] Step[3441] Loss=0.6352 GradNorm=3.9405 StepSize=0.0394 RelImp=72.34%\n",
            "Epoch[0/2] Step[3442] Loss=0.2660 GradNorm=2.5855 StepSize=0.0259 RelImp=88.42%\n",
            "Epoch[0/2] Step[3443] Loss=0.6282 GradNorm=4.8190 StepSize=0.0482 RelImp=72.65%\n",
            "Epoch[0/2] Step[3444] Loss=0.3955 GradNorm=4.5876 StepSize=0.0459 RelImp=82.78%\n",
            "Epoch[0/2] Step[3445] Loss=0.5401 GradNorm=5.1032 StepSize=0.0510 RelImp=76.48%\n",
            "Epoch[0/2] Step[3446] Loss=0.1912 GradNorm=2.3285 StepSize=0.0233 RelImp=91.67%\n",
            "Epoch[0/2] Step[3447] Loss=0.2116 GradNorm=2.1938 StepSize=0.0219 RelImp=90.79%\n",
            "Epoch[0/2] Step[3448] Loss=0.3728 GradNorm=2.2792 StepSize=0.0228 RelImp=83.77%\n",
            "Epoch[0/2] Step[3449] Loss=0.1974 GradNorm=2.7212 StepSize=0.0272 RelImp=91.41%\n",
            "Epoch[0/2] Step[3450] Loss=0.6927 GradNorm=3.2350 StepSize=0.0323 RelImp=69.84%\n",
            "Epoch[0/2] Step[3451] Loss=0.7186 GradNorm=3.0683 StepSize=0.0307 RelImp=68.71%\n",
            "Epoch[0/2] Step[3452] Loss=0.3422 GradNorm=3.4107 StepSize=0.0341 RelImp=85.10%\n",
            "Epoch[0/2] Step[3453] Loss=0.1093 GradNorm=1.3165 StepSize=0.0132 RelImp=95.24%\n",
            "Epoch[0/2] Step[3454] Loss=0.3748 GradNorm=4.8657 StepSize=0.0487 RelImp=83.68%\n",
            "Epoch[0/2] Step[3455] Loss=0.2558 GradNorm=2.3266 StepSize=0.0233 RelImp=88.86%\n",
            "Epoch[0/2] Step[3456] Loss=0.3387 GradNorm=3.3261 StepSize=0.0333 RelImp=85.25%\n",
            "Epoch[0/2] Step[3457] Loss=0.0409 GradNorm=0.6739 StepSize=0.0067 RelImp=98.22%\n",
            "Epoch[0/2] Step[3458] Loss=0.1844 GradNorm=1.5422 StepSize=0.0154 RelImp=91.97%\n",
            "Epoch[0/2] Step[3459] Loss=0.1474 GradNorm=2.1808 StepSize=0.0218 RelImp=93.58%\n",
            "Epoch[0/2] Step[3460] Loss=0.1164 GradNorm=1.5054 StepSize=0.0151 RelImp=94.93%\n",
            "Epoch[0/2] Step[3461] Loss=0.3008 GradNorm=3.0926 StepSize=0.0309 RelImp=86.90%\n",
            "Epoch[0/2] Step[3462] Loss=0.1964 GradNorm=1.9429 StepSize=0.0194 RelImp=91.45%\n",
            "Epoch[0/2] Step[3463] Loss=0.4948 GradNorm=5.0030 StepSize=0.0500 RelImp=78.46%\n",
            "Epoch[0/2] Step[3464] Loss=0.0362 GradNorm=0.5779 StepSize=0.0058 RelImp=98.42%\n",
            "Epoch[0/2] Step[3465] Loss=0.4390 GradNorm=3.6894 StepSize=0.0369 RelImp=80.88%\n",
            "Epoch[0/2] Step[3466] Loss=1.2230 GradNorm=5.9816 StepSize=0.0598 RelImp=46.75%\n",
            "Epoch[0/2] Step[3467] Loss=0.4159 GradNorm=4.0363 StepSize=0.0404 RelImp=81.89%\n",
            "Epoch[0/2] Step[3468] Loss=0.2737 GradNorm=2.4822 StepSize=0.0248 RelImp=88.08%\n",
            "Epoch[0/2] Step[3469] Loss=0.5823 GradNorm=4.4468 StepSize=0.0445 RelImp=74.64%\n",
            "Epoch[0/2] Step[3470] Loss=0.1634 GradNorm=2.7705 StepSize=0.0277 RelImp=92.89%\n",
            "Epoch[0/2] Step[3471] Loss=0.8599 GradNorm=4.5070 StepSize=0.0451 RelImp=62.56%\n",
            "Epoch[0/2] Step[3472] Loss=0.7892 GradNorm=4.4436 StepSize=0.0444 RelImp=65.64%\n",
            "Epoch[0/2] Step[3473] Loss=0.3615 GradNorm=3.4402 StepSize=0.0344 RelImp=84.26%\n",
            "Epoch[0/2] Step[3474] Loss=0.4702 GradNorm=3.7238 StepSize=0.0372 RelImp=79.53%\n",
            "Epoch[0/2] Step[3475] Loss=0.6847 GradNorm=3.4909 StepSize=0.0349 RelImp=70.19%\n",
            "Epoch[0/2] Step[3476] Loss=1.1722 GradNorm=5.7207 StepSize=0.0572 RelImp=48.96%\n",
            "Epoch[0/2] Step[3477] Loss=0.2490 GradNorm=3.0515 StepSize=0.0305 RelImp=89.16%\n",
            "Epoch[0/2] Step[3478] Loss=1.0923 GradNorm=4.2162 StepSize=0.0422 RelImp=52.44%\n",
            "Epoch[0/2] Step[3479] Loss=0.2938 GradNorm=3.4144 StepSize=0.0341 RelImp=87.21%\n",
            "Epoch[0/2] Step[3480] Loss=0.4721 GradNorm=3.6306 StepSize=0.0363 RelImp=79.44%\n",
            "Epoch[0/2] Step[3481] Loss=0.0732 GradNorm=0.7753 StepSize=0.0078 RelImp=96.81%\n",
            "Epoch[0/2] Step[3482] Loss=0.5219 GradNorm=4.8349 StepSize=0.0483 RelImp=77.28%\n",
            "Epoch[0/2] Step[3483] Loss=0.3894 GradNorm=3.1701 StepSize=0.0317 RelImp=83.04%\n",
            "Epoch[0/2] Step[3484] Loss=0.3125 GradNorm=2.1788 StepSize=0.0218 RelImp=86.39%\n",
            "Epoch[0/2] Step[3485] Loss=0.4020 GradNorm=5.5603 StepSize=0.0556 RelImp=82.50%\n",
            "Epoch[0/2] Step[3486] Loss=0.3763 GradNorm=3.2188 StepSize=0.0322 RelImp=83.62%\n",
            "Epoch[0/2] Step[3487] Loss=0.7506 GradNorm=4.4612 StepSize=0.0446 RelImp=67.32%\n",
            "Epoch[0/2] Step[3488] Loss=0.8802 GradNorm=3.7973 StepSize=0.0380 RelImp=61.67%\n",
            "Epoch[0/2] Step[3489] Loss=0.5337 GradNorm=3.2613 StepSize=0.0326 RelImp=76.76%\n",
            "Epoch[0/2] Step[3490] Loss=0.1538 GradNorm=2.1965 StepSize=0.0220 RelImp=93.30%\n",
            "Epoch[0/2] Step[3491] Loss=0.1916 GradNorm=2.8572 StepSize=0.0286 RelImp=91.66%\n",
            "Epoch[0/2] Step[3492] Loss=0.2441 GradNorm=3.1507 StepSize=0.0315 RelImp=89.37%\n",
            "Epoch[0/2] Step[3493] Loss=0.3602 GradNorm=3.1232 StepSize=0.0312 RelImp=84.32%\n",
            "Epoch[0/2] Step[3494] Loss=0.4078 GradNorm=3.3824 StepSize=0.0338 RelImp=82.24%\n",
            "Epoch[0/2] Step[3495] Loss=0.2603 GradNorm=2.5211 StepSize=0.0252 RelImp=88.66%\n",
            "Epoch[0/2] Step[3496] Loss=0.1630 GradNorm=1.7518 StepSize=0.0175 RelImp=92.90%\n",
            "Epoch[0/2] Step[3497] Loss=0.2807 GradNorm=2.6116 StepSize=0.0261 RelImp=87.78%\n",
            "Epoch[0/2] Step[3498] Loss=0.2730 GradNorm=2.4205 StepSize=0.0242 RelImp=88.11%\n",
            "Epoch[0/2] Step[3499] Loss=1.0589 GradNorm=4.4095 StepSize=0.0441 RelImp=53.89%\n",
            "Epoch[0/2] Step[3500] Loss=0.2475 GradNorm=3.0972 StepSize=0.0310 RelImp=89.22%\n",
            "Epoch[0/2] Step[3501] Loss=0.3737 GradNorm=3.0123 StepSize=0.0301 RelImp=83.73%\n",
            "Epoch[0/2] Step[3502] Loss=0.7273 GradNorm=3.5752 StepSize=0.0358 RelImp=68.33%\n",
            "Epoch[0/2] Step[3503] Loss=0.3323 GradNorm=3.2336 StepSize=0.0323 RelImp=85.53%\n",
            "Epoch[0/2] Step[3504] Loss=0.5093 GradNorm=2.6069 StepSize=0.0261 RelImp=77.82%\n",
            "Epoch[0/2] Step[3505] Loss=0.4450 GradNorm=3.7755 StepSize=0.0378 RelImp=80.62%\n",
            "Epoch[0/2] Step[3506] Loss=0.2179 GradNorm=2.3679 StepSize=0.0237 RelImp=90.51%\n",
            "Epoch[0/2] Step[3507] Loss=0.0807 GradNorm=0.7706 StepSize=0.0077 RelImp=96.49%\n",
            "Epoch[0/2] Step[3508] Loss=0.3397 GradNorm=3.9326 StepSize=0.0393 RelImp=85.21%\n",
            "Epoch[0/2] Step[3509] Loss=0.3084 GradNorm=4.1155 StepSize=0.0412 RelImp=86.57%\n",
            "Epoch[0/2] Step[3510] Loss=0.2172 GradNorm=2.4358 StepSize=0.0244 RelImp=90.54%\n",
            "Epoch[0/2] Step[3511] Loss=0.2177 GradNorm=2.3352 StepSize=0.0234 RelImp=90.52%\n",
            "Epoch[0/2] Step[3512] Loss=0.1954 GradNorm=2.5773 StepSize=0.0258 RelImp=91.49%\n",
            "Epoch[0/2] Step[3513] Loss=0.5280 GradNorm=2.6004 StepSize=0.0260 RelImp=77.01%\n",
            "Epoch[0/2] Step[3514] Loss=0.2054 GradNorm=2.0966 StepSize=0.0210 RelImp=91.06%\n",
            "Epoch[0/2] Step[3515] Loss=0.2165 GradNorm=1.3560 StepSize=0.0136 RelImp=90.57%\n",
            "Epoch[0/2] Step[3516] Loss=0.3600 GradNorm=2.6878 StepSize=0.0269 RelImp=84.32%\n",
            "Epoch[0/2] Step[3517] Loss=0.6125 GradNorm=3.9690 StepSize=0.0397 RelImp=73.33%\n",
            "Epoch[0/2] Step[3518] Loss=0.1209 GradNorm=1.5998 StepSize=0.0160 RelImp=94.74%\n",
            "Epoch[0/2] Step[3519] Loss=0.1719 GradNorm=2.1818 StepSize=0.0218 RelImp=92.52%\n",
            "Epoch[0/2] Step[3520] Loss=0.2715 GradNorm=1.6784 StepSize=0.0168 RelImp=88.18%\n",
            "Epoch[0/2] Step[3521] Loss=0.1944 GradNorm=1.5702 StepSize=0.0157 RelImp=91.54%\n",
            "Epoch[0/2] Step[3522] Loss=0.6910 GradNorm=5.4299 StepSize=0.0543 RelImp=69.91%\n",
            "Epoch[0/2] Step[3523] Loss=0.2436 GradNorm=2.7194 StepSize=0.0272 RelImp=89.39%\n",
            "Epoch[0/2] Step[3524] Loss=0.2769 GradNorm=2.5719 StepSize=0.0257 RelImp=87.94%\n",
            "Epoch[0/2] Step[3525] Loss=0.3283 GradNorm=3.9384 StepSize=0.0394 RelImp=85.71%\n",
            "Epoch[0/2] Step[3526] Loss=0.4678 GradNorm=5.2609 StepSize=0.0526 RelImp=79.63%\n",
            "Epoch[0/2] Step[3527] Loss=0.3301 GradNorm=3.6101 StepSize=0.0361 RelImp=85.63%\n",
            "Epoch[0/2] Step[3528] Loss=0.3063 GradNorm=4.8540 StepSize=0.0485 RelImp=86.66%\n",
            "Epoch[0/2] Step[3529] Loss=0.3223 GradNorm=2.4989 StepSize=0.0250 RelImp=85.97%\n",
            "Epoch[0/2] Step[3530] Loss=0.6689 GradNorm=6.8484 StepSize=0.0685 RelImp=70.87%\n",
            "Epoch[0/2] Step[3531] Loss=0.4210 GradNorm=2.7639 StepSize=0.0276 RelImp=81.67%\n",
            "Epoch[0/2] Step[3532] Loss=0.6184 GradNorm=4.0681 StepSize=0.0407 RelImp=73.07%\n",
            "Epoch[0/2] Step[3533] Loss=0.2772 GradNorm=3.0117 StepSize=0.0301 RelImp=87.93%\n",
            "Epoch[0/2] Step[3534] Loss=0.5797 GradNorm=3.1185 StepSize=0.0312 RelImp=74.76%\n",
            "Epoch[0/2] Step[3535] Loss=0.2424 GradNorm=2.9160 StepSize=0.0292 RelImp=89.45%\n",
            "Epoch[0/2] Step[3536] Loss=0.2785 GradNorm=1.9477 StepSize=0.0195 RelImp=87.87%\n",
            "Epoch[0/2] Step[3537] Loss=0.2534 GradNorm=3.2422 StepSize=0.0324 RelImp=88.97%\n",
            "Epoch[0/2] Step[3538] Loss=0.5334 GradNorm=5.1427 StepSize=0.0514 RelImp=76.77%\n",
            "Epoch[0/2] Step[3539] Loss=0.0826 GradNorm=0.8430 StepSize=0.0084 RelImp=96.40%\n",
            "Epoch[0/2] Step[3540] Loss=0.1315 GradNorm=1.6541 StepSize=0.0165 RelImp=94.28%\n",
            "Epoch[0/2] Step[3541] Loss=0.2205 GradNorm=1.8619 StepSize=0.0186 RelImp=90.40%\n",
            "Epoch[0/2] Step[3542] Loss=0.3012 GradNorm=3.2296 StepSize=0.0323 RelImp=86.89%\n",
            "Epoch[0/2] Step[3543] Loss=0.2309 GradNorm=2.8590 StepSize=0.0286 RelImp=89.94%\n",
            "Epoch[0/2] Step[3544] Loss=0.5442 GradNorm=4.0647 StepSize=0.0406 RelImp=76.31%\n",
            "Epoch[0/2] Step[3545] Loss=0.1618 GradNorm=2.5347 StepSize=0.0253 RelImp=92.96%\n",
            "Epoch[0/2] Step[3546] Loss=0.4014 GradNorm=4.3033 StepSize=0.0430 RelImp=82.52%\n",
            "Epoch[0/2] Step[3547] Loss=0.4525 GradNorm=2.9022 StepSize=0.0290 RelImp=80.30%\n",
            "Epoch[0/2] Step[3548] Loss=0.3491 GradNorm=4.0303 StepSize=0.0403 RelImp=84.80%\n",
            "Epoch[0/2] Step[3549] Loss=0.4730 GradNorm=4.1937 StepSize=0.0419 RelImp=79.40%\n",
            "Epoch[0/2] Step[3550] Loss=0.6704 GradNorm=4.0276 StepSize=0.0403 RelImp=70.81%\n",
            "Epoch[0/2] Step[3551] Loss=0.1877 GradNorm=2.5189 StepSize=0.0252 RelImp=91.83%\n",
            "Epoch[0/2] Step[3552] Loss=0.5075 GradNorm=3.3703 StepSize=0.0337 RelImp=77.90%\n",
            "Epoch[0/2] Step[3553] Loss=0.2838 GradNorm=4.2892 StepSize=0.0429 RelImp=87.64%\n",
            "Epoch[0/2] Step[3554] Loss=0.5176 GradNorm=5.1912 StepSize=0.0519 RelImp=77.46%\n",
            "Epoch[0/2] Step[3555] Loss=0.8806 GradNorm=5.9479 StepSize=0.0595 RelImp=61.66%\n",
            "Epoch[0/2] Step[3556] Loss=0.3097 GradNorm=1.9923 StepSize=0.0199 RelImp=86.51%\n",
            "Epoch[0/2] Step[3557] Loss=0.7461 GradNorm=5.3050 StepSize=0.0531 RelImp=67.51%\n",
            "Epoch[0/2] Step[3558] Loss=0.3465 GradNorm=2.9441 StepSize=0.0294 RelImp=84.91%\n",
            "Epoch[0/2] Step[3559] Loss=0.5758 GradNorm=3.1951 StepSize=0.0320 RelImp=74.93%\n",
            "Epoch[0/2] Step[3560] Loss=0.0979 GradNorm=1.0053 StepSize=0.0101 RelImp=95.74%\n",
            "Epoch[0/2] Step[3561] Loss=0.3113 GradNorm=2.6090 StepSize=0.0261 RelImp=86.44%\n",
            "Epoch[0/2] Step[3562] Loss=0.3218 GradNorm=3.6132 StepSize=0.0361 RelImp=85.99%\n",
            "Epoch[0/2] Step[3563] Loss=0.9261 GradNorm=5.3476 StepSize=0.0535 RelImp=59.67%\n",
            "Epoch[0/2] Step[3564] Loss=0.7343 GradNorm=4.5237 StepSize=0.0452 RelImp=68.03%\n",
            "Epoch[0/2] Step[3565] Loss=0.4779 GradNorm=4.5852 StepSize=0.0459 RelImp=79.19%\n",
            "Epoch[0/2] Step[3566] Loss=0.5674 GradNorm=4.6313 StepSize=0.0463 RelImp=75.30%\n",
            "Epoch[0/2] Step[3567] Loss=0.3232 GradNorm=2.1489 StepSize=0.0215 RelImp=85.93%\n",
            "Epoch[0/2] Step[3568] Loss=0.4227 GradNorm=4.3160 StepSize=0.0432 RelImp=81.59%\n",
            "Epoch[0/2] Step[3569] Loss=0.8796 GradNorm=4.9072 StepSize=0.0491 RelImp=61.70%\n",
            "Epoch[0/2] Step[3570] Loss=0.4227 GradNorm=3.7341 StepSize=0.0373 RelImp=81.59%\n",
            "Epoch[0/2] Step[3571] Loss=0.5851 GradNorm=3.7018 StepSize=0.0370 RelImp=74.52%\n",
            "Epoch[0/2] Step[3572] Loss=0.3856 GradNorm=3.5961 StepSize=0.0360 RelImp=83.21%\n",
            "Epoch[0/2] Step[3573] Loss=0.4224 GradNorm=3.2552 StepSize=0.0326 RelImp=81.61%\n",
            "Epoch[0/2] Step[3574] Loss=0.6515 GradNorm=2.0184 StepSize=0.0202 RelImp=71.63%\n",
            "Epoch[0/2] Step[3575] Loss=0.4500 GradNorm=3.5835 StepSize=0.0358 RelImp=80.41%\n",
            "Epoch[0/2] Step[3576] Loss=0.2233 GradNorm=3.1547 StepSize=0.0315 RelImp=90.28%\n",
            "Epoch[0/2] Step[3577] Loss=0.2377 GradNorm=2.5122 StepSize=0.0251 RelImp=89.65%\n",
            "Epoch[0/2] Step[3578] Loss=0.1948 GradNorm=2.6701 StepSize=0.0267 RelImp=91.52%\n",
            "Epoch[0/2] Step[3579] Loss=0.2225 GradNorm=2.6140 StepSize=0.0261 RelImp=90.31%\n",
            "Epoch[0/2] Step[3580] Loss=0.2337 GradNorm=3.2789 StepSize=0.0328 RelImp=89.83%\n",
            "Epoch[0/2] Step[3581] Loss=0.0927 GradNorm=1.3361 StepSize=0.0134 RelImp=95.96%\n",
            "Epoch[0/2] Step[3582] Loss=0.2075 GradNorm=2.3919 StepSize=0.0239 RelImp=90.96%\n",
            "Epoch[0/2] Step[3583] Loss=0.1980 GradNorm=2.3106 StepSize=0.0231 RelImp=91.38%\n",
            "Epoch[0/2] Step[3584] Loss=0.1214 GradNorm=1.7055 StepSize=0.0171 RelImp=94.71%\n",
            "Epoch[0/2] Step[3585] Loss=0.2108 GradNorm=2.2502 StepSize=0.0225 RelImp=90.82%\n",
            "Epoch[0/2] Step[3586] Loss=0.2037 GradNorm=2.7971 StepSize=0.0280 RelImp=91.13%\n",
            "Epoch[0/2] Step[3587] Loss=0.7962 GradNorm=3.7517 StepSize=0.0375 RelImp=65.33%\n",
            "Epoch[0/2] Step[3588] Loss=0.5825 GradNorm=3.3618 StepSize=0.0336 RelImp=74.64%\n",
            "Epoch[0/2] Step[3589] Loss=0.1945 GradNorm=1.7446 StepSize=0.0174 RelImp=91.53%\n",
            "Epoch[0/2] Step[3590] Loss=0.2880 GradNorm=3.5388 StepSize=0.0354 RelImp=87.46%\n",
            "Epoch[0/2] Step[3591] Loss=0.4514 GradNorm=4.1514 StepSize=0.0415 RelImp=80.34%\n",
            "Epoch[0/2] Step[3592] Loss=0.7135 GradNorm=5.0401 StepSize=0.0504 RelImp=68.93%\n",
            "Epoch[0/2] Step[3593] Loss=0.5729 GradNorm=3.8195 StepSize=0.0382 RelImp=75.06%\n",
            "Epoch[0/2] Step[3594] Loss=0.2633 GradNorm=3.2234 StepSize=0.0322 RelImp=88.53%\n",
            "Epoch[0/2] Step[3595] Loss=0.1518 GradNorm=2.2535 StepSize=0.0225 RelImp=93.39%\n",
            "Epoch[0/2] Step[3596] Loss=0.3520 GradNorm=4.3359 StepSize=0.0434 RelImp=84.67%\n",
            "Epoch[0/2] Step[3597] Loss=0.1656 GradNorm=2.9293 StepSize=0.0293 RelImp=92.79%\n",
            "Epoch[0/2] Step[3598] Loss=0.0479 GradNorm=0.7848 StepSize=0.0078 RelImp=97.91%\n",
            "Epoch[0/2] Step[3599] Loss=0.3641 GradNorm=3.1910 StepSize=0.0319 RelImp=84.14%\n",
            "Epoch[0/2] Step[3600] Loss=0.3488 GradNorm=3.2452 StepSize=0.0325 RelImp=84.81%\n",
            "Epoch[0/2] Step[3601] Loss=0.3353 GradNorm=2.6626 StepSize=0.0266 RelImp=85.40%\n",
            "Epoch[0/2] Step[3602] Loss=0.2437 GradNorm=1.8941 StepSize=0.0189 RelImp=89.39%\n",
            "Epoch[0/2] Step[3603] Loss=0.1302 GradNorm=1.6430 StepSize=0.0164 RelImp=94.33%\n",
            "Epoch[0/2] Step[3604] Loss=0.1216 GradNorm=1.4990 StepSize=0.0150 RelImp=94.71%\n",
            "Epoch[0/2] Step[3605] Loss=0.2047 GradNorm=2.2348 StepSize=0.0223 RelImp=91.09%\n",
            "Epoch[0/2] Step[3606] Loss=0.3596 GradNorm=1.9295 StepSize=0.0193 RelImp=84.34%\n",
            "Epoch[0/2] Step[3607] Loss=0.3179 GradNorm=2.5534 StepSize=0.0255 RelImp=86.16%\n",
            "Epoch[0/2] Step[3608] Loss=0.1764 GradNorm=2.7068 StepSize=0.0271 RelImp=92.32%\n",
            "Epoch[0/2] Step[3609] Loss=0.3731 GradNorm=3.4624 StepSize=0.0346 RelImp=83.75%\n",
            "Epoch[0/2] Step[3610] Loss=0.4117 GradNorm=3.9244 StepSize=0.0392 RelImp=82.07%\n",
            "Epoch[0/2] Step[3611] Loss=0.2349 GradNorm=1.6121 StepSize=0.0161 RelImp=89.77%\n",
            "Epoch[0/2] Step[3612] Loss=0.1608 GradNorm=2.6669 StepSize=0.0267 RelImp=93.00%\n",
            "Epoch[0/2] Step[3613] Loss=0.7915 GradNorm=3.7804 StepSize=0.0378 RelImp=65.53%\n",
            "Epoch[0/2] Step[3614] Loss=0.4864 GradNorm=3.5136 StepSize=0.0351 RelImp=78.82%\n",
            "Epoch[0/2] Step[3615] Loss=0.1240 GradNorm=1.4021 StepSize=0.0140 RelImp=94.60%\n",
            "Epoch[0/2] Step[3616] Loss=0.3750 GradNorm=2.8549 StepSize=0.0285 RelImp=83.67%\n",
            "Epoch[0/2] Step[3617] Loss=0.4401 GradNorm=1.4303 StepSize=0.0143 RelImp=80.84%\n",
            "Epoch[0/2] Step[3618] Loss=0.1409 GradNorm=1.7345 StepSize=0.0173 RelImp=93.86%\n",
            "Epoch[0/2] Step[3619] Loss=0.3162 GradNorm=2.3564 StepSize=0.0236 RelImp=86.23%\n",
            "Epoch[0/2] Step[3620] Loss=0.4746 GradNorm=3.5011 StepSize=0.0350 RelImp=79.33%\n",
            "Epoch[0/2] Step[3621] Loss=0.1948 GradNorm=2.4749 StepSize=0.0247 RelImp=91.52%\n",
            "Epoch[0/2] Step[3622] Loss=0.4338 GradNorm=3.4185 StepSize=0.0342 RelImp=81.11%\n",
            "Epoch[0/2] Step[3623] Loss=0.1734 GradNorm=2.7398 StepSize=0.0274 RelImp=92.45%\n",
            "Epoch[0/2] Step[3624] Loss=0.2009 GradNorm=2.2515 StepSize=0.0225 RelImp=91.25%\n",
            "Epoch[0/2] Step[3625] Loss=0.1216 GradNorm=1.7237 StepSize=0.0172 RelImp=94.70%\n",
            "Epoch[0/2] Step[3626] Loss=0.6089 GradNorm=4.3275 StepSize=0.0433 RelImp=73.49%\n",
            "Epoch[0/2] Step[3627] Loss=0.3142 GradNorm=2.7083 StepSize=0.0271 RelImp=86.32%\n",
            "Epoch[0/2] Step[3628] Loss=0.7406 GradNorm=3.6907 StepSize=0.0369 RelImp=67.75%\n",
            "Epoch[0/2] Step[3629] Loss=0.5810 GradNorm=5.5151 StepSize=0.0552 RelImp=74.70%\n",
            "Epoch[0/2] Step[3630] Loss=0.3211 GradNorm=2.9370 StepSize=0.0294 RelImp=86.02%\n",
            "Epoch[0/2] Step[3631] Loss=0.6595 GradNorm=5.2060 StepSize=0.0521 RelImp=71.29%\n",
            "Epoch[0/2] Step[3632] Loss=0.1154 GradNorm=1.3738 StepSize=0.0137 RelImp=94.98%\n",
            "Epoch[0/2] Step[3633] Loss=0.1783 GradNorm=2.6274 StepSize=0.0263 RelImp=92.24%\n",
            "Epoch[0/2] Step[3634] Loss=0.4732 GradNorm=4.2171 StepSize=0.0422 RelImp=79.39%\n",
            "Epoch[0/2] Step[3635] Loss=0.5620 GradNorm=3.9952 StepSize=0.0400 RelImp=75.53%\n",
            "Epoch[0/2] Step[3636] Loss=0.5919 GradNorm=3.7513 StepSize=0.0375 RelImp=74.23%\n",
            "Epoch[0/2] Step[3637] Loss=0.3460 GradNorm=2.4319 StepSize=0.0243 RelImp=84.93%\n",
            "Epoch[0/2] Step[3638] Loss=0.4765 GradNorm=3.3481 StepSize=0.0335 RelImp=79.25%\n",
            "Epoch[0/2] Step[3639] Loss=0.3513 GradNorm=2.2679 StepSize=0.0227 RelImp=84.70%\n",
            "Epoch[0/2] Step[3640] Loss=0.0405 GradNorm=0.5229 StepSize=0.0052 RelImp=98.24%\n",
            "Epoch[0/2] Step[3641] Loss=0.3097 GradNorm=2.7969 StepSize=0.0280 RelImp=86.52%\n",
            "Epoch[0/2] Step[3642] Loss=0.6648 GradNorm=4.6946 StepSize=0.0469 RelImp=71.05%\n",
            "Epoch[0/2] Step[3643] Loss=0.2381 GradNorm=2.8297 StepSize=0.0283 RelImp=89.63%\n",
            "Epoch[0/2] Step[3644] Loss=0.5039 GradNorm=3.9706 StepSize=0.0397 RelImp=78.06%\n",
            "Epoch[0/2] Step[3645] Loss=0.1696 GradNorm=1.5081 StepSize=0.0151 RelImp=92.62%\n",
            "Epoch[0/2] Step[3646] Loss=0.4405 GradNorm=3.2420 StepSize=0.0324 RelImp=80.82%\n",
            "Epoch[0/2] Step[3647] Loss=0.5092 GradNorm=5.1104 StepSize=0.0511 RelImp=77.83%\n",
            "Epoch[0/2] Step[3648] Loss=0.3941 GradNorm=2.6266 StepSize=0.0263 RelImp=82.84%\n",
            "Epoch[0/2] Step[3649] Loss=0.1286 GradNorm=1.7229 StepSize=0.0172 RelImp=94.40%\n",
            "Epoch[0/2] Step[3650] Loss=0.6608 GradNorm=4.0184 StepSize=0.0402 RelImp=71.23%\n",
            "Epoch[0/2] Step[3651] Loss=0.5908 GradNorm=3.7114 StepSize=0.0371 RelImp=74.28%\n",
            "Epoch[0/2] Step[3652] Loss=0.3914 GradNorm=2.2646 StepSize=0.0226 RelImp=82.96%\n",
            "Epoch[0/2] Step[3653] Loss=0.3513 GradNorm=3.6378 StepSize=0.0364 RelImp=84.70%\n",
            "Epoch[0/2] Step[3654] Loss=0.3078 GradNorm=2.8734 StepSize=0.0287 RelImp=86.60%\n",
            "Epoch[0/2] Step[3655] Loss=0.0990 GradNorm=0.6477 StepSize=0.0065 RelImp=95.69%\n",
            "Epoch[0/2] Step[3656] Loss=0.1125 GradNorm=1.3915 StepSize=0.0139 RelImp=95.10%\n",
            "Epoch[0/2] Step[3657] Loss=0.8295 GradNorm=4.3979 StepSize=0.0440 RelImp=63.88%\n",
            "Epoch[0/2] Step[3658] Loss=0.1744 GradNorm=2.0401 StepSize=0.0204 RelImp=92.41%\n",
            "Epoch[0/2] Step[3659] Loss=0.3527 GradNorm=2.3449 StepSize=0.0234 RelImp=84.64%\n",
            "Epoch[0/2] Step[3660] Loss=0.7563 GradNorm=3.8504 StepSize=0.0385 RelImp=67.07%\n",
            "Epoch[0/2] Step[3661] Loss=0.1191 GradNorm=1.5235 StepSize=0.0152 RelImp=94.81%\n",
            "Epoch[0/2] Step[3662] Loss=0.4567 GradNorm=2.1533 StepSize=0.0215 RelImp=80.11%\n",
            "Epoch[0/2] Step[3663] Loss=0.2571 GradNorm=2.9465 StepSize=0.0295 RelImp=88.80%\n",
            "Epoch[0/2] Step[3664] Loss=0.3842 GradNorm=2.9230 StepSize=0.0292 RelImp=83.27%\n",
            "Epoch[0/2] Step[3665] Loss=0.1269 GradNorm=2.0785 StepSize=0.0208 RelImp=94.47%\n",
            "Epoch[0/2] Step[3666] Loss=0.4615 GradNorm=3.4935 StepSize=0.0349 RelImp=79.90%\n",
            "Epoch[0/2] Step[3667] Loss=0.7254 GradNorm=4.3012 StepSize=0.0430 RelImp=68.42%\n",
            "Epoch[0/2] Step[3668] Loss=0.2027 GradNorm=2.7481 StepSize=0.0275 RelImp=91.18%\n",
            "Epoch[0/2] Step[3669] Loss=0.1413 GradNorm=1.8094 StepSize=0.0181 RelImp=93.85%\n",
            "Epoch[0/2] Step[3670] Loss=0.3914 GradNorm=3.1724 StepSize=0.0317 RelImp=82.96%\n",
            "Epoch[0/2] Step[3671] Loss=0.4226 GradNorm=4.1029 StepSize=0.0410 RelImp=81.60%\n",
            "Epoch[0/2] Step[3672] Loss=0.2090 GradNorm=2.0182 StepSize=0.0202 RelImp=90.90%\n",
            "Epoch[0/2] Step[3673] Loss=0.1279 GradNorm=1.4996 StepSize=0.0150 RelImp=94.43%\n",
            "Epoch[0/2] Step[3674] Loss=0.2986 GradNorm=3.3531 StepSize=0.0335 RelImp=87.00%\n",
            "Epoch[0/2] Step[3675] Loss=0.2064 GradNorm=2.0506 StepSize=0.0205 RelImp=91.01%\n",
            "Epoch[0/2] Step[3676] Loss=0.6405 GradNorm=3.3337 StepSize=0.0333 RelImp=72.11%\n",
            "Epoch[0/2] Step[3677] Loss=0.1631 GradNorm=1.5621 StepSize=0.0156 RelImp=92.90%\n",
            "Epoch[0/2] Step[3678] Loss=0.1175 GradNorm=2.3672 StepSize=0.0237 RelImp=94.89%\n",
            "Epoch[0/2] Step[3679] Loss=0.3417 GradNorm=1.9765 StepSize=0.0198 RelImp=85.12%\n",
            "Epoch[0/2] Step[3680] Loss=0.6937 GradNorm=3.7921 StepSize=0.0379 RelImp=69.79%\n",
            "Epoch[0/2] Step[3681] Loss=0.2663 GradNorm=3.6100 StepSize=0.0361 RelImp=88.41%\n",
            "Epoch[0/2] Step[3682] Loss=0.1308 GradNorm=1.6837 StepSize=0.0168 RelImp=94.30%\n",
            "Epoch[0/2] Step[3683] Loss=0.2323 GradNorm=1.7172 StepSize=0.0172 RelImp=89.88%\n",
            "Epoch[0/2] Step[3684] Loss=0.2396 GradNorm=2.3905 StepSize=0.0239 RelImp=89.57%\n",
            "Epoch[0/2] Step[3685] Loss=0.2852 GradNorm=2.6577 StepSize=0.0266 RelImp=87.58%\n",
            "Epoch[0/2] Step[3686] Loss=0.1358 GradNorm=1.5228 StepSize=0.0152 RelImp=94.09%\n",
            "Epoch[0/2] Step[3687] Loss=0.5083 GradNorm=6.1488 StepSize=0.0615 RelImp=77.87%\n",
            "Epoch[0/2] Step[3688] Loss=0.3262 GradNorm=3.4315 StepSize=0.0343 RelImp=85.80%\n",
            "Epoch[0/2] Step[3689] Loss=0.3574 GradNorm=4.2099 StepSize=0.0421 RelImp=84.44%\n",
            "Epoch[0/2] Step[3690] Loss=0.9225 GradNorm=4.5814 StepSize=0.0458 RelImp=59.83%\n",
            "Epoch[0/2] Step[3691] Loss=0.3174 GradNorm=3.7874 StepSize=0.0379 RelImp=86.18%\n",
            "Epoch[0/2] Step[3692] Loss=0.3913 GradNorm=3.3453 StepSize=0.0335 RelImp=82.96%\n",
            "Epoch[0/2] Step[3693] Loss=0.6974 GradNorm=3.6522 StepSize=0.0365 RelImp=69.63%\n",
            "Epoch[0/2] Step[3694] Loss=0.3744 GradNorm=3.5274 StepSize=0.0353 RelImp=83.70%\n",
            "Epoch[0/2] Step[3695] Loss=0.6606 GradNorm=4.3352 StepSize=0.0434 RelImp=71.24%\n",
            "Epoch[0/2] Step[3696] Loss=0.5388 GradNorm=4.4293 StepSize=0.0443 RelImp=76.54%\n",
            "Epoch[0/2] Step[3697] Loss=0.7820 GradNorm=4.4470 StepSize=0.0445 RelImp=65.95%\n",
            "Epoch[0/2] Step[3698] Loss=0.1972 GradNorm=2.4708 StepSize=0.0247 RelImp=91.41%\n",
            "Epoch[0/2] Step[3699] Loss=0.1442 GradNorm=1.5111 StepSize=0.0151 RelImp=93.72%\n",
            "Epoch[0/2] Step[3700] Loss=0.2091 GradNorm=3.3741 StepSize=0.0337 RelImp=90.89%\n",
            "Epoch[0/2] Step[3701] Loss=0.7857 GradNorm=5.0112 StepSize=0.0501 RelImp=65.79%\n",
            "Epoch[0/2] Step[3702] Loss=0.4809 GradNorm=5.2463 StepSize=0.0525 RelImp=79.06%\n",
            "Epoch[0/2] Step[3703] Loss=0.3745 GradNorm=3.5310 StepSize=0.0353 RelImp=83.69%\n",
            "Epoch[0/2] Step[3704] Loss=0.2754 GradNorm=3.6282 StepSize=0.0363 RelImp=88.01%\n",
            "Epoch[0/2] Step[3705] Loss=0.5897 GradNorm=4.2532 StepSize=0.0425 RelImp=74.32%\n",
            "Epoch[0/2] Step[3706] Loss=0.7631 GradNorm=4.6559 StepSize=0.0466 RelImp=66.77%\n",
            "Epoch[0/2] Step[3707] Loss=0.2653 GradNorm=2.5249 StepSize=0.0252 RelImp=88.45%\n",
            "Epoch[0/2] Step[3708] Loss=0.1605 GradNorm=1.7926 StepSize=0.0179 RelImp=93.01%\n",
            "Epoch[0/2] Step[3709] Loss=0.1435 GradNorm=1.3775 StepSize=0.0138 RelImp=93.75%\n",
            "Epoch[0/2] Step[3710] Loss=0.2535 GradNorm=2.3963 StepSize=0.0240 RelImp=88.96%\n",
            "Epoch[0/2] Step[3711] Loss=0.4090 GradNorm=3.8579 StepSize=0.0386 RelImp=82.19%\n",
            "Epoch[0/2] Step[3712] Loss=0.2237 GradNorm=1.7659 StepSize=0.0177 RelImp=90.26%\n",
            "Epoch[0/2] Step[3713] Loss=0.0520 GradNorm=0.7610 StepSize=0.0076 RelImp=97.73%\n",
            "Epoch[0/2] Step[3714] Loss=0.5915 GradNorm=3.8706 StepSize=0.0387 RelImp=74.25%\n",
            "Epoch[0/2] Step[3715] Loss=0.3601 GradNorm=2.7761 StepSize=0.0278 RelImp=84.32%\n",
            "Epoch[0/2] Step[3716] Loss=0.4697 GradNorm=4.9110 StepSize=0.0491 RelImp=79.55%\n",
            "Epoch[0/2] Step[3717] Loss=0.2563 GradNorm=2.6066 StepSize=0.0261 RelImp=88.84%\n",
            "Epoch[0/2] Step[3718] Loss=0.5148 GradNorm=4.5812 StepSize=0.0458 RelImp=77.59%\n",
            "Epoch[0/2] Step[3719] Loss=0.9469 GradNorm=4.3475 StepSize=0.0435 RelImp=58.77%\n",
            "Epoch[0/2] Step[3720] Loss=0.1935 GradNorm=2.8287 StepSize=0.0283 RelImp=91.57%\n",
            "Epoch[0/2] Step[3721] Loss=0.3273 GradNorm=2.5489 StepSize=0.0255 RelImp=85.75%\n",
            "Epoch[0/2] Step[3722] Loss=0.2536 GradNorm=2.2031 StepSize=0.0220 RelImp=88.96%\n",
            "Epoch[0/2] Step[3723] Loss=0.3906 GradNorm=3.4466 StepSize=0.0345 RelImp=82.99%\n",
            "Epoch[0/2] Step[3724] Loss=0.4116 GradNorm=2.8924 StepSize=0.0289 RelImp=82.08%\n",
            "Epoch[0/2] Step[3725] Loss=0.0967 GradNorm=1.2590 StepSize=0.0126 RelImp=95.79%\n",
            "Epoch[0/2] Step[3726] Loss=0.1253 GradNorm=1.5773 StepSize=0.0158 RelImp=94.54%\n",
            "Epoch[0/2] Step[3727] Loss=0.3855 GradNorm=3.1154 StepSize=0.0312 RelImp=83.21%\n",
            "Epoch[0/2] Step[3728] Loss=0.2126 GradNorm=2.3035 StepSize=0.0230 RelImp=90.74%\n",
            "Epoch[0/2] Step[3729] Loss=0.0644 GradNorm=1.1876 StepSize=0.0119 RelImp=97.20%\n",
            "Epoch[0/2] Step[3730] Loss=0.2630 GradNorm=3.5955 StepSize=0.0360 RelImp=88.55%\n",
            "Epoch[0/2] Step[3731] Loss=0.2074 GradNorm=3.1939 StepSize=0.0319 RelImp=90.97%\n",
            "Epoch[0/2] Step[3732] Loss=0.4846 GradNorm=3.0688 StepSize=0.0307 RelImp=78.90%\n",
            "Epoch[0/2] Step[3733] Loss=0.1944 GradNorm=2.4143 StepSize=0.0241 RelImp=91.53%\n",
            "Epoch[0/2] Step[3734] Loss=0.6097 GradNorm=4.8561 StepSize=0.0486 RelImp=73.45%\n",
            "Epoch[0/2] Step[3735] Loss=0.7138 GradNorm=5.3378 StepSize=0.0534 RelImp=68.92%\n",
            "Epoch[0/2] Step[3736] Loss=0.2386 GradNorm=2.8131 StepSize=0.0281 RelImp=89.61%\n",
            "Epoch[0/2] Step[3737] Loss=0.5137 GradNorm=3.7346 StepSize=0.0373 RelImp=77.63%\n",
            "Epoch[0/2] Step[3738] Loss=0.1298 GradNorm=1.3739 StepSize=0.0137 RelImp=94.35%\n",
            "Epoch[0/2] Step[3739] Loss=0.6163 GradNorm=2.9605 StepSize=0.0296 RelImp=73.16%\n",
            "Epoch[0/2] Step[3740] Loss=0.6545 GradNorm=4.0489 StepSize=0.0405 RelImp=71.50%\n",
            "Epoch[0/2] Step[3741] Loss=0.7830 GradNorm=5.9250 StepSize=0.0592 RelImp=65.90%\n",
            "Epoch[0/2] Step[3742] Loss=0.5300 GradNorm=3.3848 StepSize=0.0338 RelImp=76.92%\n",
            "Epoch[0/2] Step[3743] Loss=0.3731 GradNorm=2.1717 StepSize=0.0217 RelImp=83.75%\n",
            "Epoch[0/2] Step[3744] Loss=0.2334 GradNorm=2.5380 StepSize=0.0254 RelImp=89.84%\n",
            "Epoch[0/2] Step[3745] Loss=0.8723 GradNorm=3.8262 StepSize=0.0383 RelImp=62.02%\n",
            "Epoch[0/2] Step[3746] Loss=0.1874 GradNorm=1.8948 StepSize=0.0189 RelImp=91.84%\n",
            "Epoch[0/2] Step[3747] Loss=0.5381 GradNorm=4.3875 StepSize=0.0439 RelImp=76.57%\n",
            "Epoch[0/2] Step[3748] Loss=0.6512 GradNorm=3.2590 StepSize=0.0326 RelImp=71.64%\n",
            "Epoch[0/2] Step[3749] Loss=0.1822 GradNorm=1.4811 StepSize=0.0148 RelImp=92.07%\n",
            "Epoch[0/2] Step[3750] Loss=0.2108 GradNorm=2.0623 StepSize=0.0206 RelImp=90.82%\n",
            "Epoch 1/2 => TrainLoss=0.7581 TestLoss=0.3514 Acc=89.56%\n",
            "Epoch[1/2] Step[3751] Loss=0.4871 GradNorm=4.1420 StepSize=0.0414 RelImp=78.79%\n",
            "Epoch[1/2] Step[3752] Loss=0.5083 GradNorm=3.3053 StepSize=0.0331 RelImp=77.87%\n",
            "Epoch[1/2] Step[3753] Loss=0.5565 GradNorm=4.3044 StepSize=0.0430 RelImp=75.77%\n",
            "Epoch[1/2] Step[3754] Loss=0.3565 GradNorm=3.7052 StepSize=0.0371 RelImp=84.48%\n",
            "Epoch[1/2] Step[3755] Loss=0.7071 GradNorm=4.7047 StepSize=0.0470 RelImp=69.21%\n",
            "Epoch[1/2] Step[3756] Loss=0.2997 GradNorm=3.9821 StepSize=0.0398 RelImp=86.95%\n",
            "Epoch[1/2] Step[3757] Loss=0.4492 GradNorm=4.3628 StepSize=0.0436 RelImp=80.44%\n",
            "Epoch[1/2] Step[3758] Loss=0.2669 GradNorm=2.4368 StepSize=0.0244 RelImp=88.38%\n",
            "Epoch[1/2] Step[3759] Loss=0.3730 GradNorm=3.6684 StepSize=0.0367 RelImp=83.76%\n",
            "Epoch[1/2] Step[3760] Loss=0.2836 GradNorm=2.8967 StepSize=0.0290 RelImp=87.65%\n",
            "Epoch[1/2] Step[3761] Loss=0.1373 GradNorm=1.3619 StepSize=0.0136 RelImp=94.02%\n",
            "Epoch[1/2] Step[3762] Loss=0.2526 GradNorm=2.8232 StepSize=0.0282 RelImp=89.00%\n",
            "Epoch[1/2] Step[3763] Loss=0.2933 GradNorm=2.3215 StepSize=0.0232 RelImp=87.23%\n",
            "Epoch[1/2] Step[3764] Loss=0.5899 GradNorm=3.9481 StepSize=0.0395 RelImp=74.31%\n",
            "Epoch[1/2] Step[3765] Loss=0.2517 GradNorm=3.0986 StepSize=0.0310 RelImp=89.04%\n",
            "Epoch[1/2] Step[3766] Loss=0.4527 GradNorm=4.6355 StepSize=0.0464 RelImp=80.29%\n",
            "Epoch[1/2] Step[3767] Loss=0.1980 GradNorm=3.1495 StepSize=0.0315 RelImp=91.38%\n",
            "Epoch[1/2] Step[3768] Loss=0.2442 GradNorm=3.0609 StepSize=0.0306 RelImp=89.37%\n",
            "Epoch[1/2] Step[3769] Loss=0.1049 GradNorm=1.1915 StepSize=0.0119 RelImp=95.43%\n",
            "Epoch[1/2] Step[3770] Loss=0.3199 GradNorm=4.4857 StepSize=0.0449 RelImp=86.07%\n",
            "Epoch[1/2] Step[3771] Loss=0.0948 GradNorm=1.2375 StepSize=0.0124 RelImp=95.87%\n",
            "Epoch[1/2] Step[3772] Loss=0.5204 GradNorm=5.1081 StepSize=0.0511 RelImp=77.34%\n",
            "Epoch[1/2] Step[3773] Loss=0.5972 GradNorm=3.7684 StepSize=0.0377 RelImp=74.00%\n",
            "Epoch[1/2] Step[3774] Loss=0.2624 GradNorm=3.2295 StepSize=0.0323 RelImp=88.57%\n",
            "Epoch[1/2] Step[3775] Loss=0.3610 GradNorm=3.1015 StepSize=0.0310 RelImp=84.28%\n",
            "Epoch[1/2] Step[3776] Loss=0.0890 GradNorm=1.3087 StepSize=0.0131 RelImp=96.12%\n",
            "Epoch[1/2] Step[3777] Loss=0.5785 GradNorm=5.4175 StepSize=0.0542 RelImp=74.81%\n",
            "Epoch[1/2] Step[3778] Loss=0.8025 GradNorm=4.2547 StepSize=0.0425 RelImp=65.06%\n",
            "Epoch[1/2] Step[3779] Loss=0.7203 GradNorm=3.7615 StepSize=0.0376 RelImp=68.63%\n",
            "Epoch[1/2] Step[3780] Loss=0.2052 GradNorm=2.7760 StepSize=0.0278 RelImp=91.06%\n",
            "Epoch[1/2] Step[3781] Loss=0.3426 GradNorm=3.1730 StepSize=0.0317 RelImp=85.08%\n",
            "Epoch[1/2] Step[3782] Loss=0.3669 GradNorm=2.9947 StepSize=0.0299 RelImp=84.02%\n",
            "Epoch[1/2] Step[3783] Loss=0.2696 GradNorm=3.1632 StepSize=0.0316 RelImp=88.26%\n",
            "Epoch[1/2] Step[3784] Loss=0.3179 GradNorm=2.9053 StepSize=0.0291 RelImp=86.16%\n",
            "Epoch[1/2] Step[3785] Loss=0.3084 GradNorm=4.4020 StepSize=0.0440 RelImp=86.57%\n",
            "Epoch[1/2] Step[3786] Loss=0.4352 GradNorm=5.2557 StepSize=0.0526 RelImp=81.05%\n",
            "Epoch[1/2] Step[3787] Loss=0.9151 GradNorm=4.8120 StepSize=0.0481 RelImp=60.15%\n",
            "Epoch[1/2] Step[3788] Loss=0.3839 GradNorm=3.2328 StepSize=0.0323 RelImp=83.28%\n",
            "Epoch[1/2] Step[3789] Loss=0.1927 GradNorm=1.9122 StepSize=0.0191 RelImp=91.61%\n",
            "Epoch[1/2] Step[3790] Loss=0.5405 GradNorm=3.8586 StepSize=0.0386 RelImp=76.47%\n",
            "Epoch[1/2] Step[3791] Loss=0.5170 GradNorm=5.4872 StepSize=0.0549 RelImp=77.49%\n",
            "Epoch[1/2] Step[3792] Loss=0.3122 GradNorm=4.4284 StepSize=0.0443 RelImp=86.40%\n",
            "Epoch[1/2] Step[3793] Loss=0.2948 GradNorm=2.8613 StepSize=0.0286 RelImp=87.16%\n",
            "Epoch[1/2] Step[3794] Loss=0.1560 GradNorm=2.1967 StepSize=0.0220 RelImp=93.21%\n",
            "Epoch[1/2] Step[3795] Loss=0.3910 GradNorm=2.9769 StepSize=0.0298 RelImp=82.97%\n",
            "Epoch[1/2] Step[3796] Loss=1.0419 GradNorm=6.2711 StepSize=0.0627 RelImp=54.63%\n",
            "Epoch[1/2] Step[3797] Loss=0.7283 GradNorm=5.0125 StepSize=0.0501 RelImp=68.29%\n",
            "Epoch[1/2] Step[3798] Loss=0.3506 GradNorm=4.4816 StepSize=0.0448 RelImp=84.73%\n",
            "Epoch[1/2] Step[3799] Loss=0.1796 GradNorm=2.2612 StepSize=0.0226 RelImp=92.18%\n",
            "Epoch[1/2] Step[3800] Loss=0.4818 GradNorm=5.7517 StepSize=0.0575 RelImp=79.02%\n",
            "Epoch[1/2] Step[3801] Loss=0.2211 GradNorm=2.3401 StepSize=0.0234 RelImp=90.37%\n",
            "Epoch[1/2] Step[3802] Loss=0.7899 GradNorm=5.3725 StepSize=0.0537 RelImp=65.60%\n",
            "Epoch[1/2] Step[3803] Loss=0.4158 GradNorm=4.2188 StepSize=0.0422 RelImp=81.90%\n",
            "Epoch[1/2] Step[3804] Loss=0.5067 GradNorm=4.0159 StepSize=0.0402 RelImp=77.94%\n",
            "Epoch[1/2] Step[3805] Loss=0.1212 GradNorm=1.1281 StepSize=0.0113 RelImp=94.72%\n",
            "Epoch[1/2] Step[3806] Loss=0.7050 GradNorm=3.3186 StepSize=0.0332 RelImp=69.30%\n",
            "Epoch[1/2] Step[3807] Loss=0.1276 GradNorm=1.0045 StepSize=0.0100 RelImp=94.45%\n",
            "Epoch[1/2] Step[3808] Loss=0.7567 GradNorm=3.2033 StepSize=0.0320 RelImp=67.05%\n",
            "Epoch[1/2] Step[3809] Loss=0.3190 GradNorm=2.3946 StepSize=0.0239 RelImp=86.11%\n",
            "Epoch[1/2] Step[3810] Loss=0.0610 GradNorm=0.5253 StepSize=0.0053 RelImp=97.34%\n",
            "Epoch[1/2] Step[3811] Loss=0.7200 GradNorm=6.9986 StepSize=0.0700 RelImp=68.65%\n",
            "Epoch[1/2] Step[3812] Loss=0.3283 GradNorm=2.6601 StepSize=0.0266 RelImp=85.70%\n",
            "Epoch[1/2] Step[3813] Loss=0.0367 GradNorm=0.5806 StepSize=0.0058 RelImp=98.40%\n",
            "Epoch[1/2] Step[3814] Loss=0.3468 GradNorm=2.7107 StepSize=0.0271 RelImp=84.90%\n",
            "Epoch[1/2] Step[3815] Loss=0.6855 GradNorm=3.7554 StepSize=0.0376 RelImp=70.15%\n",
            "Epoch[1/2] Step[3816] Loss=0.1825 GradNorm=2.3659 StepSize=0.0237 RelImp=92.05%\n",
            "Epoch[1/2] Step[3817] Loss=0.4824 GradNorm=2.7339 StepSize=0.0273 RelImp=79.00%\n",
            "Epoch[1/2] Step[3818] Loss=0.6219 GradNorm=2.5005 StepSize=0.0250 RelImp=72.92%\n",
            "Epoch[1/2] Step[3819] Loss=0.4458 GradNorm=3.9767 StepSize=0.0398 RelImp=80.59%\n",
            "Epoch[1/2] Step[3820] Loss=0.3780 GradNorm=4.4487 StepSize=0.0445 RelImp=83.54%\n",
            "Epoch[1/2] Step[3821] Loss=0.4122 GradNorm=2.1149 StepSize=0.0211 RelImp=82.05%\n",
            "Epoch[1/2] Step[3822] Loss=0.2805 GradNorm=2.7914 StepSize=0.0279 RelImp=87.79%\n",
            "Epoch[1/2] Step[3823] Loss=0.5091 GradNorm=2.6102 StepSize=0.0261 RelImp=77.83%\n",
            "Epoch[1/2] Step[3824] Loss=0.2047 GradNorm=1.9931 StepSize=0.0199 RelImp=91.09%\n",
            "Epoch[1/2] Step[3825] Loss=0.6762 GradNorm=4.1156 StepSize=0.0412 RelImp=70.55%\n",
            "Epoch[1/2] Step[3826] Loss=0.6084 GradNorm=4.4953 StepSize=0.0450 RelImp=73.51%\n",
            "Epoch[1/2] Step[3827] Loss=0.4647 GradNorm=2.1443 StepSize=0.0214 RelImp=79.77%\n",
            "Epoch[1/2] Step[3828] Loss=0.8494 GradNorm=4.4842 StepSize=0.0448 RelImp=63.01%\n",
            "Epoch[1/2] Step[3829] Loss=0.1430 GradNorm=2.0764 StepSize=0.0208 RelImp=93.78%\n",
            "Epoch[1/2] Step[3830] Loss=0.6534 GradNorm=4.5219 StepSize=0.0452 RelImp=71.55%\n",
            "Epoch[1/2] Step[3831] Loss=0.4551 GradNorm=2.6786 StepSize=0.0268 RelImp=80.18%\n",
            "Epoch[1/2] Step[3832] Loss=0.1883 GradNorm=2.3662 StepSize=0.0237 RelImp=91.80%\n",
            "Epoch[1/2] Step[3833] Loss=0.3194 GradNorm=2.4900 StepSize=0.0249 RelImp=86.09%\n",
            "Epoch[1/2] Step[3834] Loss=0.3361 GradNorm=2.8059 StepSize=0.0281 RelImp=85.36%\n",
            "Epoch[1/2] Step[3835] Loss=0.0994 GradNorm=1.0540 StepSize=0.0105 RelImp=95.67%\n",
            "Epoch[1/2] Step[3836] Loss=0.0963 GradNorm=1.3655 StepSize=0.0137 RelImp=95.81%\n",
            "Epoch[1/2] Step[3837] Loss=0.2772 GradNorm=2.3650 StepSize=0.0237 RelImp=87.93%\n",
            "Epoch[1/2] Step[3838] Loss=0.4320 GradNorm=3.5180 StepSize=0.0352 RelImp=81.19%\n",
            "Epoch[1/2] Step[3839] Loss=0.6411 GradNorm=3.5304 StepSize=0.0353 RelImp=72.08%\n",
            "Epoch[1/2] Step[3840] Loss=0.3363 GradNorm=3.2779 StepSize=0.0328 RelImp=85.36%\n",
            "Epoch[1/2] Step[3841] Loss=0.1845 GradNorm=2.4999 StepSize=0.0250 RelImp=91.97%\n",
            "Epoch[1/2] Step[3842] Loss=0.3660 GradNorm=3.1441 StepSize=0.0314 RelImp=84.06%\n",
            "Epoch[1/2] Step[3843] Loss=0.9314 GradNorm=6.6361 StepSize=0.0664 RelImp=59.44%\n",
            "Epoch[1/2] Step[3844] Loss=0.1248 GradNorm=1.7410 StepSize=0.0174 RelImp=94.57%\n",
            "Epoch[1/2] Step[3845] Loss=0.2056 GradNorm=2.7499 StepSize=0.0275 RelImp=91.05%\n",
            "Epoch[1/2] Step[3846] Loss=0.3735 GradNorm=4.5475 StepSize=0.0455 RelImp=83.74%\n",
            "Epoch[1/2] Step[3847] Loss=0.2985 GradNorm=2.7243 StepSize=0.0272 RelImp=87.00%\n",
            "Epoch[1/2] Step[3848] Loss=0.1126 GradNorm=1.6300 StepSize=0.0163 RelImp=95.10%\n",
            "Epoch[1/2] Step[3849] Loss=0.4144 GradNorm=3.2081 StepSize=0.0321 RelImp=81.96%\n",
            "Epoch[1/2] Step[3850] Loss=0.0640 GradNorm=0.5022 StepSize=0.0050 RelImp=97.21%\n",
            "Epoch[1/2] Step[3851] Loss=0.2195 GradNorm=3.0647 StepSize=0.0306 RelImp=90.44%\n",
            "Epoch[1/2] Step[3852] Loss=0.4288 GradNorm=4.4413 StepSize=0.0444 RelImp=81.33%\n",
            "Epoch[1/2] Step[3853] Loss=0.5888 GradNorm=4.9606 StepSize=0.0496 RelImp=74.36%\n",
            "Epoch[1/2] Step[3854] Loss=0.5370 GradNorm=3.5120 StepSize=0.0351 RelImp=76.62%\n",
            "Epoch[1/2] Step[3855] Loss=0.6267 GradNorm=6.3001 StepSize=0.0630 RelImp=72.71%\n",
            "Epoch[1/2] Step[3856] Loss=0.1203 GradNorm=1.8059 StepSize=0.0181 RelImp=94.76%\n",
            "Epoch[1/2] Step[3857] Loss=0.2841 GradNorm=3.0968 StepSize=0.0310 RelImp=87.63%\n",
            "Epoch[1/2] Step[3858] Loss=0.2715 GradNorm=2.5079 StepSize=0.0251 RelImp=88.18%\n",
            "Epoch[1/2] Step[3859] Loss=0.1254 GradNorm=1.5266 StepSize=0.0153 RelImp=94.54%\n",
            "Epoch[1/2] Step[3860] Loss=0.4796 GradNorm=3.1799 StepSize=0.0318 RelImp=79.12%\n",
            "Epoch[1/2] Step[3861] Loss=0.2742 GradNorm=4.8007 StepSize=0.0480 RelImp=88.06%\n",
            "Epoch[1/2] Step[3862] Loss=0.3692 GradNorm=2.2669 StepSize=0.0227 RelImp=83.93%\n",
            "Epoch[1/2] Step[3863] Loss=0.2453 GradNorm=3.5034 StepSize=0.0350 RelImp=89.32%\n",
            "Epoch[1/2] Step[3864] Loss=0.1497 GradNorm=1.1309 StepSize=0.0113 RelImp=93.48%\n",
            "Epoch[1/2] Step[3865] Loss=0.5624 GradNorm=4.8071 StepSize=0.0481 RelImp=75.51%\n",
            "Epoch[1/2] Step[3866] Loss=0.7291 GradNorm=4.0536 StepSize=0.0405 RelImp=68.25%\n",
            "Epoch[1/2] Step[3867] Loss=0.1215 GradNorm=1.4022 StepSize=0.0140 RelImp=94.71%\n",
            "Epoch[1/2] Step[3868] Loss=0.1414 GradNorm=1.7206 StepSize=0.0172 RelImp=93.84%\n",
            "Epoch[1/2] Step[3869] Loss=0.4580 GradNorm=4.1133 StepSize=0.0411 RelImp=80.06%\n",
            "Epoch[1/2] Step[3870] Loss=0.2556 GradNorm=3.6932 StepSize=0.0369 RelImp=88.87%\n",
            "Epoch[1/2] Step[3871] Loss=0.1614 GradNorm=2.3819 StepSize=0.0238 RelImp=92.97%\n",
            "Epoch[1/2] Step[3872] Loss=0.1342 GradNorm=2.3021 StepSize=0.0230 RelImp=94.16%\n",
            "Epoch[1/2] Step[3873] Loss=0.1172 GradNorm=1.1241 StepSize=0.0112 RelImp=94.90%\n",
            "Epoch[1/2] Step[3874] Loss=0.2688 GradNorm=2.7277 StepSize=0.0273 RelImp=88.29%\n",
            "Epoch[1/2] Step[3875] Loss=0.4063 GradNorm=2.6763 StepSize=0.0268 RelImp=82.31%\n",
            "Epoch[1/2] Step[3876] Loss=0.4052 GradNorm=3.5262 StepSize=0.0353 RelImp=82.36%\n",
            "Epoch[1/2] Step[3877] Loss=0.3559 GradNorm=3.5287 StepSize=0.0353 RelImp=84.50%\n",
            "Epoch[1/2] Step[3878] Loss=0.1403 GradNorm=2.0019 StepSize=0.0200 RelImp=93.89%\n",
            "Epoch[1/2] Step[3879] Loss=0.9740 GradNorm=5.5341 StepSize=0.0553 RelImp=57.59%\n",
            "Epoch[1/2] Step[3880] Loss=0.2334 GradNorm=2.8155 StepSize=0.0282 RelImp=89.84%\n",
            "Epoch[1/2] Step[3881] Loss=0.1181 GradNorm=1.5983 StepSize=0.0160 RelImp=94.86%\n",
            "Epoch[1/2] Step[3882] Loss=0.3623 GradNorm=4.8137 StepSize=0.0481 RelImp=84.23%\n",
            "Epoch[1/2] Step[3883] Loss=0.1390 GradNorm=2.3400 StepSize=0.0234 RelImp=93.95%\n",
            "Epoch[1/2] Step[3884] Loss=0.3781 GradNorm=2.8671 StepSize=0.0287 RelImp=83.54%\n",
            "Epoch[1/2] Step[3885] Loss=0.1836 GradNorm=2.0656 StepSize=0.0207 RelImp=92.01%\n",
            "Epoch[1/2] Step[3886] Loss=0.4412 GradNorm=2.6233 StepSize=0.0262 RelImp=80.79%\n",
            "Epoch[1/2] Step[3887] Loss=0.0867 GradNorm=1.7184 StepSize=0.0172 RelImp=96.23%\n",
            "Epoch[1/2] Step[3888] Loss=0.1133 GradNorm=1.7494 StepSize=0.0175 RelImp=95.07%\n",
            "Epoch[1/2] Step[3889] Loss=0.1273 GradNorm=1.2803 StepSize=0.0128 RelImp=94.46%\n",
            "Epoch[1/2] Step[3890] Loss=0.3044 GradNorm=4.3356 StepSize=0.0434 RelImp=86.75%\n",
            "Epoch[1/2] Step[3891] Loss=0.2171 GradNorm=2.8115 StepSize=0.0281 RelImp=90.55%\n",
            "Epoch[1/2] Step[3892] Loss=0.2280 GradNorm=2.6930 StepSize=0.0269 RelImp=90.07%\n",
            "Epoch[1/2] Step[3893] Loss=0.3474 GradNorm=3.9231 StepSize=0.0392 RelImp=84.87%\n",
            "Epoch[1/2] Step[3894] Loss=0.1339 GradNorm=1.4694 StepSize=0.0147 RelImp=94.17%\n",
            "Epoch[1/2] Step[3895] Loss=0.3433 GradNorm=3.5661 StepSize=0.0357 RelImp=85.05%\n",
            "Epoch[1/2] Step[3896] Loss=0.5072 GradNorm=4.9854 StepSize=0.0499 RelImp=77.92%\n",
            "Epoch[1/2] Step[3897] Loss=0.3678 GradNorm=2.9769 StepSize=0.0298 RelImp=83.98%\n",
            "Epoch[1/2] Step[3898] Loss=0.3260 GradNorm=3.1960 StepSize=0.0320 RelImp=85.81%\n",
            "Epoch[1/2] Step[3899] Loss=0.1030 GradNorm=1.3716 StepSize=0.0137 RelImp=95.51%\n",
            "Epoch[1/2] Step[3900] Loss=0.3883 GradNorm=4.0926 StepSize=0.0409 RelImp=83.09%\n",
            "Epoch[1/2] Step[3901] Loss=0.1097 GradNorm=1.3743 StepSize=0.0137 RelImp=95.22%\n",
            "Epoch[1/2] Step[3902] Loss=0.4297 GradNorm=3.7462 StepSize=0.0375 RelImp=81.29%\n",
            "Epoch[1/2] Step[3903] Loss=0.8535 GradNorm=6.6202 StepSize=0.0662 RelImp=62.84%\n",
            "Epoch[1/2] Step[3904] Loss=0.3380 GradNorm=2.9151 StepSize=0.0292 RelImp=85.28%\n",
            "Epoch[1/2] Step[3905] Loss=0.3689 GradNorm=3.1870 StepSize=0.0319 RelImp=83.94%\n",
            "Epoch[1/2] Step[3906] Loss=0.5288 GradNorm=4.6463 StepSize=0.0465 RelImp=76.97%\n",
            "Epoch[1/2] Step[3907] Loss=0.6720 GradNorm=3.4494 StepSize=0.0345 RelImp=70.74%\n",
            "Epoch[1/2] Step[3908] Loss=0.5713 GradNorm=4.1061 StepSize=0.0411 RelImp=75.12%\n",
            "Epoch[1/2] Step[3909] Loss=0.3031 GradNorm=2.5948 StepSize=0.0259 RelImp=86.80%\n",
            "Epoch[1/2] Step[3910] Loss=0.2796 GradNorm=3.1092 StepSize=0.0311 RelImp=87.83%\n",
            "Epoch[1/2] Step[3911] Loss=0.2134 GradNorm=1.2558 StepSize=0.0126 RelImp=90.71%\n",
            "Epoch[1/2] Step[3912] Loss=0.4021 GradNorm=4.5828 StepSize=0.0458 RelImp=82.49%\n",
            "Epoch[1/2] Step[3913] Loss=0.2120 GradNorm=2.3189 StepSize=0.0232 RelImp=90.77%\n",
            "Epoch[1/2] Step[3914] Loss=0.0891 GradNorm=1.1496 StepSize=0.0115 RelImp=96.12%\n",
            "Epoch[1/2] Step[3915] Loss=0.1839 GradNorm=2.6995 StepSize=0.0270 RelImp=91.99%\n",
            "Epoch[1/2] Step[3916] Loss=0.4001 GradNorm=3.9728 StepSize=0.0397 RelImp=82.58%\n",
            "Epoch[1/2] Step[3917] Loss=0.3242 GradNorm=3.5978 StepSize=0.0360 RelImp=85.88%\n",
            "Epoch[1/2] Step[3918] Loss=0.4373 GradNorm=3.0694 StepSize=0.0307 RelImp=80.96%\n",
            "Epoch[1/2] Step[3919] Loss=0.2081 GradNorm=2.1395 StepSize=0.0214 RelImp=90.94%\n",
            "Epoch[1/2] Step[3920] Loss=0.4246 GradNorm=2.9485 StepSize=0.0295 RelImp=81.51%\n",
            "Epoch[1/2] Step[3921] Loss=0.1626 GradNorm=2.0683 StepSize=0.0207 RelImp=92.92%\n",
            "Epoch[1/2] Step[3922] Loss=0.4165 GradNorm=3.3844 StepSize=0.0338 RelImp=81.86%\n",
            "Epoch[1/2] Step[3923] Loss=0.7385 GradNorm=3.4982 StepSize=0.0350 RelImp=67.84%\n",
            "Epoch[1/2] Step[3924] Loss=0.4781 GradNorm=4.0762 StepSize=0.0408 RelImp=79.18%\n",
            "Epoch[1/2] Step[3925] Loss=0.1560 GradNorm=2.5006 StepSize=0.0250 RelImp=93.21%\n",
            "Epoch[1/2] Step[3926] Loss=0.4631 GradNorm=4.4648 StepSize=0.0446 RelImp=79.83%\n",
            "Epoch[1/2] Step[3927] Loss=0.4897 GradNorm=3.1438 StepSize=0.0314 RelImp=78.68%\n",
            "Epoch[1/2] Step[3928] Loss=0.5032 GradNorm=4.1893 StepSize=0.0419 RelImp=78.09%\n",
            "Epoch[1/2] Step[3929] Loss=0.2582 GradNorm=3.8234 StepSize=0.0382 RelImp=88.76%\n",
            "Epoch[1/2] Step[3930] Loss=0.3613 GradNorm=2.6885 StepSize=0.0269 RelImp=84.27%\n",
            "Epoch[1/2] Step[3931] Loss=0.4742 GradNorm=2.4333 StepSize=0.0243 RelImp=79.35%\n",
            "Epoch[1/2] Step[3932] Loss=0.4699 GradNorm=4.0696 StepSize=0.0407 RelImp=79.54%\n",
            "Epoch[1/2] Step[3933] Loss=0.1960 GradNorm=1.9493 StepSize=0.0195 RelImp=91.46%\n",
            "Epoch[1/2] Step[3934] Loss=0.3855 GradNorm=5.2791 StepSize=0.0528 RelImp=83.21%\n",
            "Epoch[1/2] Step[3935] Loss=0.1575 GradNorm=1.7836 StepSize=0.0178 RelImp=93.14%\n",
            "Epoch[1/2] Step[3936] Loss=0.1178 GradNorm=1.1663 StepSize=0.0117 RelImp=94.87%\n",
            "Epoch[1/2] Step[3937] Loss=0.3275 GradNorm=2.4812 StepSize=0.0248 RelImp=85.74%\n",
            "Epoch[1/2] Step[3938] Loss=0.2507 GradNorm=2.8727 StepSize=0.0287 RelImp=89.08%\n",
            "Epoch[1/2] Step[3939] Loss=0.8039 GradNorm=4.6006 StepSize=0.0460 RelImp=65.00%\n",
            "Epoch[1/2] Step[3940] Loss=0.5165 GradNorm=4.0734 StepSize=0.0407 RelImp=77.51%\n",
            "Epoch[1/2] Step[3941] Loss=0.3804 GradNorm=3.9112 StepSize=0.0391 RelImp=83.44%\n",
            "Epoch[1/2] Step[3942] Loss=0.7557 GradNorm=4.8834 StepSize=0.0488 RelImp=67.10%\n",
            "Epoch[1/2] Step[3943] Loss=0.0879 GradNorm=0.8739 StepSize=0.0087 RelImp=96.17%\n",
            "Epoch[1/2] Step[3944] Loss=0.2315 GradNorm=2.8182 StepSize=0.0282 RelImp=89.92%\n",
            "Epoch[1/2] Step[3945] Loss=0.7005 GradNorm=5.8928 StepSize=0.0589 RelImp=69.50%\n",
            "Epoch[1/2] Step[3946] Loss=0.2769 GradNorm=3.6020 StepSize=0.0360 RelImp=87.94%\n",
            "Epoch[1/2] Step[3947] Loss=0.4696 GradNorm=5.5258 StepSize=0.0553 RelImp=79.55%\n",
            "Epoch[1/2] Step[3948] Loss=0.7227 GradNorm=4.7147 StepSize=0.0471 RelImp=68.53%\n",
            "Epoch[1/2] Step[3949] Loss=0.5576 GradNorm=4.0595 StepSize=0.0406 RelImp=75.72%\n",
            "Epoch[1/2] Step[3950] Loss=0.1869 GradNorm=2.0252 StepSize=0.0203 RelImp=91.86%\n",
            "Epoch[1/2] Step[3951] Loss=0.6545 GradNorm=2.8161 StepSize=0.0282 RelImp=71.50%\n",
            "Epoch[1/2] Step[3952] Loss=0.7787 GradNorm=5.1597 StepSize=0.0516 RelImp=66.09%\n",
            "Epoch[1/2] Step[3953] Loss=0.2824 GradNorm=4.4911 StepSize=0.0449 RelImp=87.70%\n",
            "Epoch[1/2] Step[3954] Loss=0.4801 GradNorm=5.2247 StepSize=0.0522 RelImp=79.10%\n",
            "Epoch[1/2] Step[3955] Loss=0.2055 GradNorm=2.8008 StepSize=0.0280 RelImp=91.05%\n",
            "Epoch[1/2] Step[3956] Loss=0.5075 GradNorm=2.9848 StepSize=0.0298 RelImp=77.90%\n",
            "Epoch[1/2] Step[3957] Loss=0.1846 GradNorm=2.0701 StepSize=0.0207 RelImp=91.96%\n",
            "Epoch[1/2] Step[3958] Loss=0.6206 GradNorm=5.8226 StepSize=0.0582 RelImp=72.98%\n",
            "Epoch[1/2] Step[3959] Loss=0.4567 GradNorm=3.8447 StepSize=0.0384 RelImp=80.12%\n",
            "Epoch[1/2] Step[3960] Loss=0.4592 GradNorm=3.5765 StepSize=0.0358 RelImp=80.01%\n",
            "Epoch[1/2] Step[3961] Loss=0.2196 GradNorm=2.4800 StepSize=0.0248 RelImp=90.44%\n",
            "Epoch[1/2] Step[3962] Loss=0.1126 GradNorm=1.4718 StepSize=0.0147 RelImp=95.10%\n",
            "Epoch[1/2] Step[3963] Loss=0.4528 GradNorm=4.8660 StepSize=0.0487 RelImp=80.28%\n",
            "Epoch[1/2] Step[3964] Loss=0.3992 GradNorm=2.7996 StepSize=0.0280 RelImp=82.62%\n",
            "Epoch[1/2] Step[3965] Loss=0.2219 GradNorm=2.3146 StepSize=0.0231 RelImp=90.34%\n",
            "Epoch[1/2] Step[3966] Loss=0.5856 GradNorm=3.5142 StepSize=0.0351 RelImp=74.50%\n",
            "Epoch[1/2] Step[3967] Loss=0.4147 GradNorm=4.3763 StepSize=0.0438 RelImp=81.94%\n",
            "Epoch[1/2] Step[3968] Loss=0.0944 GradNorm=1.0789 StepSize=0.0108 RelImp=95.89%\n",
            "Epoch[1/2] Step[3969] Loss=0.3780 GradNorm=3.6021 StepSize=0.0360 RelImp=83.54%\n",
            "Epoch[1/2] Step[3970] Loss=0.6764 GradNorm=5.3528 StepSize=0.0535 RelImp=70.55%\n",
            "Epoch[1/2] Step[3971] Loss=0.6148 GradNorm=5.0781 StepSize=0.0508 RelImp=73.23%\n",
            "Epoch[1/2] Step[3972] Loss=0.8242 GradNorm=3.2296 StepSize=0.0323 RelImp=64.11%\n",
            "Epoch[1/2] Step[3973] Loss=0.4765 GradNorm=3.3703 StepSize=0.0337 RelImp=79.25%\n",
            "Epoch[1/2] Step[3974] Loss=0.5668 GradNorm=4.0429 StepSize=0.0404 RelImp=75.32%\n",
            "Epoch[1/2] Step[3975] Loss=0.1883 GradNorm=2.7979 StepSize=0.0280 RelImp=91.80%\n",
            "Epoch[1/2] Step[3976] Loss=0.2363 GradNorm=3.2693 StepSize=0.0327 RelImp=89.71%\n",
            "Epoch[1/2] Step[3977] Loss=0.5179 GradNorm=3.3733 StepSize=0.0337 RelImp=77.45%\n",
            "Epoch[1/2] Step[3978] Loss=0.1566 GradNorm=1.6822 StepSize=0.0168 RelImp=93.18%\n",
            "Epoch[1/2] Step[3979] Loss=0.6545 GradNorm=3.6336 StepSize=0.0363 RelImp=71.50%\n",
            "Epoch[1/2] Step[3980] Loss=0.4301 GradNorm=3.0734 StepSize=0.0307 RelImp=81.27%\n",
            "Epoch[1/2] Step[3981] Loss=0.4333 GradNorm=3.7409 StepSize=0.0374 RelImp=81.13%\n",
            "Epoch[1/2] Step[3982] Loss=0.1722 GradNorm=2.1566 StepSize=0.0216 RelImp=92.50%\n",
            "Epoch[1/2] Step[3983] Loss=0.3738 GradNorm=3.4672 StepSize=0.0347 RelImp=83.72%\n",
            "Epoch[1/2] Step[3984] Loss=0.6579 GradNorm=4.4505 StepSize=0.0445 RelImp=71.36%\n",
            "Epoch[1/2] Step[3985] Loss=0.5052 GradNorm=4.1060 StepSize=0.0411 RelImp=78.00%\n",
            "Epoch[1/2] Step[3986] Loss=0.3111 GradNorm=3.1928 StepSize=0.0319 RelImp=86.45%\n",
            "Epoch[1/2] Step[3987] Loss=0.0568 GradNorm=1.0399 StepSize=0.0104 RelImp=97.53%\n",
            "Epoch[1/2] Step[3988] Loss=0.5148 GradNorm=3.1485 StepSize=0.0315 RelImp=77.58%\n",
            "Epoch[1/2] Step[3989] Loss=0.8024 GradNorm=4.6873 StepSize=0.0469 RelImp=65.06%\n",
            "Epoch[1/2] Step[3990] Loss=0.4877 GradNorm=3.3664 StepSize=0.0337 RelImp=78.76%\n",
            "Epoch[1/2] Step[3991] Loss=0.8387 GradNorm=4.4828 StepSize=0.0448 RelImp=63.48%\n",
            "Epoch[1/2] Step[3992] Loss=0.3574 GradNorm=3.8484 StepSize=0.0385 RelImp=84.44%\n",
            "Epoch[1/2] Step[3993] Loss=0.1090 GradNorm=1.1185 StepSize=0.0112 RelImp=95.26%\n",
            "Epoch[1/2] Step[3994] Loss=0.2734 GradNorm=2.9877 StepSize=0.0299 RelImp=88.10%\n",
            "Epoch[1/2] Step[3995] Loss=0.2109 GradNorm=2.5216 StepSize=0.0252 RelImp=90.82%\n",
            "Epoch[1/2] Step[3996] Loss=0.2071 GradNorm=2.3721 StepSize=0.0237 RelImp=90.98%\n",
            "Epoch[1/2] Step[3997] Loss=0.3744 GradNorm=3.3508 StepSize=0.0335 RelImp=83.70%\n",
            "Epoch[1/2] Step[3998] Loss=0.4539 GradNorm=3.3857 StepSize=0.0339 RelImp=80.24%\n",
            "Epoch[1/2] Step[3999] Loss=0.7519 GradNorm=3.7458 StepSize=0.0375 RelImp=67.26%\n",
            "Epoch[1/2] Step[4000] Loss=0.3796 GradNorm=3.7929 StepSize=0.0379 RelImp=83.47%\n",
            "Epoch[1/2] Step[4001] Loss=0.7880 GradNorm=4.5636 StepSize=0.0456 RelImp=65.69%\n",
            "Epoch[1/2] Step[4002] Loss=0.3439 GradNorm=2.5889 StepSize=0.0259 RelImp=85.03%\n",
            "Epoch[1/2] Step[4003] Loss=0.2671 GradNorm=2.5948 StepSize=0.0259 RelImp=88.37%\n",
            "Epoch[1/2] Step[4004] Loss=0.3747 GradNorm=3.8457 StepSize=0.0385 RelImp=83.69%\n",
            "Epoch[1/2] Step[4005] Loss=0.8445 GradNorm=5.1111 StepSize=0.0511 RelImp=63.23%\n",
            "Epoch[1/2] Step[4006] Loss=0.2789 GradNorm=3.6981 StepSize=0.0370 RelImp=87.86%\n",
            "Epoch[1/2] Step[4007] Loss=0.2930 GradNorm=4.8488 StepSize=0.0485 RelImp=87.24%\n",
            "Epoch[1/2] Step[4008] Loss=0.4706 GradNorm=3.9952 StepSize=0.0400 RelImp=79.51%\n",
            "Epoch[1/2] Step[4009] Loss=0.2731 GradNorm=2.5572 StepSize=0.0256 RelImp=88.11%\n",
            "Epoch[1/2] Step[4010] Loss=0.5270 GradNorm=3.4986 StepSize=0.0350 RelImp=77.05%\n",
            "Epoch[1/2] Step[4011] Loss=0.1794 GradNorm=2.6224 StepSize=0.0262 RelImp=92.19%\n",
            "Epoch[1/2] Step[4012] Loss=0.3367 GradNorm=2.4478 StepSize=0.0245 RelImp=85.34%\n",
            "Epoch[1/2] Step[4013] Loss=0.3565 GradNorm=2.0992 StepSize=0.0210 RelImp=84.48%\n",
            "Epoch[1/2] Step[4014] Loss=0.6019 GradNorm=4.1014 StepSize=0.0410 RelImp=73.79%\n",
            "Epoch[1/2] Step[4015] Loss=0.2955 GradNorm=3.0167 StepSize=0.0302 RelImp=87.13%\n",
            "Epoch[1/2] Step[4016] Loss=0.4201 GradNorm=2.7814 StepSize=0.0278 RelImp=81.71%\n",
            "Epoch[1/2] Step[4017] Loss=0.3261 GradNorm=2.8311 StepSize=0.0283 RelImp=85.80%\n",
            "Epoch[1/2] Step[4018] Loss=0.4383 GradNorm=3.7614 StepSize=0.0376 RelImp=80.91%\n",
            "Epoch[1/2] Step[4019] Loss=0.6162 GradNorm=4.1214 StepSize=0.0412 RelImp=73.17%\n",
            "Epoch[1/2] Step[4020] Loss=0.3511 GradNorm=3.5573 StepSize=0.0356 RelImp=84.71%\n",
            "Epoch[1/2] Step[4021] Loss=0.3996 GradNorm=4.8459 StepSize=0.0485 RelImp=82.60%\n",
            "Epoch[1/2] Step[4022] Loss=0.8521 GradNorm=4.0600 StepSize=0.0406 RelImp=62.90%\n",
            "Epoch[1/2] Step[4023] Loss=0.3959 GradNorm=2.7837 StepSize=0.0278 RelImp=82.76%\n",
            "Epoch[1/2] Step[4024] Loss=0.2836 GradNorm=3.8653 StepSize=0.0387 RelImp=87.65%\n",
            "Epoch[1/2] Step[4025] Loss=0.5968 GradNorm=2.7655 StepSize=0.0277 RelImp=74.01%\n",
            "Epoch[1/2] Step[4026] Loss=0.4194 GradNorm=2.5038 StepSize=0.0250 RelImp=81.74%\n",
            "Epoch[1/2] Step[4027] Loss=0.2156 GradNorm=2.9697 StepSize=0.0297 RelImp=90.61%\n",
            "Epoch[1/2] Step[4028] Loss=0.7058 GradNorm=4.7025 StepSize=0.0470 RelImp=69.27%\n",
            "Epoch[1/2] Step[4029] Loss=0.5189 GradNorm=3.7433 StepSize=0.0374 RelImp=77.41%\n",
            "Epoch[1/2] Step[4030] Loss=0.2119 GradNorm=2.8436 StepSize=0.0284 RelImp=90.78%\n",
            "Epoch[1/2] Step[4031] Loss=0.4078 GradNorm=3.0745 StepSize=0.0307 RelImp=82.24%\n",
            "Epoch[1/2] Step[4032] Loss=0.4710 GradNorm=4.7148 StepSize=0.0471 RelImp=79.49%\n",
            "Epoch[1/2] Step[4033] Loss=0.3729 GradNorm=2.7703 StepSize=0.0277 RelImp=83.76%\n",
            "Epoch[1/2] Step[4034] Loss=0.1107 GradNorm=1.6664 StepSize=0.0167 RelImp=95.18%\n",
            "Epoch[1/2] Step[4035] Loss=0.1864 GradNorm=2.4351 StepSize=0.0244 RelImp=91.89%\n",
            "Epoch[1/2] Step[4036] Loss=0.1306 GradNorm=1.4377 StepSize=0.0144 RelImp=94.31%\n",
            "Epoch[1/2] Step[4037] Loss=0.3841 GradNorm=1.6951 StepSize=0.0170 RelImp=83.27%\n",
            "Epoch[1/2] Step[4038] Loss=0.5156 GradNorm=3.6050 StepSize=0.0360 RelImp=77.55%\n",
            "Epoch[1/2] Step[4039] Loss=0.3235 GradNorm=3.3710 StepSize=0.0337 RelImp=85.91%\n",
            "Epoch[1/2] Step[4040] Loss=0.1434 GradNorm=1.3037 StepSize=0.0130 RelImp=93.75%\n",
            "Epoch[1/2] Step[4041] Loss=0.5506 GradNorm=4.3582 StepSize=0.0436 RelImp=76.03%\n",
            "Epoch[1/2] Step[4042] Loss=0.5507 GradNorm=3.8579 StepSize=0.0386 RelImp=76.02%\n",
            "Epoch[1/2] Step[4043] Loss=0.2444 GradNorm=2.7759 StepSize=0.0278 RelImp=89.36%\n",
            "Epoch[1/2] Step[4044] Loss=0.4913 GradNorm=2.9114 StepSize=0.0291 RelImp=78.61%\n",
            "Epoch[1/2] Step[4045] Loss=0.6124 GradNorm=4.8650 StepSize=0.0486 RelImp=73.34%\n",
            "Epoch[1/2] Step[4046] Loss=0.4778 GradNorm=3.0168 StepSize=0.0302 RelImp=79.20%\n",
            "Epoch[1/2] Step[4047] Loss=0.1759 GradNorm=2.1485 StepSize=0.0215 RelImp=92.34%\n",
            "Epoch[1/2] Step[4048] Loss=0.1296 GradNorm=1.4583 StepSize=0.0146 RelImp=94.36%\n",
            "Epoch[1/2] Step[4049] Loss=0.3405 GradNorm=3.5691 StepSize=0.0357 RelImp=85.18%\n",
            "Epoch[1/2] Step[4050] Loss=0.4983 GradNorm=4.3905 StepSize=0.0439 RelImp=78.30%\n",
            "Epoch[1/2] Step[4051] Loss=0.0944 GradNorm=1.6094 StepSize=0.0161 RelImp=95.89%\n",
            "Epoch[1/2] Step[4052] Loss=0.3176 GradNorm=2.5378 StepSize=0.0254 RelImp=86.17%\n",
            "Epoch[1/2] Step[4053] Loss=0.1048 GradNorm=0.9918 StepSize=0.0099 RelImp=95.44%\n",
            "Epoch[1/2] Step[4054] Loss=0.2521 GradNorm=2.3450 StepSize=0.0234 RelImp=89.02%\n",
            "Epoch[1/2] Step[4055] Loss=0.2557 GradNorm=2.6998 StepSize=0.0270 RelImp=88.87%\n",
            "Epoch[1/2] Step[4056] Loss=0.2388 GradNorm=3.3944 StepSize=0.0339 RelImp=89.60%\n",
            "Epoch[1/2] Step[4057] Loss=0.1295 GradNorm=2.0917 StepSize=0.0209 RelImp=94.36%\n",
            "Epoch[1/2] Step[4058] Loss=0.1002 GradNorm=1.0199 StepSize=0.0102 RelImp=95.64%\n",
            "Epoch[1/2] Step[4059] Loss=0.4784 GradNorm=3.5929 StepSize=0.0359 RelImp=79.17%\n",
            "Epoch[1/2] Step[4060] Loss=0.4777 GradNorm=3.3411 StepSize=0.0334 RelImp=79.20%\n",
            "Epoch[1/2] Step[4061] Loss=0.3080 GradNorm=2.1280 StepSize=0.0213 RelImp=86.59%\n",
            "Epoch[1/2] Step[4062] Loss=0.2295 GradNorm=1.8356 StepSize=0.0184 RelImp=90.01%\n",
            "Epoch[1/2] Step[4063] Loss=0.4175 GradNorm=2.4815 StepSize=0.0248 RelImp=81.82%\n",
            "Epoch[1/2] Step[4064] Loss=0.5428 GradNorm=4.3336 StepSize=0.0433 RelImp=76.37%\n",
            "Epoch[1/2] Step[4065] Loss=0.5060 GradNorm=3.4132 StepSize=0.0341 RelImp=77.97%\n",
            "Epoch[1/2] Step[4066] Loss=0.2095 GradNorm=3.2482 StepSize=0.0325 RelImp=90.88%\n",
            "Epoch[1/2] Step[4067] Loss=0.5656 GradNorm=3.6306 StepSize=0.0363 RelImp=75.37%\n",
            "Epoch[1/2] Step[4068] Loss=0.3183 GradNorm=3.7940 StepSize=0.0379 RelImp=86.14%\n",
            "Epoch[1/2] Step[4069] Loss=0.1773 GradNorm=2.5373 StepSize=0.0254 RelImp=92.28%\n",
            "Epoch[1/2] Step[4070] Loss=0.2009 GradNorm=3.2300 StepSize=0.0323 RelImp=91.25%\n",
            "Epoch[1/2] Step[4071] Loss=0.3136 GradNorm=3.5025 StepSize=0.0350 RelImp=86.35%\n",
            "Epoch[1/2] Step[4072] Loss=0.3025 GradNorm=2.4913 StepSize=0.0249 RelImp=86.83%\n",
            "Epoch[1/2] Step[4073] Loss=0.1490 GradNorm=2.1247 StepSize=0.0212 RelImp=93.51%\n",
            "Epoch[1/2] Step[4074] Loss=0.3312 GradNorm=3.2445 StepSize=0.0324 RelImp=85.58%\n",
            "Epoch[1/2] Step[4075] Loss=0.1813 GradNorm=3.2093 StepSize=0.0321 RelImp=92.11%\n",
            "Epoch[1/2] Step[4076] Loss=0.0679 GradNorm=0.9377 StepSize=0.0094 RelImp=97.04%\n",
            "Epoch[1/2] Step[4077] Loss=0.1412 GradNorm=2.1930 StepSize=0.0219 RelImp=93.85%\n",
            "Epoch[1/2] Step[4078] Loss=0.2006 GradNorm=2.5073 StepSize=0.0251 RelImp=91.26%\n",
            "Epoch[1/2] Step[4079] Loss=0.6150 GradNorm=3.6799 StepSize=0.0368 RelImp=73.22%\n",
            "Epoch[1/2] Step[4080] Loss=0.2726 GradNorm=3.3553 StepSize=0.0336 RelImp=88.13%\n",
            "Epoch[1/2] Step[4081] Loss=0.2038 GradNorm=1.2335 StepSize=0.0123 RelImp=91.12%\n",
            "Epoch[1/2] Step[4082] Loss=0.2430 GradNorm=1.9645 StepSize=0.0196 RelImp=89.42%\n",
            "Epoch[1/2] Step[4083] Loss=0.5894 GradNorm=3.1875 StepSize=0.0319 RelImp=74.34%\n",
            "Epoch[1/2] Step[4084] Loss=0.5031 GradNorm=3.6259 StepSize=0.0363 RelImp=78.09%\n",
            "Epoch[1/2] Step[4085] Loss=0.2811 GradNorm=3.5718 StepSize=0.0357 RelImp=87.76%\n",
            "Epoch[1/2] Step[4086] Loss=0.2722 GradNorm=2.7731 StepSize=0.0277 RelImp=88.15%\n",
            "Epoch[1/2] Step[4087] Loss=0.2179 GradNorm=2.2525 StepSize=0.0225 RelImp=90.51%\n",
            "Epoch[1/2] Step[4088] Loss=0.4349 GradNorm=3.2117 StepSize=0.0321 RelImp=81.07%\n",
            "Epoch[1/2] Step[4089] Loss=0.2221 GradNorm=2.9405 StepSize=0.0294 RelImp=90.33%\n",
            "Epoch[1/2] Step[4090] Loss=0.1190 GradNorm=1.5844 StepSize=0.0158 RelImp=94.82%\n",
            "Epoch[1/2] Step[4091] Loss=0.2723 GradNorm=2.4359 StepSize=0.0244 RelImp=88.14%\n",
            "Epoch[1/2] Step[4092] Loss=0.1927 GradNorm=3.3058 StepSize=0.0331 RelImp=91.61%\n",
            "Epoch[1/2] Step[4093] Loss=0.0968 GradNorm=1.1476 StepSize=0.0115 RelImp=95.79%\n",
            "Epoch[1/2] Step[4094] Loss=0.4097 GradNorm=5.3735 StepSize=0.0537 RelImp=82.16%\n",
            "Epoch[1/2] Step[4095] Loss=0.1829 GradNorm=2.0055 StepSize=0.0201 RelImp=92.04%\n",
            "Epoch[1/2] Step[4096] Loss=0.4917 GradNorm=4.1908 StepSize=0.0419 RelImp=78.59%\n",
            "Epoch[1/2] Step[4097] Loss=0.2714 GradNorm=2.2842 StepSize=0.0228 RelImp=88.18%\n",
            "Epoch[1/2] Step[4098] Loss=0.2781 GradNorm=2.6530 StepSize=0.0265 RelImp=87.89%\n",
            "Epoch[1/2] Step[4099] Loss=0.2959 GradNorm=3.6460 StepSize=0.0365 RelImp=87.11%\n",
            "Epoch[1/2] Step[4100] Loss=0.0757 GradNorm=1.0915 StepSize=0.0109 RelImp=96.70%\n",
            "Epoch[1/2] Step[4101] Loss=0.4211 GradNorm=4.8158 StepSize=0.0482 RelImp=81.66%\n",
            "Epoch[1/2] Step[4102] Loss=0.3541 GradNorm=3.8015 StepSize=0.0380 RelImp=84.58%\n",
            "Epoch[1/2] Step[4103] Loss=0.3802 GradNorm=3.2028 StepSize=0.0320 RelImp=83.44%\n",
            "Epoch[1/2] Step[4104] Loss=0.2879 GradNorm=2.3583 StepSize=0.0236 RelImp=87.46%\n",
            "Epoch[1/2] Step[4105] Loss=0.2248 GradNorm=1.6213 StepSize=0.0162 RelImp=90.21%\n",
            "Epoch[1/2] Step[4106] Loss=0.4232 GradNorm=4.1734 StepSize=0.0417 RelImp=81.57%\n",
            "Epoch[1/2] Step[4107] Loss=0.1350 GradNorm=1.4993 StepSize=0.0150 RelImp=94.12%\n",
            "Epoch[1/2] Step[4108] Loss=0.1096 GradNorm=1.6677 StepSize=0.0167 RelImp=95.23%\n",
            "Epoch[1/2] Step[4109] Loss=0.1358 GradNorm=1.8656 StepSize=0.0187 RelImp=94.09%\n",
            "Epoch[1/2] Step[4110] Loss=0.1849 GradNorm=3.3498 StepSize=0.0335 RelImp=91.95%\n",
            "Epoch[1/2] Step[4111] Loss=0.5856 GradNorm=3.0800 StepSize=0.0308 RelImp=74.50%\n",
            "Epoch[1/2] Step[4112] Loss=0.1141 GradNorm=1.4423 StepSize=0.0144 RelImp=95.03%\n",
            "Epoch[1/2] Step[4113] Loss=0.2190 GradNorm=1.3257 StepSize=0.0133 RelImp=90.47%\n",
            "Epoch[1/2] Step[4114] Loss=0.3041 GradNorm=3.2339 StepSize=0.0323 RelImp=86.76%\n",
            "Epoch[1/2] Step[4115] Loss=0.3505 GradNorm=3.5452 StepSize=0.0355 RelImp=84.74%\n",
            "Epoch[1/2] Step[4116] Loss=0.3335 GradNorm=3.5357 StepSize=0.0354 RelImp=85.48%\n",
            "Epoch[1/2] Step[4117] Loss=0.2102 GradNorm=3.3132 StepSize=0.0331 RelImp=90.85%\n",
            "Epoch[1/2] Step[4118] Loss=0.2593 GradNorm=2.5238 StepSize=0.0252 RelImp=88.71%\n",
            "Epoch[1/2] Step[4119] Loss=0.3648 GradNorm=2.1865 StepSize=0.0219 RelImp=84.12%\n",
            "Epoch[1/2] Step[4120] Loss=0.9743 GradNorm=4.4209 StepSize=0.0442 RelImp=57.58%\n",
            "Epoch[1/2] Step[4121] Loss=0.2716 GradNorm=3.0232 StepSize=0.0302 RelImp=88.17%\n",
            "Epoch[1/2] Step[4122] Loss=0.2740 GradNorm=3.4370 StepSize=0.0344 RelImp=88.07%\n",
            "Epoch[1/2] Step[4123] Loss=0.6807 GradNorm=4.6882 StepSize=0.0469 RelImp=70.36%\n",
            "Epoch[1/2] Step[4124] Loss=0.0368 GradNorm=0.6202 StepSize=0.0062 RelImp=98.40%\n",
            "Epoch[1/2] Step[4125] Loss=0.1223 GradNorm=1.3421 StepSize=0.0134 RelImp=94.68%\n",
            "Epoch[1/2] Step[4126] Loss=0.6014 GradNorm=3.4460 StepSize=0.0345 RelImp=73.81%\n",
            "Epoch[1/2] Step[4127] Loss=0.0901 GradNorm=1.1640 StepSize=0.0116 RelImp=96.08%\n",
            "Epoch[1/2] Step[4128] Loss=0.2178 GradNorm=3.4880 StepSize=0.0349 RelImp=90.52%\n",
            "Epoch[1/2] Step[4129] Loss=0.3780 GradNorm=4.2821 StepSize=0.0428 RelImp=83.54%\n",
            "Epoch[1/2] Step[4130] Loss=0.4437 GradNorm=4.0417 StepSize=0.0404 RelImp=80.68%\n",
            "Epoch[1/2] Step[4131] Loss=0.3296 GradNorm=3.4203 StepSize=0.0342 RelImp=85.65%\n",
            "Epoch[1/2] Step[4132] Loss=0.3640 GradNorm=2.2433 StepSize=0.0224 RelImp=84.15%\n",
            "Epoch[1/2] Step[4133] Loss=0.6690 GradNorm=3.5081 StepSize=0.0351 RelImp=70.87%\n",
            "Epoch[1/2] Step[4134] Loss=0.3559 GradNorm=3.0740 StepSize=0.0307 RelImp=84.51%\n",
            "Epoch[1/2] Step[4135] Loss=0.4067 GradNorm=4.4476 StepSize=0.0445 RelImp=82.29%\n",
            "Epoch[1/2] Step[4136] Loss=0.7567 GradNorm=4.4689 StepSize=0.0447 RelImp=67.05%\n",
            "Epoch[1/2] Step[4137] Loss=0.2458 GradNorm=2.9543 StepSize=0.0295 RelImp=89.30%\n",
            "Epoch[1/2] Step[4138] Loss=0.1509 GradNorm=1.0142 StepSize=0.0101 RelImp=93.43%\n",
            "Epoch[1/2] Step[4139] Loss=0.3350 GradNorm=2.9247 StepSize=0.0292 RelImp=85.41%\n",
            "Epoch[1/2] Step[4140] Loss=0.4627 GradNorm=3.7592 StepSize=0.0376 RelImp=79.85%\n",
            "Epoch[1/2] Step[4141] Loss=0.8997 GradNorm=4.3648 StepSize=0.0436 RelImp=60.82%\n",
            "Epoch[1/2] Step[4142] Loss=0.1158 GradNorm=1.7877 StepSize=0.0179 RelImp=94.96%\n",
            "Epoch[1/2] Step[4143] Loss=0.5437 GradNorm=4.2079 StepSize=0.0421 RelImp=76.33%\n",
            "Epoch[1/2] Step[4144] Loss=0.3739 GradNorm=2.7396 StepSize=0.0274 RelImp=83.72%\n",
            "Epoch[1/2] Step[4145] Loss=0.1211 GradNorm=1.3616 StepSize=0.0136 RelImp=94.73%\n",
            "Epoch[1/2] Step[4146] Loss=0.1546 GradNorm=2.2464 StepSize=0.0225 RelImp=93.27%\n",
            "Epoch[1/2] Step[4147] Loss=0.1799 GradNorm=2.2195 StepSize=0.0222 RelImp=92.17%\n",
            "Epoch[1/2] Step[4148] Loss=0.4018 GradNorm=3.1375 StepSize=0.0314 RelImp=82.50%\n",
            "Epoch[1/2] Step[4149] Loss=0.7987 GradNorm=4.8821 StepSize=0.0488 RelImp=65.22%\n",
            "Epoch[1/2] Step[4150] Loss=0.2627 GradNorm=2.4257 StepSize=0.0243 RelImp=88.56%\n",
            "Epoch[1/2] Step[4151] Loss=0.9512 GradNorm=4.4958 StepSize=0.0450 RelImp=58.58%\n",
            "Epoch[1/2] Step[4152] Loss=0.1758 GradNorm=2.0610 StepSize=0.0206 RelImp=92.34%\n",
            "Epoch[1/2] Step[4153] Loss=0.1278 GradNorm=1.7191 StepSize=0.0172 RelImp=94.43%\n",
            "Epoch[1/2] Step[4154] Loss=0.0760 GradNorm=1.2266 StepSize=0.0123 RelImp=96.69%\n",
            "Epoch[1/2] Step[4155] Loss=0.3284 GradNorm=3.5818 StepSize=0.0358 RelImp=85.70%\n",
            "Epoch[1/2] Step[4156] Loss=0.2342 GradNorm=2.4447 StepSize=0.0244 RelImp=89.80%\n",
            "Epoch[1/2] Step[4157] Loss=0.6463 GradNorm=3.9671 StepSize=0.0397 RelImp=71.86%\n",
            "Epoch[1/2] Step[4158] Loss=0.1830 GradNorm=2.2717 StepSize=0.0227 RelImp=92.03%\n",
            "Epoch[1/2] Step[4159] Loss=0.2954 GradNorm=3.6885 StepSize=0.0369 RelImp=87.14%\n",
            "Epoch[1/2] Step[4160] Loss=0.4065 GradNorm=4.0818 StepSize=0.0408 RelImp=82.30%\n",
            "Epoch[1/2] Step[4161] Loss=0.2276 GradNorm=3.1642 StepSize=0.0316 RelImp=90.09%\n",
            "Epoch[1/2] Step[4162] Loss=0.0889 GradNorm=1.1066 StepSize=0.0111 RelImp=96.13%\n",
            "Epoch[1/2] Step[4163] Loss=0.0728 GradNorm=1.2978 StepSize=0.0130 RelImp=96.83%\n",
            "Epoch[1/2] Step[4164] Loss=0.4638 GradNorm=5.4179 StepSize=0.0542 RelImp=79.80%\n",
            "Epoch[1/2] Step[4165] Loss=0.3211 GradNorm=4.2207 StepSize=0.0422 RelImp=86.02%\n",
            "Epoch[1/2] Step[4166] Loss=0.4882 GradNorm=3.3874 StepSize=0.0339 RelImp=78.74%\n",
            "Epoch[1/2] Step[4167] Loss=0.8346 GradNorm=4.1194 StepSize=0.0412 RelImp=63.66%\n",
            "Epoch[1/2] Step[4168] Loss=0.2885 GradNorm=3.7965 StepSize=0.0380 RelImp=87.44%\n",
            "Epoch[1/2] Step[4169] Loss=0.3814 GradNorm=2.4363 StepSize=0.0244 RelImp=83.39%\n",
            "Epoch[1/2] Step[4170] Loss=0.3245 GradNorm=3.0223 StepSize=0.0302 RelImp=85.87%\n",
            "Epoch[1/2] Step[4171] Loss=0.1432 GradNorm=1.8781 StepSize=0.0188 RelImp=93.77%\n",
            "Epoch[1/2] Step[4172] Loss=0.2185 GradNorm=3.5245 StepSize=0.0352 RelImp=90.49%\n",
            "Epoch[1/2] Step[4173] Loss=0.7090 GradNorm=4.4027 StepSize=0.0440 RelImp=69.13%\n",
            "Epoch[1/2] Step[4174] Loss=0.0818 GradNorm=1.1726 StepSize=0.0117 RelImp=96.44%\n",
            "Epoch[1/2] Step[4175] Loss=0.6140 GradNorm=5.5184 StepSize=0.0552 RelImp=73.26%\n",
            "Epoch[1/2] Step[4176] Loss=0.1646 GradNorm=2.1558 StepSize=0.0216 RelImp=92.83%\n",
            "Epoch[1/2] Step[4177] Loss=0.5196 GradNorm=3.6590 StepSize=0.0366 RelImp=77.38%\n",
            "Epoch[1/2] Step[4178] Loss=0.5508 GradNorm=4.0586 StepSize=0.0406 RelImp=76.02%\n",
            "Epoch[1/2] Step[4179] Loss=0.1755 GradNorm=1.9578 StepSize=0.0196 RelImp=92.36%\n",
            "Epoch[1/2] Step[4180] Loss=0.3068 GradNorm=2.5333 StepSize=0.0253 RelImp=86.64%\n",
            "Epoch[1/2] Step[4181] Loss=0.2814 GradNorm=2.0442 StepSize=0.0204 RelImp=87.75%\n",
            "Epoch[1/2] Step[4182] Loss=0.2066 GradNorm=2.6264 StepSize=0.0263 RelImp=91.00%\n",
            "Epoch[1/2] Step[4183] Loss=0.3350 GradNorm=4.1236 StepSize=0.0412 RelImp=85.41%\n",
            "Epoch[1/2] Step[4184] Loss=0.3223 GradNorm=3.9632 StepSize=0.0396 RelImp=85.97%\n",
            "Epoch[1/2] Step[4185] Loss=0.2368 GradNorm=3.1020 StepSize=0.0310 RelImp=89.69%\n",
            "Epoch[1/2] Step[4186] Loss=0.3400 GradNorm=4.3738 StepSize=0.0437 RelImp=85.20%\n",
            "Epoch[1/2] Step[4187] Loss=0.2829 GradNorm=2.2265 StepSize=0.0223 RelImp=87.68%\n",
            "Epoch[1/2] Step[4188] Loss=0.3593 GradNorm=3.6277 StepSize=0.0363 RelImp=84.35%\n",
            "Epoch[1/2] Step[4189] Loss=0.3882 GradNorm=2.7858 StepSize=0.0279 RelImp=83.10%\n",
            "Epoch[1/2] Step[4190] Loss=0.4595 GradNorm=4.7902 StepSize=0.0479 RelImp=79.99%\n",
            "Epoch[1/2] Step[4191] Loss=0.2561 GradNorm=2.7523 StepSize=0.0275 RelImp=88.85%\n",
            "Epoch[1/2] Step[4192] Loss=0.8251 GradNorm=2.9998 StepSize=0.0300 RelImp=64.07%\n",
            "Epoch[1/2] Step[4193] Loss=0.1041 GradNorm=1.5414 StepSize=0.0154 RelImp=95.47%\n",
            "Epoch[1/2] Step[4194] Loss=0.3839 GradNorm=2.7531 StepSize=0.0275 RelImp=83.28%\n",
            "Epoch[1/2] Step[4195] Loss=0.4150 GradNorm=3.2483 StepSize=0.0325 RelImp=81.93%\n",
            "Epoch[1/2] Step[4196] Loss=0.2330 GradNorm=1.6851 StepSize=0.0169 RelImp=89.85%\n",
            "Epoch[1/2] Step[4197] Loss=0.3245 GradNorm=2.8861 StepSize=0.0289 RelImp=85.87%\n",
            "Epoch[1/2] Step[4198] Loss=0.4000 GradNorm=2.7945 StepSize=0.0279 RelImp=82.58%\n",
            "Epoch[1/2] Step[4199] Loss=0.1786 GradNorm=2.6228 StepSize=0.0262 RelImp=92.22%\n",
            "Epoch[1/2] Step[4200] Loss=0.2675 GradNorm=2.8035 StepSize=0.0280 RelImp=88.35%\n",
            "Epoch[1/2] Step[4201] Loss=0.0690 GradNorm=0.9964 StepSize=0.0100 RelImp=97.00%\n",
            "Epoch[1/2] Step[4202] Loss=0.4324 GradNorm=4.4865 StepSize=0.0449 RelImp=81.17%\n",
            "Epoch[1/2] Step[4203] Loss=0.2946 GradNorm=3.2171 StepSize=0.0322 RelImp=87.17%\n",
            "Epoch[1/2] Step[4204] Loss=0.7483 GradNorm=5.5993 StepSize=0.0560 RelImp=67.42%\n",
            "Epoch[1/2] Step[4205] Loss=0.6823 GradNorm=4.4403 StepSize=0.0444 RelImp=70.29%\n",
            "Epoch[1/2] Step[4206] Loss=0.4715 GradNorm=4.1377 StepSize=0.0414 RelImp=79.47%\n",
            "Epoch[1/2] Step[4207] Loss=0.4390 GradNorm=3.5613 StepSize=0.0356 RelImp=80.88%\n",
            "Epoch[1/2] Step[4208] Loss=0.3674 GradNorm=3.9975 StepSize=0.0400 RelImp=84.00%\n",
            "Epoch[1/2] Step[4209] Loss=0.3135 GradNorm=2.8496 StepSize=0.0285 RelImp=86.35%\n",
            "Epoch[1/2] Step[4210] Loss=0.2302 GradNorm=2.6102 StepSize=0.0261 RelImp=89.98%\n",
            "Epoch[1/2] Step[4211] Loss=0.1635 GradNorm=1.9320 StepSize=0.0193 RelImp=92.88%\n",
            "Epoch[1/2] Step[4212] Loss=0.5663 GradNorm=4.1185 StepSize=0.0412 RelImp=75.34%\n",
            "Epoch[1/2] Step[4213] Loss=0.2696 GradNorm=2.7595 StepSize=0.0276 RelImp=88.26%\n",
            "Epoch[1/2] Step[4214] Loss=0.5247 GradNorm=4.0188 StepSize=0.0402 RelImp=77.15%\n",
            "Epoch[1/2] Step[4215] Loss=0.2237 GradNorm=2.1937 StepSize=0.0219 RelImp=90.26%\n",
            "Epoch[1/2] Step[4216] Loss=0.4730 GradNorm=4.7439 StepSize=0.0474 RelImp=79.40%\n",
            "Epoch[1/2] Step[4217] Loss=0.2944 GradNorm=2.8318 StepSize=0.0283 RelImp=87.18%\n",
            "Epoch[1/2] Step[4218] Loss=0.1234 GradNorm=1.8909 StepSize=0.0189 RelImp=94.63%\n",
            "Epoch[1/2] Step[4219] Loss=0.1981 GradNorm=1.9677 StepSize=0.0197 RelImp=91.37%\n",
            "Epoch[1/2] Step[4220] Loss=0.3834 GradNorm=3.3268 StepSize=0.0333 RelImp=83.31%\n",
            "Epoch[1/2] Step[4221] Loss=0.5273 GradNorm=4.1285 StepSize=0.0413 RelImp=77.04%\n",
            "Epoch[1/2] Step[4222] Loss=0.2594 GradNorm=2.9384 StepSize=0.0294 RelImp=88.71%\n",
            "Epoch[1/2] Step[4223] Loss=0.1329 GradNorm=1.7714 StepSize=0.0177 RelImp=94.21%\n",
            "Epoch[1/2] Step[4224] Loss=0.0967 GradNorm=0.8815 StepSize=0.0088 RelImp=95.79%\n",
            "Epoch[1/2] Step[4225] Loss=0.0945 GradNorm=1.3849 StepSize=0.0138 RelImp=95.89%\n",
            "Epoch[1/2] Step[4226] Loss=0.2236 GradNorm=2.8431 StepSize=0.0284 RelImp=90.26%\n",
            "Epoch[1/2] Step[4227] Loss=0.1496 GradNorm=1.8483 StepSize=0.0185 RelImp=93.49%\n",
            "Epoch[1/2] Step[4228] Loss=0.4168 GradNorm=5.5415 StepSize=0.0554 RelImp=81.85%\n",
            "Epoch[1/2] Step[4229] Loss=0.3616 GradNorm=3.8069 StepSize=0.0381 RelImp=84.26%\n",
            "Epoch[1/2] Step[4230] Loss=0.1506 GradNorm=2.2025 StepSize=0.0220 RelImp=93.44%\n",
            "Epoch[1/2] Step[4231] Loss=0.1443 GradNorm=2.1368 StepSize=0.0214 RelImp=93.71%\n",
            "Epoch[1/2] Step[4232] Loss=0.3284 GradNorm=3.1873 StepSize=0.0319 RelImp=85.70%\n",
            "Epoch[1/2] Step[4233] Loss=0.4904 GradNorm=3.2399 StepSize=0.0324 RelImp=78.65%\n",
            "Epoch[1/2] Step[4234] Loss=0.4096 GradNorm=3.1342 StepSize=0.0313 RelImp=82.17%\n",
            "Epoch[1/2] Step[4235] Loss=0.7936 GradNorm=3.6847 StepSize=0.0368 RelImp=65.45%\n",
            "Epoch[1/2] Step[4236] Loss=0.3611 GradNorm=3.6975 StepSize=0.0370 RelImp=84.28%\n",
            "Epoch[1/2] Step[4237] Loss=0.0620 GradNorm=0.9561 StepSize=0.0096 RelImp=97.30%\n",
            "Epoch[1/2] Step[4238] Loss=0.1285 GradNorm=1.3976 StepSize=0.0140 RelImp=94.40%\n",
            "Epoch[1/2] Step[4239] Loss=0.2788 GradNorm=3.4890 StepSize=0.0349 RelImp=87.86%\n",
            "Epoch[1/2] Step[4240] Loss=0.8923 GradNorm=5.1982 StepSize=0.0520 RelImp=61.15%\n",
            "Epoch[1/2] Step[4241] Loss=0.7461 GradNorm=5.6883 StepSize=0.0569 RelImp=67.51%\n",
            "Epoch[1/2] Step[4242] Loss=0.3778 GradNorm=3.6613 StepSize=0.0366 RelImp=83.55%\n",
            "Epoch[1/2] Step[4243] Loss=0.6199 GradNorm=6.3650 StepSize=0.0637 RelImp=73.01%\n",
            "Epoch[1/2] Step[4244] Loss=0.3622 GradNorm=3.3472 StepSize=0.0335 RelImp=84.23%\n",
            "Epoch[1/2] Step[4245] Loss=0.2639 GradNorm=2.9378 StepSize=0.0294 RelImp=88.51%\n",
            "Epoch[1/2] Step[4246] Loss=0.2039 GradNorm=2.7016 StepSize=0.0270 RelImp=91.12%\n",
            "Epoch[1/2] Step[4247] Loss=0.0440 GradNorm=0.8823 StepSize=0.0088 RelImp=98.09%\n",
            "Epoch[1/2] Step[4248] Loss=0.2651 GradNorm=2.3688 StepSize=0.0237 RelImp=88.46%\n",
            "Epoch[1/2] Step[4249] Loss=0.5411 GradNorm=4.1830 StepSize=0.0418 RelImp=76.44%\n",
            "Epoch[1/2] Step[4250] Loss=0.4197 GradNorm=3.6787 StepSize=0.0368 RelImp=81.73%\n",
            "Epoch[1/2] Step[4251] Loss=0.2559 GradNorm=2.2338 StepSize=0.0223 RelImp=88.86%\n",
            "Epoch[1/2] Step[4252] Loss=0.6656 GradNorm=4.6193 StepSize=0.0462 RelImp=71.02%\n",
            "Epoch[1/2] Step[4253] Loss=0.2595 GradNorm=2.7271 StepSize=0.0273 RelImp=88.70%\n",
            "Epoch[1/2] Step[4254] Loss=0.1472 GradNorm=1.9660 StepSize=0.0197 RelImp=93.59%\n",
            "Epoch[1/2] Step[4255] Loss=0.1882 GradNorm=2.5302 StepSize=0.0253 RelImp=91.81%\n",
            "Epoch[1/2] Step[4256] Loss=0.2292 GradNorm=2.6128 StepSize=0.0261 RelImp=90.02%\n",
            "Epoch[1/2] Step[4257] Loss=0.1145 GradNorm=2.1573 StepSize=0.0216 RelImp=95.01%\n",
            "Epoch[1/2] Step[4258] Loss=0.3514 GradNorm=3.3865 StepSize=0.0339 RelImp=84.70%\n",
            "Epoch[1/2] Step[4259] Loss=0.5878 GradNorm=5.2018 StepSize=0.0520 RelImp=74.41%\n",
            "Epoch[1/2] Step[4260] Loss=0.9229 GradNorm=4.0836 StepSize=0.0408 RelImp=59.82%\n",
            "Epoch[1/2] Step[4261] Loss=0.6829 GradNorm=4.2834 StepSize=0.0428 RelImp=70.26%\n",
            "Epoch[1/2] Step[4262] Loss=0.3650 GradNorm=3.5874 StepSize=0.0359 RelImp=84.11%\n",
            "Epoch[1/2] Step[4263] Loss=0.0936 GradNorm=1.4574 StepSize=0.0146 RelImp=95.92%\n",
            "Epoch[1/2] Step[4264] Loss=0.2372 GradNorm=2.0196 StepSize=0.0202 RelImp=89.67%\n",
            "Epoch[1/2] Step[4265] Loss=0.2354 GradNorm=2.5946 StepSize=0.0259 RelImp=89.75%\n",
            "Epoch[1/2] Step[4266] Loss=0.2496 GradNorm=2.3388 StepSize=0.0234 RelImp=89.13%\n",
            "Epoch[1/2] Step[4267] Loss=0.4482 GradNorm=4.4207 StepSize=0.0442 RelImp=80.48%\n",
            "Epoch[1/2] Step[4268] Loss=0.8432 GradNorm=4.9865 StepSize=0.0499 RelImp=63.29%\n",
            "Epoch[1/2] Step[4269] Loss=0.3470 GradNorm=3.2783 StepSize=0.0328 RelImp=84.89%\n",
            "Epoch[1/2] Step[4270] Loss=0.7372 GradNorm=3.5021 StepSize=0.0350 RelImp=67.90%\n",
            "Epoch[1/2] Step[4271] Loss=0.4630 GradNorm=4.2479 StepSize=0.0425 RelImp=79.84%\n",
            "Epoch[1/2] Step[4272] Loss=0.2243 GradNorm=2.4708 StepSize=0.0247 RelImp=90.23%\n",
            "Epoch[1/2] Step[4273] Loss=0.4338 GradNorm=4.0701 StepSize=0.0407 RelImp=81.11%\n",
            "Epoch[1/2] Step[4274] Loss=0.3296 GradNorm=3.5276 StepSize=0.0353 RelImp=85.65%\n",
            "Epoch[1/2] Step[4275] Loss=0.2190 GradNorm=3.5367 StepSize=0.0354 RelImp=90.46%\n",
            "Epoch[1/2] Step[4276] Loss=0.2903 GradNorm=3.0396 StepSize=0.0304 RelImp=87.36%\n",
            "Epoch[1/2] Step[4277] Loss=0.5263 GradNorm=3.0232 StepSize=0.0302 RelImp=77.08%\n",
            "Epoch[1/2] Step[4278] Loss=0.4730 GradNorm=3.1866 StepSize=0.0319 RelImp=79.41%\n",
            "Epoch[1/2] Step[4279] Loss=0.5391 GradNorm=4.1788 StepSize=0.0418 RelImp=76.52%\n",
            "Epoch[1/2] Step[4280] Loss=0.7450 GradNorm=4.9156 StepSize=0.0492 RelImp=67.56%\n",
            "Epoch[1/2] Step[4281] Loss=0.6798 GradNorm=4.7530 StepSize=0.0475 RelImp=70.40%\n",
            "Epoch[1/2] Step[4282] Loss=0.5732 GradNorm=4.1810 StepSize=0.0418 RelImp=75.04%\n",
            "Epoch[1/2] Step[4283] Loss=0.2274 GradNorm=2.4824 StepSize=0.0248 RelImp=90.10%\n",
            "Epoch[1/2] Step[4284] Loss=0.1388 GradNorm=1.9732 StepSize=0.0197 RelImp=93.96%\n",
            "Epoch[1/2] Step[4285] Loss=0.1779 GradNorm=2.7520 StepSize=0.0275 RelImp=92.25%\n",
            "Epoch[1/2] Step[4286] Loss=0.7157 GradNorm=6.9218 StepSize=0.0692 RelImp=68.84%\n",
            "Epoch[1/2] Step[4287] Loss=0.6744 GradNorm=3.2583 StepSize=0.0326 RelImp=70.63%\n",
            "Epoch[1/2] Step[4288] Loss=0.4806 GradNorm=4.1705 StepSize=0.0417 RelImp=79.07%\n",
            "Epoch[1/2] Step[4289] Loss=0.9419 GradNorm=4.6843 StepSize=0.0468 RelImp=58.99%\n",
            "Epoch[1/2] Step[4290] Loss=0.4136 GradNorm=3.7935 StepSize=0.0379 RelImp=81.99%\n",
            "Epoch[1/2] Step[4291] Loss=0.3523 GradNorm=3.9252 StepSize=0.0393 RelImp=84.66%\n",
            "Epoch[1/2] Step[4292] Loss=0.1889 GradNorm=2.3804 StepSize=0.0238 RelImp=91.77%\n",
            "Epoch[1/2] Step[4293] Loss=0.3918 GradNorm=3.5312 StepSize=0.0353 RelImp=82.94%\n",
            "Epoch[1/2] Step[4294] Loss=0.4824 GradNorm=5.8906 StepSize=0.0589 RelImp=78.99%\n",
            "Epoch[1/2] Step[4295] Loss=0.5737 GradNorm=3.4737 StepSize=0.0347 RelImp=75.02%\n",
            "Epoch[1/2] Step[4296] Loss=0.1409 GradNorm=2.5597 StepSize=0.0256 RelImp=93.86%\n",
            "Epoch[1/2] Step[4297] Loss=0.1475 GradNorm=2.6797 StepSize=0.0268 RelImp=93.58%\n",
            "Epoch[1/2] Step[4298] Loss=0.7028 GradNorm=2.9968 StepSize=0.0300 RelImp=69.40%\n",
            "Epoch[1/2] Step[4299] Loss=0.3051 GradNorm=4.3750 StepSize=0.0437 RelImp=86.72%\n",
            "Epoch[1/2] Step[4300] Loss=0.0729 GradNorm=0.9614 StepSize=0.0096 RelImp=96.82%\n",
            "Epoch[1/2] Step[4301] Loss=0.2395 GradNorm=3.1112 StepSize=0.0311 RelImp=89.57%\n",
            "Epoch[1/2] Step[4302] Loss=0.2739 GradNorm=3.4805 StepSize=0.0348 RelImp=88.08%\n",
            "Epoch[1/2] Step[4303] Loss=0.2788 GradNorm=3.5168 StepSize=0.0352 RelImp=87.86%\n",
            "Epoch[1/2] Step[4304] Loss=0.6614 GradNorm=3.4267 StepSize=0.0343 RelImp=71.20%\n",
            "Epoch[1/2] Step[4305] Loss=0.2260 GradNorm=2.7181 StepSize=0.0272 RelImp=90.16%\n",
            "Epoch[1/2] Step[4306] Loss=0.0691 GradNorm=0.5103 StepSize=0.0051 RelImp=96.99%\n",
            "Epoch[1/2] Step[4307] Loss=0.1750 GradNorm=1.6714 StepSize=0.0167 RelImp=92.38%\n",
            "Epoch[1/2] Step[4308] Loss=0.2844 GradNorm=2.7538 StepSize=0.0275 RelImp=87.62%\n",
            "Epoch[1/2] Step[4309] Loss=0.1733 GradNorm=2.0133 StepSize=0.0201 RelImp=92.45%\n",
            "Epoch[1/2] Step[4310] Loss=0.2023 GradNorm=1.6849 StepSize=0.0168 RelImp=91.19%\n",
            "Epoch[1/2] Step[4311] Loss=0.2234 GradNorm=3.0909 StepSize=0.0309 RelImp=90.27%\n",
            "Epoch[1/2] Step[4312] Loss=0.5529 GradNorm=3.3214 StepSize=0.0332 RelImp=75.93%\n",
            "Epoch[1/2] Step[4313] Loss=0.5694 GradNorm=4.6565 StepSize=0.0466 RelImp=75.21%\n",
            "Epoch[1/2] Step[4314] Loss=0.5250 GradNorm=2.9397 StepSize=0.0294 RelImp=77.14%\n",
            "Epoch[1/2] Step[4315] Loss=0.6735 GradNorm=6.2195 StepSize=0.0622 RelImp=70.67%\n",
            "Epoch[1/2] Step[4316] Loss=0.9469 GradNorm=7.0190 StepSize=0.0702 RelImp=58.77%\n",
            "Epoch[1/2] Step[4317] Loss=0.5430 GradNorm=4.2840 StepSize=0.0428 RelImp=76.36%\n",
            "Epoch[1/2] Step[4318] Loss=0.5220 GradNorm=3.6191 StepSize=0.0362 RelImp=77.27%\n",
            "Epoch[1/2] Step[4319] Loss=0.0274 GradNorm=0.5707 StepSize=0.0057 RelImp=98.81%\n",
            "Epoch[1/2] Step[4320] Loss=0.6538 GradNorm=4.0477 StepSize=0.0405 RelImp=71.53%\n",
            "Epoch[1/2] Step[4321] Loss=0.2029 GradNorm=2.4640 StepSize=0.0246 RelImp=91.16%\n",
            "Epoch[1/2] Step[4322] Loss=0.4525 GradNorm=3.1778 StepSize=0.0318 RelImp=80.30%\n",
            "Epoch[1/2] Step[4323] Loss=0.1669 GradNorm=2.0233 StepSize=0.0202 RelImp=92.73%\n",
            "Epoch[1/2] Step[4324] Loss=0.2896 GradNorm=3.7502 StepSize=0.0375 RelImp=87.39%\n",
            "Epoch[1/2] Step[4325] Loss=0.1580 GradNorm=1.6934 StepSize=0.0169 RelImp=93.12%\n",
            "Epoch[1/2] Step[4326] Loss=0.3885 GradNorm=2.8636 StepSize=0.0286 RelImp=83.08%\n",
            "Epoch[1/2] Step[4327] Loss=0.1005 GradNorm=1.1469 StepSize=0.0115 RelImp=95.62%\n",
            "Epoch[1/2] Step[4328] Loss=0.3570 GradNorm=3.1661 StepSize=0.0317 RelImp=84.45%\n",
            "Epoch[1/2] Step[4329] Loss=0.3071 GradNorm=2.4702 StepSize=0.0247 RelImp=86.63%\n",
            "Epoch[1/2] Step[4330] Loss=0.3256 GradNorm=1.5796 StepSize=0.0158 RelImp=85.82%\n",
            "Epoch[1/2] Step[4331] Loss=0.2150 GradNorm=2.8822 StepSize=0.0288 RelImp=90.64%\n",
            "Epoch[1/2] Step[4332] Loss=0.1624 GradNorm=1.7507 StepSize=0.0175 RelImp=92.93%\n",
            "Epoch[1/2] Step[4333] Loss=0.1910 GradNorm=1.8249 StepSize=0.0182 RelImp=91.68%\n",
            "Epoch[1/2] Step[4334] Loss=0.1610 GradNorm=2.3404 StepSize=0.0234 RelImp=92.99%\n",
            "Epoch[1/2] Step[4335] Loss=0.5493 GradNorm=3.9041 StepSize=0.0390 RelImp=76.08%\n",
            "Epoch[1/2] Step[4336] Loss=0.4013 GradNorm=4.1216 StepSize=0.0412 RelImp=82.53%\n",
            "Epoch[1/2] Step[4337] Loss=0.4534 GradNorm=3.3536 StepSize=0.0335 RelImp=80.26%\n",
            "Epoch[1/2] Step[4338] Loss=0.2016 GradNorm=2.8419 StepSize=0.0284 RelImp=91.22%\n",
            "Epoch[1/2] Step[4339] Loss=0.3195 GradNorm=2.5218 StepSize=0.0252 RelImp=86.09%\n",
            "Epoch[1/2] Step[4340] Loss=0.2070 GradNorm=2.7464 StepSize=0.0275 RelImp=90.99%\n",
            "Epoch[1/2] Step[4341] Loss=0.2886 GradNorm=2.3469 StepSize=0.0235 RelImp=87.43%\n",
            "Epoch[1/2] Step[4342] Loss=0.1288 GradNorm=1.7440 StepSize=0.0174 RelImp=94.39%\n",
            "Epoch[1/2] Step[4343] Loss=0.3563 GradNorm=4.0991 StepSize=0.0410 RelImp=84.48%\n",
            "Epoch[1/2] Step[4344] Loss=0.1965 GradNorm=2.8019 StepSize=0.0280 RelImp=91.44%\n",
            "Epoch[1/2] Step[4345] Loss=0.4848 GradNorm=4.2312 StepSize=0.0423 RelImp=78.89%\n",
            "Epoch[1/2] Step[4346] Loss=0.2588 GradNorm=3.4598 StepSize=0.0346 RelImp=88.73%\n",
            "Epoch[1/2] Step[4347] Loss=0.2221 GradNorm=2.3681 StepSize=0.0237 RelImp=90.33%\n",
            "Epoch[1/2] Step[4348] Loss=0.3078 GradNorm=4.6736 StepSize=0.0467 RelImp=86.60%\n",
            "Epoch[1/2] Step[4349] Loss=0.2360 GradNorm=2.6497 StepSize=0.0265 RelImp=89.72%\n",
            "Epoch[1/2] Step[4350] Loss=0.1192 GradNorm=1.9089 StepSize=0.0191 RelImp=94.81%\n",
            "Epoch[1/2] Step[4351] Loss=0.2700 GradNorm=3.3699 StepSize=0.0337 RelImp=88.24%\n",
            "Epoch[1/2] Step[4352] Loss=0.5485 GradNorm=5.6664 StepSize=0.0567 RelImp=76.12%\n",
            "Epoch[1/2] Step[4353] Loss=0.2975 GradNorm=4.2321 StepSize=0.0423 RelImp=87.05%\n",
            "Epoch[1/2] Step[4354] Loss=0.2197 GradNorm=2.1820 StepSize=0.0218 RelImp=90.43%\n",
            "Epoch[1/2] Step[4355] Loss=0.8442 GradNorm=6.5949 StepSize=0.0659 RelImp=63.24%\n",
            "Epoch[1/2] Step[4356] Loss=0.4343 GradNorm=3.2696 StepSize=0.0327 RelImp=81.09%\n",
            "Epoch[1/2] Step[4357] Loss=0.3122 GradNorm=2.7605 StepSize=0.0276 RelImp=86.41%\n",
            "Epoch[1/2] Step[4358] Loss=0.2867 GradNorm=3.9812 StepSize=0.0398 RelImp=87.52%\n",
            "Epoch[1/2] Step[4359] Loss=0.2320 GradNorm=2.8684 StepSize=0.0287 RelImp=89.90%\n",
            "Epoch[1/2] Step[4360] Loss=0.2305 GradNorm=3.7417 StepSize=0.0374 RelImp=89.96%\n",
            "Epoch[1/2] Step[4361] Loss=0.2219 GradNorm=2.3873 StepSize=0.0239 RelImp=90.34%\n",
            "Epoch[1/2] Step[4362] Loss=0.2779 GradNorm=3.6617 StepSize=0.0366 RelImp=87.90%\n",
            "Epoch[1/2] Step[4363] Loss=0.4719 GradNorm=3.8739 StepSize=0.0387 RelImp=79.45%\n",
            "Epoch[1/2] Step[4364] Loss=0.2267 GradNorm=2.7672 StepSize=0.0277 RelImp=90.13%\n",
            "Epoch[1/2] Step[4365] Loss=0.6062 GradNorm=4.5673 StepSize=0.0457 RelImp=73.60%\n",
            "Epoch[1/2] Step[4366] Loss=0.1376 GradNorm=2.0602 StepSize=0.0206 RelImp=94.01%\n",
            "Epoch[1/2] Step[4367] Loss=0.5119 GradNorm=3.6303 StepSize=0.0363 RelImp=77.71%\n",
            "Epoch[1/2] Step[4368] Loss=0.2098 GradNorm=2.7233 StepSize=0.0272 RelImp=90.87%\n",
            "Epoch[1/2] Step[4369] Loss=0.1983 GradNorm=2.4484 StepSize=0.0245 RelImp=91.36%\n",
            "Epoch[1/2] Step[4370] Loss=0.1226 GradNorm=1.6894 StepSize=0.0169 RelImp=94.66%\n",
            "Epoch[1/2] Step[4371] Loss=0.2735 GradNorm=3.2871 StepSize=0.0329 RelImp=88.09%\n",
            "Epoch[1/2] Step[4372] Loss=0.1557 GradNorm=1.7586 StepSize=0.0176 RelImp=93.22%\n",
            "Epoch[1/2] Step[4373] Loss=0.2494 GradNorm=2.2624 StepSize=0.0226 RelImp=89.14%\n",
            "Epoch[1/2] Step[4374] Loss=0.2863 GradNorm=3.2133 StepSize=0.0321 RelImp=87.53%\n",
            "Epoch[1/2] Step[4375] Loss=0.4022 GradNorm=2.5283 StepSize=0.0253 RelImp=82.49%\n",
            "Epoch[1/2] Step[4376] Loss=0.1485 GradNorm=1.8807 StepSize=0.0188 RelImp=93.54%\n",
            "Epoch[1/2] Step[4377] Loss=0.2198 GradNorm=2.8045 StepSize=0.0280 RelImp=90.43%\n",
            "Epoch[1/2] Step[4378] Loss=0.7949 GradNorm=4.5120 StepSize=0.0451 RelImp=65.39%\n",
            "Epoch[1/2] Step[4379] Loss=0.2141 GradNorm=3.3294 StepSize=0.0333 RelImp=90.68%\n",
            "Epoch[1/2] Step[4380] Loss=0.2402 GradNorm=1.8246 StepSize=0.0182 RelImp=89.54%\n",
            "Epoch[1/2] Step[4381] Loss=0.4318 GradNorm=5.0271 StepSize=0.0503 RelImp=81.20%\n",
            "Epoch[1/2] Step[4382] Loss=0.1445 GradNorm=1.4607 StepSize=0.0146 RelImp=93.71%\n",
            "Epoch[1/2] Step[4383] Loss=0.1416 GradNorm=2.3027 StepSize=0.0230 RelImp=93.84%\n",
            "Epoch[1/2] Step[4384] Loss=0.4278 GradNorm=5.3600 StepSize=0.0536 RelImp=81.37%\n",
            "Epoch[1/2] Step[4385] Loss=0.2570 GradNorm=2.6097 StepSize=0.0261 RelImp=88.81%\n",
            "Epoch[1/2] Step[4386] Loss=0.1516 GradNorm=1.5093 StepSize=0.0151 RelImp=93.40%\n",
            "Epoch[1/2] Step[4387] Loss=0.1267 GradNorm=1.3790 StepSize=0.0138 RelImp=94.48%\n",
            "Epoch[1/2] Step[4388] Loss=0.3506 GradNorm=2.6696 StepSize=0.0267 RelImp=84.73%\n",
            "Epoch[1/2] Step[4389] Loss=0.9133 GradNorm=4.7542 StepSize=0.0475 RelImp=60.23%\n",
            "Epoch[1/2] Step[4390] Loss=0.5652 GradNorm=3.9077 StepSize=0.0391 RelImp=75.39%\n",
            "Epoch[1/2] Step[4391] Loss=0.8314 GradNorm=4.6186 StepSize=0.0462 RelImp=63.80%\n",
            "Epoch[1/2] Step[4392] Loss=0.6067 GradNorm=4.6537 StepSize=0.0465 RelImp=73.58%\n",
            "Epoch[1/2] Step[4393] Loss=0.2517 GradNorm=2.6494 StepSize=0.0265 RelImp=89.04%\n",
            "Epoch[1/2] Step[4394] Loss=0.2984 GradNorm=2.3727 StepSize=0.0237 RelImp=87.01%\n",
            "Epoch[1/2] Step[4395] Loss=0.3319 GradNorm=2.7683 StepSize=0.0277 RelImp=85.55%\n",
            "Epoch[1/2] Step[4396] Loss=0.3917 GradNorm=5.5245 StepSize=0.0552 RelImp=82.94%\n",
            "Epoch[1/2] Step[4397] Loss=0.2704 GradNorm=2.4328 StepSize=0.0243 RelImp=88.23%\n",
            "Epoch[1/2] Step[4398] Loss=0.0779 GradNorm=1.0595 StepSize=0.0106 RelImp=96.61%\n",
            "Epoch[1/2] Step[4399] Loss=0.1993 GradNorm=2.6704 StepSize=0.0267 RelImp=91.32%\n",
            "Epoch[1/2] Step[4400] Loss=0.2292 GradNorm=2.5473 StepSize=0.0255 RelImp=90.02%\n",
            "Epoch[1/2] Step[4401] Loss=0.7748 GradNorm=5.2064 StepSize=0.0521 RelImp=66.26%\n",
            "Epoch[1/2] Step[4402] Loss=0.7582 GradNorm=4.1594 StepSize=0.0416 RelImp=66.98%\n",
            "Epoch[1/2] Step[4403] Loss=0.0667 GradNorm=0.8942 StepSize=0.0089 RelImp=97.09%\n",
            "Epoch[1/2] Step[4404] Loss=0.8934 GradNorm=5.3277 StepSize=0.0533 RelImp=61.10%\n",
            "Epoch[1/2] Step[4405] Loss=0.0842 GradNorm=1.0861 StepSize=0.0109 RelImp=96.33%\n",
            "Epoch[1/2] Step[4406] Loss=0.3797 GradNorm=4.4503 StepSize=0.0445 RelImp=83.47%\n",
            "Epoch[1/2] Step[4407] Loss=0.2313 GradNorm=2.3507 StepSize=0.0235 RelImp=89.93%\n",
            "Epoch[1/2] Step[4408] Loss=0.8713 GradNorm=5.1453 StepSize=0.0515 RelImp=62.06%\n",
            "Epoch[1/2] Step[4409] Loss=0.3126 GradNorm=2.7785 StepSize=0.0278 RelImp=86.39%\n",
            "Epoch[1/2] Step[4410] Loss=0.2367 GradNorm=2.6977 StepSize=0.0270 RelImp=89.69%\n",
            "Epoch[1/2] Step[4411] Loss=0.4926 GradNorm=2.9023 StepSize=0.0290 RelImp=78.55%\n",
            "Epoch[1/2] Step[4412] Loss=0.2215 GradNorm=3.2965 StepSize=0.0330 RelImp=90.36%\n",
            "Epoch[1/2] Step[4413] Loss=0.6720 GradNorm=4.7761 StepSize=0.0478 RelImp=70.74%\n",
            "Epoch[1/2] Step[4414] Loss=0.5305 GradNorm=4.4358 StepSize=0.0444 RelImp=76.90%\n",
            "Epoch[1/2] Step[4415] Loss=0.8880 GradNorm=4.7215 StepSize=0.0472 RelImp=61.34%\n",
            "Epoch[1/2] Step[4416] Loss=0.2004 GradNorm=2.4713 StepSize=0.0247 RelImp=91.27%\n",
            "Epoch[1/2] Step[4417] Loss=0.4716 GradNorm=4.3666 StepSize=0.0437 RelImp=79.47%\n",
            "Epoch[1/2] Step[4418] Loss=0.1993 GradNorm=2.8012 StepSize=0.0280 RelImp=91.32%\n",
            "Epoch[1/2] Step[4419] Loss=0.1014 GradNorm=1.5439 StepSize=0.0154 RelImp=95.58%\n",
            "Epoch[1/2] Step[4420] Loss=0.1936 GradNorm=2.2543 StepSize=0.0225 RelImp=91.57%\n",
            "Epoch[1/2] Step[4421] Loss=0.2399 GradNorm=3.5046 StepSize=0.0350 RelImp=89.55%\n",
            "Epoch[1/2] Step[4422] Loss=0.2950 GradNorm=3.3679 StepSize=0.0337 RelImp=87.15%\n",
            "Epoch[1/2] Step[4423] Loss=0.1507 GradNorm=2.1027 StepSize=0.0210 RelImp=93.44%\n",
            "Epoch[1/2] Step[4424] Loss=0.2116 GradNorm=2.4747 StepSize=0.0247 RelImp=90.78%\n",
            "Epoch[1/2] Step[4425] Loss=0.1565 GradNorm=2.0721 StepSize=0.0207 RelImp=93.19%\n",
            "Epoch[1/2] Step[4426] Loss=0.1072 GradNorm=1.9915 StepSize=0.0199 RelImp=95.33%\n",
            "Epoch[1/2] Step[4427] Loss=0.1747 GradNorm=2.6899 StepSize=0.0269 RelImp=92.39%\n",
            "Epoch[1/2] Step[4428] Loss=0.1613 GradNorm=1.8666 StepSize=0.0187 RelImp=92.98%\n",
            "Epoch[1/2] Step[4429] Loss=0.1980 GradNorm=2.7045 StepSize=0.0270 RelImp=91.38%\n",
            "Epoch[1/2] Step[4430] Loss=0.6368 GradNorm=4.0222 StepSize=0.0402 RelImp=72.27%\n",
            "Epoch[1/2] Step[4431] Loss=0.5276 GradNorm=3.6214 StepSize=0.0362 RelImp=77.03%\n",
            "Epoch[1/2] Step[4432] Loss=0.3954 GradNorm=3.5466 StepSize=0.0355 RelImp=82.78%\n",
            "Epoch[1/2] Step[4433] Loss=0.5542 GradNorm=4.1639 StepSize=0.0416 RelImp=75.87%\n",
            "Epoch[1/2] Step[4434] Loss=0.3434 GradNorm=4.0481 StepSize=0.0405 RelImp=85.05%\n",
            "Epoch[1/2] Step[4435] Loss=0.8527 GradNorm=8.6799 StepSize=0.0868 RelImp=62.87%\n",
            "Epoch[1/2] Step[4436] Loss=0.5884 GradNorm=4.1336 StepSize=0.0413 RelImp=74.38%\n",
            "Epoch[1/2] Step[4437] Loss=0.2402 GradNorm=3.3901 StepSize=0.0339 RelImp=89.54%\n",
            "Epoch[1/2] Step[4438] Loss=0.1699 GradNorm=1.1626 StepSize=0.0116 RelImp=92.60%\n",
            "Epoch[1/2] Step[4439] Loss=0.3633 GradNorm=3.7826 StepSize=0.0378 RelImp=84.18%\n",
            "Epoch[1/2] Step[4440] Loss=0.5513 GradNorm=4.8756 StepSize=0.0488 RelImp=76.00%\n",
            "Epoch[1/2] Step[4441] Loss=0.2317 GradNorm=4.2032 StepSize=0.0420 RelImp=89.91%\n",
            "Epoch[1/2] Step[4442] Loss=0.3383 GradNorm=3.7088 StepSize=0.0371 RelImp=85.27%\n",
            "Epoch[1/2] Step[4443] Loss=0.1194 GradNorm=1.9348 StepSize=0.0193 RelImp=94.80%\n",
            "Epoch[1/2] Step[4444] Loss=0.3213 GradNorm=3.9070 StepSize=0.0391 RelImp=86.01%\n",
            "Epoch[1/2] Step[4445] Loss=0.3047 GradNorm=3.1428 StepSize=0.0314 RelImp=86.73%\n",
            "Epoch[1/2] Step[4446] Loss=0.3095 GradNorm=3.7288 StepSize=0.0373 RelImp=86.52%\n",
            "Epoch[1/2] Step[4447] Loss=0.4234 GradNorm=3.7543 StepSize=0.0375 RelImp=81.56%\n",
            "Epoch[1/2] Step[4448] Loss=0.3201 GradNorm=3.4617 StepSize=0.0346 RelImp=86.06%\n",
            "Epoch[1/2] Step[4449] Loss=0.3528 GradNorm=3.2558 StepSize=0.0326 RelImp=84.64%\n",
            "Epoch[1/2] Step[4450] Loss=0.1829 GradNorm=1.8461 StepSize=0.0185 RelImp=92.03%\n",
            "Epoch[1/2] Step[4451] Loss=0.2861 GradNorm=3.2281 StepSize=0.0323 RelImp=87.54%\n",
            "Epoch[1/2] Step[4452] Loss=0.0668 GradNorm=0.8808 StepSize=0.0088 RelImp=97.09%\n",
            "Epoch[1/2] Step[4453] Loss=0.2337 GradNorm=3.1408 StepSize=0.0314 RelImp=89.82%\n",
            "Epoch[1/2] Step[4454] Loss=0.2826 GradNorm=2.6588 StepSize=0.0266 RelImp=87.70%\n",
            "Epoch[1/2] Step[4455] Loss=0.3785 GradNorm=2.8245 StepSize=0.0282 RelImp=83.52%\n",
            "Epoch[1/2] Step[4456] Loss=0.1867 GradNorm=1.6893 StepSize=0.0169 RelImp=91.87%\n",
            "Epoch[1/2] Step[4457] Loss=0.6308 GradNorm=6.1643 StepSize=0.0616 RelImp=72.53%\n",
            "Epoch[1/2] Step[4458] Loss=0.4886 GradNorm=4.6312 StepSize=0.0463 RelImp=78.72%\n",
            "Epoch[1/2] Step[4459] Loss=0.1080 GradNorm=1.8594 StepSize=0.0186 RelImp=95.30%\n",
            "Epoch[1/2] Step[4460] Loss=0.6145 GradNorm=4.8251 StepSize=0.0483 RelImp=73.24%\n",
            "Epoch[1/2] Step[4461] Loss=0.2214 GradNorm=2.7383 StepSize=0.0274 RelImp=90.36%\n",
            "Epoch[1/2] Step[4462] Loss=0.3979 GradNorm=5.4296 StepSize=0.0543 RelImp=82.67%\n",
            "Epoch[1/2] Step[4463] Loss=0.0831 GradNorm=1.0712 StepSize=0.0107 RelImp=96.38%\n",
            "Epoch[1/2] Step[4464] Loss=0.1712 GradNorm=1.8982 StepSize=0.0190 RelImp=92.55%\n",
            "Epoch[1/2] Step[4465] Loss=0.1328 GradNorm=1.7332 StepSize=0.0173 RelImp=94.22%\n",
            "Epoch[1/2] Step[4466] Loss=0.6557 GradNorm=5.6494 StepSize=0.0565 RelImp=71.45%\n",
            "Epoch[1/2] Step[4467] Loss=0.3133 GradNorm=2.9751 StepSize=0.0298 RelImp=86.36%\n",
            "Epoch[1/2] Step[4468] Loss=0.2499 GradNorm=2.9605 StepSize=0.0296 RelImp=89.12%\n",
            "Epoch[1/2] Step[4469] Loss=1.8922 GradNorm=7.8654 StepSize=0.0787 RelImp=17.61%\n",
            "Epoch[1/2] Step[4470] Loss=0.0791 GradNorm=1.0851 StepSize=0.0109 RelImp=96.55%\n",
            "Epoch[1/2] Step[4471] Loss=0.2048 GradNorm=2.3283 StepSize=0.0233 RelImp=91.08%\n",
            "Epoch[1/2] Step[4472] Loss=0.5524 GradNorm=4.0662 StepSize=0.0407 RelImp=75.95%\n",
            "Epoch[1/2] Step[4473] Loss=0.4879 GradNorm=4.2924 StepSize=0.0429 RelImp=78.75%\n",
            "Epoch[1/2] Step[4474] Loss=0.3551 GradNorm=3.8288 StepSize=0.0383 RelImp=84.54%\n",
            "Epoch[1/2] Step[4475] Loss=0.0442 GradNorm=0.5773 StepSize=0.0058 RelImp=98.08%\n",
            "Epoch[1/2] Step[4476] Loss=0.3852 GradNorm=3.2965 StepSize=0.0330 RelImp=83.23%\n",
            "Epoch[1/2] Step[4477] Loss=0.2493 GradNorm=4.0874 StepSize=0.0409 RelImp=89.15%\n",
            "Epoch[1/2] Step[4478] Loss=0.2957 GradNorm=2.0703 StepSize=0.0207 RelImp=87.13%\n",
            "Epoch[1/2] Step[4479] Loss=0.2907 GradNorm=3.6189 StepSize=0.0362 RelImp=87.34%\n",
            "Epoch[1/2] Step[4480] Loss=0.4028 GradNorm=3.0726 StepSize=0.0307 RelImp=82.46%\n",
            "Epoch[1/2] Step[4481] Loss=0.4265 GradNorm=3.1185 StepSize=0.0312 RelImp=81.43%\n",
            "Epoch[1/2] Step[4482] Loss=1.1076 GradNorm=6.3217 StepSize=0.0632 RelImp=51.77%\n",
            "Epoch[1/2] Step[4483] Loss=0.2262 GradNorm=2.4048 StepSize=0.0240 RelImp=90.15%\n",
            "Epoch[1/2] Step[4484] Loss=0.6831 GradNorm=8.6200 StepSize=0.0862 RelImp=70.25%\n",
            "Epoch[1/2] Step[4485] Loss=0.6558 GradNorm=4.4433 StepSize=0.0444 RelImp=71.44%\n",
            "Epoch[1/2] Step[4486] Loss=0.3219 GradNorm=3.9261 StepSize=0.0393 RelImp=85.99%\n",
            "Epoch[1/2] Step[4487] Loss=0.5160 GradNorm=4.7122 StepSize=0.0471 RelImp=77.53%\n",
            "Epoch[1/2] Step[4488] Loss=0.4400 GradNorm=5.2454 StepSize=0.0525 RelImp=80.84%\n",
            "Epoch[1/2] Step[4489] Loss=0.5586 GradNorm=4.1239 StepSize=0.0412 RelImp=75.68%\n",
            "Epoch[1/2] Step[4490] Loss=0.2107 GradNorm=2.2483 StepSize=0.0225 RelImp=90.83%\n",
            "Epoch[1/2] Step[4491] Loss=0.4428 GradNorm=4.0079 StepSize=0.0401 RelImp=80.72%\n",
            "Epoch[1/2] Step[4492] Loss=0.0970 GradNorm=1.9528 StepSize=0.0195 RelImp=95.78%\n",
            "Epoch[1/2] Step[4493] Loss=0.2232 GradNorm=3.6073 StepSize=0.0361 RelImp=90.28%\n",
            "Epoch[1/2] Step[4494] Loss=0.8609 GradNorm=4.7004 StepSize=0.0470 RelImp=62.51%\n",
            "Epoch[1/2] Step[4495] Loss=0.3444 GradNorm=3.6120 StepSize=0.0361 RelImp=85.00%\n",
            "Epoch[1/2] Step[4496] Loss=0.2799 GradNorm=3.5059 StepSize=0.0351 RelImp=87.81%\n",
            "Epoch[1/2] Step[4497] Loss=0.4268 GradNorm=3.8025 StepSize=0.0380 RelImp=81.42%\n",
            "Epoch[1/2] Step[4498] Loss=0.1308 GradNorm=2.1559 StepSize=0.0216 RelImp=94.30%\n",
            "Epoch[1/2] Step[4499] Loss=0.4861 GradNorm=4.0691 StepSize=0.0407 RelImp=78.84%\n",
            "Epoch[1/2] Step[4500] Loss=0.3213 GradNorm=3.0894 StepSize=0.0309 RelImp=86.01%\n",
            "Epoch[1/2] Step[4501] Loss=0.3012 GradNorm=2.8044 StepSize=0.0280 RelImp=86.88%\n",
            "Epoch[1/2] Step[4502] Loss=0.1878 GradNorm=2.3536 StepSize=0.0235 RelImp=91.82%\n",
            "Epoch[1/2] Step[4503] Loss=0.0697 GradNorm=1.3466 StepSize=0.0135 RelImp=96.96%\n",
            "Epoch[1/2] Step[4504] Loss=0.2131 GradNorm=1.5953 StepSize=0.0160 RelImp=90.72%\n",
            "Epoch[1/2] Step[4505] Loss=0.3494 GradNorm=3.9391 StepSize=0.0394 RelImp=84.79%\n",
            "Epoch[1/2] Step[4506] Loss=0.2318 GradNorm=2.6831 StepSize=0.0268 RelImp=89.91%\n",
            "Epoch[1/2] Step[4507] Loss=0.2703 GradNorm=3.3331 StepSize=0.0333 RelImp=88.23%\n",
            "Epoch[1/2] Step[4508] Loss=0.2911 GradNorm=3.2454 StepSize=0.0325 RelImp=87.32%\n",
            "Epoch[1/2] Step[4509] Loss=0.3229 GradNorm=3.2032 StepSize=0.0320 RelImp=85.94%\n",
            "Epoch[1/2] Step[4510] Loss=0.2749 GradNorm=2.7233 StepSize=0.0272 RelImp=88.03%\n",
            "Epoch[1/2] Step[4511] Loss=0.4022 GradNorm=2.3848 StepSize=0.0238 RelImp=82.49%\n",
            "Epoch[1/2] Step[4512] Loss=0.4084 GradNorm=4.5162 StepSize=0.0452 RelImp=82.22%\n",
            "Epoch[1/2] Step[4513] Loss=0.2424 GradNorm=3.0189 StepSize=0.0302 RelImp=89.45%\n",
            "Epoch[1/2] Step[4514] Loss=0.2793 GradNorm=2.3391 StepSize=0.0234 RelImp=87.84%\n",
            "Epoch[1/2] Step[4515] Loss=0.1020 GradNorm=1.2548 StepSize=0.0125 RelImp=95.56%\n",
            "Epoch[1/2] Step[4516] Loss=0.2266 GradNorm=3.3023 StepSize=0.0330 RelImp=90.13%\n",
            "Epoch[1/2] Step[4517] Loss=0.1803 GradNorm=2.0212 StepSize=0.0202 RelImp=92.15%\n",
            "Epoch[1/2] Step[4518] Loss=0.2359 GradNorm=3.2719 StepSize=0.0327 RelImp=89.73%\n",
            "Epoch[1/2] Step[4519] Loss=0.3248 GradNorm=2.5397 StepSize=0.0254 RelImp=85.86%\n",
            "Epoch[1/2] Step[4520] Loss=1.0667 GradNorm=5.9580 StepSize=0.0596 RelImp=53.55%\n",
            "Epoch[1/2] Step[4521] Loss=0.2241 GradNorm=2.9878 StepSize=0.0299 RelImp=90.24%\n",
            "Epoch[1/2] Step[4522] Loss=0.2236 GradNorm=2.4828 StepSize=0.0248 RelImp=90.26%\n",
            "Epoch[1/2] Step[4523] Loss=0.1299 GradNorm=2.0802 StepSize=0.0208 RelImp=94.34%\n",
            "Epoch[1/2] Step[4524] Loss=0.4310 GradNorm=2.7119 StepSize=0.0271 RelImp=81.23%\n",
            "Epoch[1/2] Step[4525] Loss=0.1650 GradNorm=1.7597 StepSize=0.0176 RelImp=92.82%\n",
            "Epoch[1/2] Step[4526] Loss=0.5377 GradNorm=4.7927 StepSize=0.0479 RelImp=76.59%\n",
            "Epoch[1/2] Step[4527] Loss=0.2492 GradNorm=4.0438 StepSize=0.0404 RelImp=89.15%\n",
            "Epoch[1/2] Step[4528] Loss=0.2624 GradNorm=3.1022 StepSize=0.0310 RelImp=88.57%\n",
            "Epoch[1/2] Step[4529] Loss=0.1391 GradNorm=2.0409 StepSize=0.0204 RelImp=93.94%\n",
            "Epoch[1/2] Step[4530] Loss=0.2021 GradNorm=2.3206 StepSize=0.0232 RelImp=91.20%\n",
            "Epoch[1/2] Step[4531] Loss=0.3936 GradNorm=3.3911 StepSize=0.0339 RelImp=82.86%\n",
            "Epoch[1/2] Step[4532] Loss=0.3013 GradNorm=2.6870 StepSize=0.0269 RelImp=86.88%\n",
            "Epoch[1/2] Step[4533] Loss=0.5501 GradNorm=4.6805 StepSize=0.0468 RelImp=76.05%\n",
            "Epoch[1/2] Step[4534] Loss=0.4266 GradNorm=4.4248 StepSize=0.0442 RelImp=81.42%\n",
            "Epoch[1/2] Step[4535] Loss=0.4287 GradNorm=3.5317 StepSize=0.0353 RelImp=81.34%\n",
            "Epoch[1/2] Step[4536] Loss=0.3091 GradNorm=2.6894 StepSize=0.0269 RelImp=86.54%\n",
            "Epoch[1/2] Step[4537] Loss=0.5028 GradNorm=3.6970 StepSize=0.0370 RelImp=78.11%\n",
            "Epoch[1/2] Step[4538] Loss=0.2693 GradNorm=2.7446 StepSize=0.0274 RelImp=88.27%\n",
            "Epoch[1/2] Step[4539] Loss=0.3572 GradNorm=3.8512 StepSize=0.0385 RelImp=84.45%\n",
            "Epoch[1/2] Step[4540] Loss=0.4447 GradNorm=2.2725 StepSize=0.0227 RelImp=80.64%\n",
            "Epoch[1/2] Step[4541] Loss=0.1904 GradNorm=2.0639 StepSize=0.0206 RelImp=91.71%\n",
            "Epoch[1/2] Step[4542] Loss=0.2485 GradNorm=2.7043 StepSize=0.0270 RelImp=89.18%\n",
            "Epoch[1/2] Step[4543] Loss=0.0961 GradNorm=1.2059 StepSize=0.0121 RelImp=95.82%\n",
            "Epoch[1/2] Step[4544] Loss=0.6651 GradNorm=4.7782 StepSize=0.0478 RelImp=71.04%\n",
            "Epoch[1/2] Step[4545] Loss=0.2574 GradNorm=3.0403 StepSize=0.0304 RelImp=88.79%\n",
            "Epoch[1/2] Step[4546] Loss=0.1561 GradNorm=2.3717 StepSize=0.0237 RelImp=93.20%\n",
            "Epoch[1/2] Step[4547] Loss=0.2066 GradNorm=3.0517 StepSize=0.0305 RelImp=91.00%\n",
            "Epoch[1/2] Step[4548] Loss=0.2925 GradNorm=2.8154 StepSize=0.0282 RelImp=87.26%\n",
            "Epoch[1/2] Step[4549] Loss=0.2155 GradNorm=2.5952 StepSize=0.0260 RelImp=90.62%\n",
            "Epoch[1/2] Step[4550] Loss=0.3106 GradNorm=2.4422 StepSize=0.0244 RelImp=86.47%\n",
            "Epoch[1/2] Step[4551] Loss=0.2693 GradNorm=3.0096 StepSize=0.0301 RelImp=88.27%\n",
            "Epoch[1/2] Step[4552] Loss=0.3594 GradNorm=4.2701 StepSize=0.0427 RelImp=84.35%\n",
            "Epoch[1/2] Step[4553] Loss=0.2989 GradNorm=4.0335 StepSize=0.0403 RelImp=86.98%\n",
            "Epoch[1/2] Step[4554] Loss=0.3547 GradNorm=4.7751 StepSize=0.0478 RelImp=84.56%\n",
            "Epoch[1/2] Step[4555] Loss=0.2355 GradNorm=3.1942 StepSize=0.0319 RelImp=89.74%\n",
            "Epoch[1/2] Step[4556] Loss=0.0627 GradNorm=0.9842 StepSize=0.0098 RelImp=97.27%\n",
            "Epoch[1/2] Step[4557] Loss=0.2170 GradNorm=1.7336 StepSize=0.0173 RelImp=90.55%\n",
            "Epoch[1/2] Step[4558] Loss=0.4593 GradNorm=2.9119 StepSize=0.0291 RelImp=80.00%\n",
            "Epoch[1/2] Step[4559] Loss=0.1798 GradNorm=2.5436 StepSize=0.0254 RelImp=92.17%\n",
            "Epoch[1/2] Step[4560] Loss=0.3042 GradNorm=3.3090 StepSize=0.0331 RelImp=86.75%\n",
            "Epoch[1/2] Step[4561] Loss=0.1664 GradNorm=2.8794 StepSize=0.0288 RelImp=92.76%\n",
            "Epoch[1/2] Step[4562] Loss=0.8848 GradNorm=4.0648 StepSize=0.0406 RelImp=61.47%\n",
            "Epoch[1/2] Step[4563] Loss=0.7680 GradNorm=5.3027 StepSize=0.0530 RelImp=66.56%\n",
            "Epoch[1/2] Step[4564] Loss=0.3319 GradNorm=2.9164 StepSize=0.0292 RelImp=85.55%\n",
            "Epoch[1/2] Step[4565] Loss=0.2845 GradNorm=3.0088 StepSize=0.0301 RelImp=87.61%\n",
            "Epoch[1/2] Step[4566] Loss=0.1472 GradNorm=2.5171 StepSize=0.0252 RelImp=93.59%\n",
            "Epoch[1/2] Step[4567] Loss=0.4745 GradNorm=4.5407 StepSize=0.0454 RelImp=79.34%\n",
            "Epoch[1/2] Step[4568] Loss=0.1824 GradNorm=2.1833 StepSize=0.0218 RelImp=92.06%\n",
            "Epoch[1/2] Step[4569] Loss=0.5314 GradNorm=2.6844 StepSize=0.0268 RelImp=76.86%\n",
            "Epoch[1/2] Step[4570] Loss=0.1567 GradNorm=2.5081 StepSize=0.0251 RelImp=93.18%\n",
            "Epoch[1/2] Step[4571] Loss=0.3194 GradNorm=2.8895 StepSize=0.0289 RelImp=86.09%\n",
            "Epoch[1/2] Step[4572] Loss=0.3697 GradNorm=4.0677 StepSize=0.0407 RelImp=83.90%\n",
            "Epoch[1/2] Step[4573] Loss=0.0817 GradNorm=1.1035 StepSize=0.0110 RelImp=96.44%\n",
            "Epoch[1/2] Step[4574] Loss=0.2229 GradNorm=3.0009 StepSize=0.0300 RelImp=90.29%\n",
            "Epoch[1/2] Step[4575] Loss=0.1951 GradNorm=2.2886 StepSize=0.0229 RelImp=91.50%\n",
            "Epoch[1/2] Step[4576] Loss=0.4033 GradNorm=3.7507 StepSize=0.0375 RelImp=82.44%\n",
            "Epoch[1/2] Step[4577] Loss=1.3269 GradNorm=6.4184 StepSize=0.0642 RelImp=42.22%\n",
            "Epoch[1/2] Step[4578] Loss=0.0607 GradNorm=0.6938 StepSize=0.0069 RelImp=97.36%\n",
            "Epoch[1/2] Step[4579] Loss=0.0913 GradNorm=1.3399 StepSize=0.0134 RelImp=96.02%\n",
            "Epoch[1/2] Step[4580] Loss=0.7526 GradNorm=4.1808 StepSize=0.0418 RelImp=67.23%\n",
            "Epoch[1/2] Step[4581] Loss=0.1363 GradNorm=1.7371 StepSize=0.0174 RelImp=94.07%\n",
            "Epoch[1/2] Step[4582] Loss=0.3257 GradNorm=3.5976 StepSize=0.0360 RelImp=85.82%\n",
            "Epoch[1/2] Step[4583] Loss=0.4022 GradNorm=3.5614 StepSize=0.0356 RelImp=82.49%\n",
            "Epoch[1/2] Step[4584] Loss=0.1299 GradNorm=1.7750 StepSize=0.0177 RelImp=94.35%\n",
            "Epoch[1/2] Step[4585] Loss=0.4273 GradNorm=4.6561 StepSize=0.0466 RelImp=81.39%\n",
            "Epoch[1/2] Step[4586] Loss=0.5017 GradNorm=3.5745 StepSize=0.0357 RelImp=78.16%\n",
            "Epoch[1/2] Step[4587] Loss=0.6261 GradNorm=5.6542 StepSize=0.0565 RelImp=72.74%\n",
            "Epoch[1/2] Step[4588] Loss=0.0890 GradNorm=1.6683 StepSize=0.0167 RelImp=96.12%\n",
            "Epoch[1/2] Step[4589] Loss=0.2699 GradNorm=3.2152 StepSize=0.0322 RelImp=88.25%\n",
            "Epoch[1/2] Step[4590] Loss=0.1072 GradNorm=1.5302 StepSize=0.0153 RelImp=95.33%\n",
            "Epoch[1/2] Step[4591] Loss=0.2622 GradNorm=4.2456 StepSize=0.0425 RelImp=88.58%\n",
            "Epoch[1/2] Step[4592] Loss=0.5330 GradNorm=4.3862 StepSize=0.0439 RelImp=76.79%\n",
            "Epoch[1/2] Step[4593] Loss=0.1105 GradNorm=1.3808 StepSize=0.0138 RelImp=95.19%\n",
            "Epoch[1/2] Step[4594] Loss=0.9809 GradNorm=3.9084 StepSize=0.0391 RelImp=57.29%\n",
            "Epoch[1/2] Step[4595] Loss=0.1741 GradNorm=2.2684 StepSize=0.0227 RelImp=92.42%\n",
            "Epoch[1/2] Step[4596] Loss=0.1525 GradNorm=2.2622 StepSize=0.0226 RelImp=93.36%\n",
            "Epoch[1/2] Step[4597] Loss=0.1283 GradNorm=1.7575 StepSize=0.0176 RelImp=94.41%\n",
            "Epoch[1/2] Step[4598] Loss=0.0686 GradNorm=1.0010 StepSize=0.0100 RelImp=97.01%\n",
            "Epoch[1/2] Step[4599] Loss=0.4619 GradNorm=5.3999 StepSize=0.0540 RelImp=79.89%\n",
            "Epoch[1/2] Step[4600] Loss=0.2720 GradNorm=3.1619 StepSize=0.0316 RelImp=88.15%\n",
            "Epoch[1/2] Step[4601] Loss=0.1122 GradNorm=1.8415 StepSize=0.0184 RelImp=95.11%\n",
            "Epoch[1/2] Step[4602] Loss=0.6693 GradNorm=4.6033 StepSize=0.0460 RelImp=70.86%\n",
            "Epoch[1/2] Step[4603] Loss=0.3044 GradNorm=3.5697 StepSize=0.0357 RelImp=86.75%\n",
            "Epoch[1/2] Step[4604] Loss=0.5895 GradNorm=4.0924 StepSize=0.0409 RelImp=74.33%\n",
            "Epoch[1/2] Step[4605] Loss=0.2843 GradNorm=2.9270 StepSize=0.0293 RelImp=87.62%\n",
            "Epoch[1/2] Step[4606] Loss=0.0768 GradNorm=1.4754 StepSize=0.0148 RelImp=96.65%\n",
            "Epoch[1/2] Step[4607] Loss=0.4585 GradNorm=4.6757 StepSize=0.0468 RelImp=80.04%\n",
            "Epoch[1/2] Step[4608] Loss=0.0944 GradNorm=1.3622 StepSize=0.0136 RelImp=95.89%\n",
            "Epoch[1/2] Step[4609] Loss=0.5445 GradNorm=3.4790 StepSize=0.0348 RelImp=76.29%\n",
            "Epoch[1/2] Step[4610] Loss=0.1209 GradNorm=1.2060 StepSize=0.0121 RelImp=94.74%\n",
            "Epoch[1/2] Step[4611] Loss=0.1960 GradNorm=2.1177 StepSize=0.0212 RelImp=91.46%\n",
            "Epoch[1/2] Step[4612] Loss=0.4231 GradNorm=3.7959 StepSize=0.0380 RelImp=81.58%\n",
            "Epoch[1/2] Step[4613] Loss=0.6053 GradNorm=4.7951 StepSize=0.0480 RelImp=73.64%\n",
            "Epoch[1/2] Step[4614] Loss=0.1907 GradNorm=2.7874 StepSize=0.0279 RelImp=91.70%\n",
            "Epoch[1/2] Step[4615] Loss=0.3749 GradNorm=3.0286 StepSize=0.0303 RelImp=83.68%\n",
            "Epoch[1/2] Step[4616] Loss=0.2757 GradNorm=2.1714 StepSize=0.0217 RelImp=87.99%\n",
            "Epoch[1/2] Step[4617] Loss=0.3294 GradNorm=3.8808 StepSize=0.0388 RelImp=85.66%\n",
            "Epoch[1/2] Step[4618] Loss=0.5559 GradNorm=7.6561 StepSize=0.0766 RelImp=75.79%\n",
            "Epoch[1/2] Step[4619] Loss=0.3005 GradNorm=3.7244 StepSize=0.0372 RelImp=86.92%\n",
            "Epoch[1/2] Step[4620] Loss=0.0774 GradNorm=1.3751 StepSize=0.0138 RelImp=96.63%\n",
            "Epoch[1/2] Step[4621] Loss=0.1970 GradNorm=2.8660 StepSize=0.0287 RelImp=91.42%\n",
            "Epoch[1/2] Step[4622] Loss=0.0904 GradNorm=0.9928 StepSize=0.0099 RelImp=96.06%\n",
            "Epoch[1/2] Step[4623] Loss=0.2541 GradNorm=3.2548 StepSize=0.0325 RelImp=88.93%\n",
            "Epoch[1/2] Step[4624] Loss=0.3345 GradNorm=2.9515 StepSize=0.0295 RelImp=85.44%\n",
            "Epoch[1/2] Step[4625] Loss=0.3225 GradNorm=3.3207 StepSize=0.0332 RelImp=85.96%\n",
            "Epoch[1/2] Step[4626] Loss=1.2534 GradNorm=4.4318 StepSize=0.0443 RelImp=45.43%\n",
            "Epoch[1/2] Step[4627] Loss=0.9042 GradNorm=5.0625 StepSize=0.0506 RelImp=60.63%\n",
            "Epoch[1/2] Step[4628] Loss=0.3100 GradNorm=3.6429 StepSize=0.0364 RelImp=86.50%\n",
            "Epoch[1/2] Step[4629] Loss=0.2007 GradNorm=2.2182 StepSize=0.0222 RelImp=91.26%\n",
            "Epoch[1/2] Step[4630] Loss=0.2457 GradNorm=2.6335 StepSize=0.0263 RelImp=89.30%\n",
            "Epoch[1/2] Step[4631] Loss=0.1868 GradNorm=2.1797 StepSize=0.0218 RelImp=91.87%\n",
            "Epoch[1/2] Step[4632] Loss=0.1547 GradNorm=2.2694 StepSize=0.0227 RelImp=93.26%\n",
            "Epoch[1/2] Step[4633] Loss=0.3877 GradNorm=3.8170 StepSize=0.0382 RelImp=83.12%\n",
            "Epoch[1/2] Step[4634] Loss=0.2278 GradNorm=2.4040 StepSize=0.0240 RelImp=90.08%\n",
            "Epoch[1/2] Step[4635] Loss=0.1620 GradNorm=1.9319 StepSize=0.0193 RelImp=92.95%\n",
            "Epoch[1/2] Step[4636] Loss=0.1369 GradNorm=1.9468 StepSize=0.0195 RelImp=94.04%\n",
            "Epoch[1/2] Step[4637] Loss=0.7332 GradNorm=4.9456 StepSize=0.0495 RelImp=68.08%\n",
            "Epoch[1/2] Step[4638] Loss=0.7370 GradNorm=4.4341 StepSize=0.0443 RelImp=67.91%\n",
            "Epoch[1/2] Step[4639] Loss=0.2281 GradNorm=2.7979 StepSize=0.0280 RelImp=90.07%\n",
            "Epoch[1/2] Step[4640] Loss=0.5701 GradNorm=3.8754 StepSize=0.0388 RelImp=75.18%\n",
            "Epoch[1/2] Step[4641] Loss=0.3264 GradNorm=3.5029 StepSize=0.0350 RelImp=85.79%\n",
            "Epoch[1/2] Step[4642] Loss=0.2507 GradNorm=2.7938 StepSize=0.0279 RelImp=89.08%\n",
            "Epoch[1/2] Step[4643] Loss=0.0744 GradNorm=0.9571 StepSize=0.0096 RelImp=96.76%\n",
            "Epoch[1/2] Step[4644] Loss=0.4103 GradNorm=4.5741 StepSize=0.0457 RelImp=82.13%\n",
            "Epoch[1/2] Step[4645] Loss=0.3766 GradNorm=3.1422 StepSize=0.0314 RelImp=83.60%\n",
            "Epoch[1/2] Step[4646] Loss=0.3857 GradNorm=2.9951 StepSize=0.0300 RelImp=83.21%\n",
            "Epoch[1/2] Step[4647] Loss=0.4172 GradNorm=4.5380 StepSize=0.0454 RelImp=81.83%\n",
            "Epoch[1/2] Step[4648] Loss=0.6181 GradNorm=4.3047 StepSize=0.0430 RelImp=73.09%\n",
            "Epoch[1/2] Step[4649] Loss=0.2658 GradNorm=2.8402 StepSize=0.0284 RelImp=88.43%\n",
            "Epoch[1/2] Step[4650] Loss=0.3997 GradNorm=3.1599 StepSize=0.0316 RelImp=82.59%\n",
            "Epoch[1/2] Step[4651] Loss=0.2064 GradNorm=2.5336 StepSize=0.0253 RelImp=91.01%\n",
            "Epoch[1/2] Step[4652] Loss=0.1115 GradNorm=1.1696 StepSize=0.0117 RelImp=95.15%\n",
            "Epoch[1/2] Step[4653] Loss=0.4669 GradNorm=4.6559 StepSize=0.0466 RelImp=79.67%\n",
            "Epoch[1/2] Step[4654] Loss=0.0913 GradNorm=1.2268 StepSize=0.0123 RelImp=96.02%\n",
            "Epoch[1/2] Step[4655] Loss=0.1590 GradNorm=1.6644 StepSize=0.0166 RelImp=93.08%\n",
            "Epoch[1/2] Step[4656] Loss=0.2057 GradNorm=2.2827 StepSize=0.0228 RelImp=91.04%\n",
            "Epoch[1/2] Step[4657] Loss=0.3315 GradNorm=3.0695 StepSize=0.0307 RelImp=85.57%\n",
            "Epoch[1/2] Step[4658] Loss=0.2868 GradNorm=2.3581 StepSize=0.0236 RelImp=87.51%\n",
            "Epoch[1/2] Step[4659] Loss=0.1414 GradNorm=1.8463 StepSize=0.0185 RelImp=93.84%\n",
            "Epoch[1/2] Step[4660] Loss=0.1521 GradNorm=2.4602 StepSize=0.0246 RelImp=93.38%\n",
            "Epoch[1/2] Step[4661] Loss=0.9019 GradNorm=4.8339 StepSize=0.0483 RelImp=60.73%\n",
            "Epoch[1/2] Step[4662] Loss=0.2586 GradNorm=3.2617 StepSize=0.0326 RelImp=88.74%\n",
            "Epoch[1/2] Step[4663] Loss=0.3486 GradNorm=2.9386 StepSize=0.0294 RelImp=84.82%\n",
            "Epoch[1/2] Step[4664] Loss=0.2184 GradNorm=2.5418 StepSize=0.0254 RelImp=90.49%\n",
            "Epoch[1/2] Step[4665] Loss=0.4258 GradNorm=3.1881 StepSize=0.0319 RelImp=81.46%\n",
            "Epoch[1/2] Step[4666] Loss=0.5586 GradNorm=3.1498 StepSize=0.0315 RelImp=75.68%\n",
            "Epoch[1/2] Step[4667] Loss=0.6462 GradNorm=4.9155 StepSize=0.0492 RelImp=71.86%\n",
            "Epoch[1/2] Step[4668] Loss=0.2377 GradNorm=2.7030 StepSize=0.0270 RelImp=89.65%\n",
            "Epoch[1/2] Step[4669] Loss=0.2408 GradNorm=2.7551 StepSize=0.0276 RelImp=89.51%\n",
            "Epoch[1/2] Step[4670] Loss=0.1197 GradNorm=2.1010 StepSize=0.0210 RelImp=94.79%\n",
            "Epoch[1/2] Step[4671] Loss=0.3667 GradNorm=4.3690 StepSize=0.0437 RelImp=84.03%\n",
            "Epoch[1/2] Step[4672] Loss=0.4177 GradNorm=3.7633 StepSize=0.0376 RelImp=81.81%\n",
            "Epoch[1/2] Step[4673] Loss=0.1954 GradNorm=2.4107 StepSize=0.0241 RelImp=91.49%\n",
            "Epoch[1/2] Step[4674] Loss=0.4408 GradNorm=4.2121 StepSize=0.0421 RelImp=80.81%\n",
            "Epoch[1/2] Step[4675] Loss=0.1307 GradNorm=2.2617 StepSize=0.0226 RelImp=94.31%\n",
            "Epoch[1/2] Step[4676] Loss=0.1575 GradNorm=2.1681 StepSize=0.0217 RelImp=93.14%\n",
            "Epoch[1/2] Step[4677] Loss=0.6699 GradNorm=5.2761 StepSize=0.0528 RelImp=70.83%\n",
            "Epoch[1/2] Step[4678] Loss=0.0800 GradNorm=1.1078 StepSize=0.0111 RelImp=96.52%\n",
            "Epoch[1/2] Step[4679] Loss=0.4240 GradNorm=3.0058 StepSize=0.0301 RelImp=81.54%\n",
            "Epoch[1/2] Step[4680] Loss=0.3781 GradNorm=3.1587 StepSize=0.0316 RelImp=83.53%\n",
            "Epoch[1/2] Step[4681] Loss=0.7844 GradNorm=4.9073 StepSize=0.0491 RelImp=65.85%\n",
            "Epoch[1/2] Step[4682] Loss=0.4164 GradNorm=4.5949 StepSize=0.0459 RelImp=81.87%\n",
            "Epoch[1/2] Step[4683] Loss=0.9435 GradNorm=5.7876 StepSize=0.0579 RelImp=58.92%\n",
            "Epoch[1/2] Step[4684] Loss=0.2385 GradNorm=3.1755 StepSize=0.0318 RelImp=89.62%\n",
            "Epoch[1/2] Step[4685] Loss=0.3476 GradNorm=2.5789 StepSize=0.0258 RelImp=84.86%\n",
            "Epoch[1/2] Step[4686] Loss=0.4542 GradNorm=3.0324 StepSize=0.0303 RelImp=80.22%\n",
            "Epoch[1/2] Step[4687] Loss=0.5217 GradNorm=2.5451 StepSize=0.0255 RelImp=77.28%\n",
            "Epoch[1/2] Step[4688] Loss=0.3838 GradNorm=4.1188 StepSize=0.0412 RelImp=83.29%\n",
            "Epoch[1/2] Step[4689] Loss=1.2659 GradNorm=5.2113 StepSize=0.0521 RelImp=44.88%\n",
            "Epoch[1/2] Step[4690] Loss=0.5362 GradNorm=6.0604 StepSize=0.0606 RelImp=76.65%\n",
            "Epoch[1/2] Step[4691] Loss=0.3487 GradNorm=4.0567 StepSize=0.0406 RelImp=84.82%\n",
            "Epoch[1/2] Step[4692] Loss=0.3661 GradNorm=2.0969 StepSize=0.0210 RelImp=84.06%\n",
            "Epoch[1/2] Step[4693] Loss=0.1706 GradNorm=2.2950 StepSize=0.0229 RelImp=92.57%\n",
            "Epoch[1/2] Step[4694] Loss=0.3455 GradNorm=2.7482 StepSize=0.0275 RelImp=84.95%\n",
            "Epoch[1/2] Step[4695] Loss=0.2742 GradNorm=2.3016 StepSize=0.0230 RelImp=88.06%\n",
            "Epoch[1/2] Step[4696] Loss=0.1471 GradNorm=2.4836 StepSize=0.0248 RelImp=93.59%\n",
            "Epoch[1/2] Step[4697] Loss=0.6234 GradNorm=5.0904 StepSize=0.0509 RelImp=72.86%\n",
            "Epoch[1/2] Step[4698] Loss=0.3556 GradNorm=3.6390 StepSize=0.0364 RelImp=84.51%\n",
            "Epoch[1/2] Step[4699] Loss=0.2141 GradNorm=2.5962 StepSize=0.0260 RelImp=90.68%\n",
            "Epoch[1/2] Step[4700] Loss=0.9224 GradNorm=4.2114 StepSize=0.0421 RelImp=59.84%\n",
            "Epoch[1/2] Step[4701] Loss=0.1654 GradNorm=1.8651 StepSize=0.0187 RelImp=92.80%\n",
            "Epoch[1/2] Step[4702] Loss=0.0955 GradNorm=1.0112 StepSize=0.0101 RelImp=95.84%\n",
            "Epoch[1/2] Step[4703] Loss=0.3180 GradNorm=2.3141 StepSize=0.0231 RelImp=86.15%\n",
            "Epoch[1/2] Step[4704] Loss=0.3149 GradNorm=3.6446 StepSize=0.0364 RelImp=86.29%\n",
            "Epoch[1/2] Step[4705] Loss=0.2424 GradNorm=3.0380 StepSize=0.0304 RelImp=89.45%\n",
            "Epoch[1/2] Step[4706] Loss=0.1662 GradNorm=3.0094 StepSize=0.0301 RelImp=92.76%\n",
            "Epoch[1/2] Step[4707] Loss=0.6131 GradNorm=3.3932 StepSize=0.0339 RelImp=73.31%\n",
            "Epoch[1/2] Step[4708] Loss=0.2364 GradNorm=2.6422 StepSize=0.0264 RelImp=89.70%\n",
            "Epoch[1/2] Step[4709] Loss=0.4856 GradNorm=5.0728 StepSize=0.0507 RelImp=78.85%\n",
            "Epoch[1/2] Step[4710] Loss=0.2009 GradNorm=2.2204 StepSize=0.0222 RelImp=91.25%\n",
            "Epoch[1/2] Step[4711] Loss=0.0970 GradNorm=1.3718 StepSize=0.0137 RelImp=95.78%\n",
            "Epoch[1/2] Step[4712] Loss=0.1509 GradNorm=1.8646 StepSize=0.0186 RelImp=93.43%\n",
            "Epoch[1/2] Step[4713] Loss=0.3616 GradNorm=3.7141 StepSize=0.0371 RelImp=84.26%\n",
            "Epoch[1/2] Step[4714] Loss=0.4431 GradNorm=4.3313 StepSize=0.0433 RelImp=80.71%\n",
            "Epoch[1/2] Step[4715] Loss=0.1937 GradNorm=2.6104 StepSize=0.0261 RelImp=91.57%\n",
            "Epoch[1/2] Step[4716] Loss=0.2612 GradNorm=2.4874 StepSize=0.0249 RelImp=88.62%\n",
            "Epoch[1/2] Step[4717] Loss=0.2292 GradNorm=2.6354 StepSize=0.0264 RelImp=90.02%\n",
            "Epoch[1/2] Step[4718] Loss=0.3795 GradNorm=3.5105 StepSize=0.0351 RelImp=83.48%\n",
            "Epoch[1/2] Step[4719] Loss=0.1644 GradNorm=2.2564 StepSize=0.0226 RelImp=92.84%\n",
            "Epoch[1/2] Step[4720] Loss=0.1058 GradNorm=1.8103 StepSize=0.0181 RelImp=95.39%\n",
            "Epoch[1/2] Step[4721] Loss=0.1519 GradNorm=1.8024 StepSize=0.0180 RelImp=93.38%\n",
            "Epoch[1/2] Step[4722] Loss=0.0807 GradNorm=1.3362 StepSize=0.0134 RelImp=96.48%\n",
            "Epoch[1/2] Step[4723] Loss=0.6826 GradNorm=4.5401 StepSize=0.0454 RelImp=70.28%\n",
            "Epoch[1/2] Step[4724] Loss=0.2129 GradNorm=3.1489 StepSize=0.0315 RelImp=90.73%\n",
            "Epoch[1/2] Step[4725] Loss=0.8506 GradNorm=6.6402 StepSize=0.0664 RelImp=62.96%\n",
            "Epoch[1/2] Step[4726] Loss=0.7282 GradNorm=5.7576 StepSize=0.0576 RelImp=68.29%\n",
            "Epoch[1/2] Step[4727] Loss=0.1505 GradNorm=2.2910 StepSize=0.0229 RelImp=93.45%\n",
            "Epoch[1/2] Step[4728] Loss=0.7729 GradNorm=4.0229 StepSize=0.0402 RelImp=66.35%\n",
            "Epoch[1/2] Step[4729] Loss=0.5831 GradNorm=3.7756 StepSize=0.0378 RelImp=74.61%\n",
            "Epoch[1/2] Step[4730] Loss=0.1491 GradNorm=2.7672 StepSize=0.0277 RelImp=93.51%\n",
            "Epoch[1/2] Step[4731] Loss=0.4842 GradNorm=4.2356 StepSize=0.0424 RelImp=78.92%\n",
            "Epoch[1/2] Step[4732] Loss=0.2750 GradNorm=2.9020 StepSize=0.0290 RelImp=88.03%\n",
            "Epoch[1/2] Step[4733] Loss=0.5303 GradNorm=4.4457 StepSize=0.0445 RelImp=76.91%\n",
            "Epoch[1/2] Step[4734] Loss=0.4825 GradNorm=5.9419 StepSize=0.0594 RelImp=78.99%\n",
            "Epoch[1/2] Step[4735] Loss=0.4678 GradNorm=2.2582 StepSize=0.0226 RelImp=79.63%\n",
            "Epoch[1/2] Step[4736] Loss=0.7258 GradNorm=3.2118 StepSize=0.0321 RelImp=68.40%\n",
            "Epoch[1/2] Step[4737] Loss=0.9083 GradNorm=6.0464 StepSize=0.0605 RelImp=60.45%\n",
            "Epoch[1/2] Step[4738] Loss=0.2640 GradNorm=2.9683 StepSize=0.0297 RelImp=88.50%\n",
            "Epoch[1/2] Step[4739] Loss=0.4106 GradNorm=3.6828 StepSize=0.0368 RelImp=82.12%\n",
            "Epoch[1/2] Step[4740] Loss=0.4008 GradNorm=3.7801 StepSize=0.0378 RelImp=82.55%\n",
            "Epoch[1/2] Step[4741] Loss=0.2064 GradNorm=2.4214 StepSize=0.0242 RelImp=91.01%\n",
            "Epoch[1/2] Step[4742] Loss=0.2067 GradNorm=2.6806 StepSize=0.0268 RelImp=91.00%\n",
            "Epoch[1/2] Step[4743] Loss=1.2079 GradNorm=4.7718 StepSize=0.0477 RelImp=47.41%\n",
            "Epoch[1/2] Step[4744] Loss=0.2614 GradNorm=2.0156 StepSize=0.0202 RelImp=88.62%\n",
            "Epoch[1/2] Step[4745] Loss=0.2189 GradNorm=2.6516 StepSize=0.0265 RelImp=90.47%\n",
            "Epoch[1/2] Step[4746] Loss=0.1709 GradNorm=1.7608 StepSize=0.0176 RelImp=92.56%\n",
            "Epoch[1/2] Step[4747] Loss=0.1729 GradNorm=2.6526 StepSize=0.0265 RelImp=92.47%\n",
            "Epoch[1/2] Step[4748] Loss=0.2586 GradNorm=4.3432 StepSize=0.0434 RelImp=88.74%\n",
            "Epoch[1/2] Step[4749] Loss=0.1365 GradNorm=1.6065 StepSize=0.0161 RelImp=94.06%\n",
            "Epoch[1/2] Step[4750] Loss=0.5369 GradNorm=4.4841 StepSize=0.0448 RelImp=76.62%\n",
            "Epoch[1/2] Step[4751] Loss=0.5760 GradNorm=3.0997 StepSize=0.0310 RelImp=74.92%\n",
            "Epoch[1/2] Step[4752] Loss=0.1492 GradNorm=1.4793 StepSize=0.0148 RelImp=93.50%\n",
            "Epoch[1/2] Step[4753] Loss=0.5936 GradNorm=4.3603 StepSize=0.0436 RelImp=74.15%\n",
            "Epoch[1/2] Step[4754] Loss=0.3542 GradNorm=2.5692 StepSize=0.0257 RelImp=84.58%\n",
            "Epoch[1/2] Step[4755] Loss=0.1881 GradNorm=2.2146 StepSize=0.0221 RelImp=91.81%\n",
            "Epoch[1/2] Step[4756] Loss=0.3833 GradNorm=3.0786 StepSize=0.0308 RelImp=83.31%\n",
            "Epoch[1/2] Step[4757] Loss=0.3492 GradNorm=3.2189 StepSize=0.0322 RelImp=84.80%\n",
            "Epoch[1/2] Step[4758] Loss=0.4194 GradNorm=3.8654 StepSize=0.0387 RelImp=81.74%\n",
            "Epoch[1/2] Step[4759] Loss=0.1544 GradNorm=2.2768 StepSize=0.0228 RelImp=93.28%\n",
            "Epoch[1/2] Step[4760] Loss=0.3587 GradNorm=2.2632 StepSize=0.0226 RelImp=84.38%\n",
            "Epoch[1/2] Step[4761] Loss=0.0982 GradNorm=1.4941 StepSize=0.0149 RelImp=95.72%\n",
            "Epoch[1/2] Step[4762] Loss=0.4102 GradNorm=3.4987 StepSize=0.0350 RelImp=82.14%\n",
            "Epoch[1/2] Step[4763] Loss=0.3991 GradNorm=2.5191 StepSize=0.0252 RelImp=82.62%\n",
            "Epoch[1/2] Step[4764] Loss=0.3664 GradNorm=2.6117 StepSize=0.0261 RelImp=84.05%\n",
            "Epoch[1/2] Step[4765] Loss=0.0730 GradNorm=0.8898 StepSize=0.0089 RelImp=96.82%\n",
            "Epoch[1/2] Step[4766] Loss=0.3872 GradNorm=2.8543 StepSize=0.0285 RelImp=83.14%\n",
            "Epoch[1/2] Step[4767] Loss=0.7331 GradNorm=5.1180 StepSize=0.0512 RelImp=68.08%\n",
            "Epoch[1/2] Step[4768] Loss=0.2133 GradNorm=2.8184 StepSize=0.0282 RelImp=90.71%\n",
            "Epoch[1/2] Step[4769] Loss=0.2580 GradNorm=3.0727 StepSize=0.0307 RelImp=88.77%\n",
            "Epoch[1/2] Step[4770] Loss=0.2651 GradNorm=1.9880 StepSize=0.0199 RelImp=88.46%\n",
            "Epoch[1/2] Step[4771] Loss=0.4554 GradNorm=3.2368 StepSize=0.0324 RelImp=80.17%\n",
            "Epoch[1/2] Step[4772] Loss=0.1792 GradNorm=2.2427 StepSize=0.0224 RelImp=92.20%\n",
            "Epoch[1/2] Step[4773] Loss=0.0925 GradNorm=1.6080 StepSize=0.0161 RelImp=95.97%\n",
            "Epoch[1/2] Step[4774] Loss=0.7153 GradNorm=5.4165 StepSize=0.0542 RelImp=68.86%\n",
            "Epoch[1/2] Step[4775] Loss=0.2515 GradNorm=2.6100 StepSize=0.0261 RelImp=89.05%\n",
            "Epoch[1/2] Step[4776] Loss=0.4917 GradNorm=2.1426 StepSize=0.0214 RelImp=78.59%\n",
            "Epoch[1/2] Step[4777] Loss=0.1997 GradNorm=2.4112 StepSize=0.0241 RelImp=91.30%\n",
            "Epoch[1/2] Step[4778] Loss=0.5642 GradNorm=3.3669 StepSize=0.0337 RelImp=75.43%\n",
            "Epoch[1/2] Step[4779] Loss=0.3096 GradNorm=3.6219 StepSize=0.0362 RelImp=86.52%\n",
            "Epoch[1/2] Step[4780] Loss=0.2986 GradNorm=2.9524 StepSize=0.0295 RelImp=87.00%\n",
            "Epoch[1/2] Step[4781] Loss=0.8235 GradNorm=5.4925 StepSize=0.0549 RelImp=64.14%\n",
            "Epoch[1/2] Step[4782] Loss=0.1392 GradNorm=1.7474 StepSize=0.0175 RelImp=93.94%\n",
            "Epoch[1/2] Step[4783] Loss=0.1125 GradNorm=1.7153 StepSize=0.0172 RelImp=95.10%\n",
            "Epoch[1/2] Step[4784] Loss=0.2651 GradNorm=3.1367 StepSize=0.0314 RelImp=88.46%\n",
            "Epoch[1/2] Step[4785] Loss=0.2034 GradNorm=2.1536 StepSize=0.0215 RelImp=91.14%\n",
            "Epoch[1/2] Step[4786] Loss=0.1908 GradNorm=2.6183 StepSize=0.0262 RelImp=91.69%\n",
            "Epoch[1/2] Step[4787] Loss=1.1630 GradNorm=3.9775 StepSize=0.0398 RelImp=49.36%\n",
            "Epoch[1/2] Step[4788] Loss=1.1431 GradNorm=4.8253 StepSize=0.0483 RelImp=50.23%\n",
            "Epoch[1/2] Step[4789] Loss=0.2831 GradNorm=3.2309 StepSize=0.0323 RelImp=87.67%\n",
            "Epoch[1/2] Step[4790] Loss=0.2328 GradNorm=2.3525 StepSize=0.0235 RelImp=89.86%\n",
            "Epoch[1/2] Step[4791] Loss=0.1924 GradNorm=2.1627 StepSize=0.0216 RelImp=91.62%\n",
            "Epoch[1/2] Step[4792] Loss=0.4184 GradNorm=2.7813 StepSize=0.0278 RelImp=81.78%\n",
            "Epoch[1/2] Step[4793] Loss=1.1777 GradNorm=3.2495 StepSize=0.0325 RelImp=48.72%\n",
            "Epoch[1/2] Step[4794] Loss=0.6747 GradNorm=3.4594 StepSize=0.0346 RelImp=70.62%\n",
            "Epoch[1/2] Step[4795] Loss=0.0795 GradNorm=0.6892 StepSize=0.0069 RelImp=96.54%\n",
            "Epoch[1/2] Step[4796] Loss=0.0744 GradNorm=0.9181 StepSize=0.0092 RelImp=96.76%\n",
            "Epoch[1/2] Step[4797] Loss=0.3099 GradNorm=2.4977 StepSize=0.0250 RelImp=86.51%\n",
            "Epoch[1/2] Step[4798] Loss=0.4210 GradNorm=3.3549 StepSize=0.0335 RelImp=81.67%\n",
            "Epoch[1/2] Step[4799] Loss=0.1992 GradNorm=3.1446 StepSize=0.0314 RelImp=91.33%\n",
            "Epoch[1/2] Step[4800] Loss=0.2338 GradNorm=3.0057 StepSize=0.0301 RelImp=89.82%\n",
            "Epoch[1/2] Step[4801] Loss=0.0977 GradNorm=1.4780 StepSize=0.0148 RelImp=95.74%\n",
            "Epoch[1/2] Step[4802] Loss=0.1429 GradNorm=1.3592 StepSize=0.0136 RelImp=93.78%\n",
            "Epoch[1/2] Step[4803] Loss=0.2492 GradNorm=3.0266 StepSize=0.0303 RelImp=89.15%\n",
            "Epoch[1/2] Step[4804] Loss=0.3471 GradNorm=3.5764 StepSize=0.0358 RelImp=84.88%\n",
            "Epoch[1/2] Step[4805] Loss=0.1604 GradNorm=2.7935 StepSize=0.0279 RelImp=93.02%\n",
            "Epoch[1/2] Step[4806] Loss=0.7294 GradNorm=3.4066 StepSize=0.0341 RelImp=68.24%\n",
            "Epoch[1/2] Step[4807] Loss=0.7582 GradNorm=3.8807 StepSize=0.0388 RelImp=66.99%\n",
            "Epoch[1/2] Step[4808] Loss=0.3727 GradNorm=4.7239 StepSize=0.0472 RelImp=83.77%\n",
            "Epoch[1/2] Step[4809] Loss=0.1492 GradNorm=1.2022 StepSize=0.0120 RelImp=93.50%\n",
            "Epoch[1/2] Step[4810] Loss=0.1041 GradNorm=1.7967 StepSize=0.0180 RelImp=95.47%\n",
            "Epoch[1/2] Step[4811] Loss=1.1754 GradNorm=6.3304 StepSize=0.0633 RelImp=48.82%\n",
            "Epoch[1/2] Step[4812] Loss=0.1367 GradNorm=1.8507 StepSize=0.0185 RelImp=94.05%\n",
            "Epoch[1/2] Step[4813] Loss=0.1702 GradNorm=2.1088 StepSize=0.0211 RelImp=92.59%\n",
            "Epoch[1/2] Step[4814] Loss=0.7156 GradNorm=3.4058 StepSize=0.0341 RelImp=68.84%\n",
            "Epoch[1/2] Step[4815] Loss=0.4394 GradNorm=3.9009 StepSize=0.0390 RelImp=80.87%\n",
            "Epoch[1/2] Step[4816] Loss=0.1102 GradNorm=1.5754 StepSize=0.0158 RelImp=95.20%\n",
            "Epoch[1/2] Step[4817] Loss=0.1744 GradNorm=1.8835 StepSize=0.0188 RelImp=92.41%\n",
            "Epoch[1/2] Step[4818] Loss=0.7207 GradNorm=4.8079 StepSize=0.0481 RelImp=68.62%\n",
            "Epoch[1/2] Step[4819] Loss=1.0026 GradNorm=3.3322 StepSize=0.0333 RelImp=56.34%\n",
            "Epoch[1/2] Step[4820] Loss=0.1133 GradNorm=1.6736 StepSize=0.0167 RelImp=95.07%\n",
            "Epoch[1/2] Step[4821] Loss=0.7890 GradNorm=3.9943 StepSize=0.0399 RelImp=65.64%\n",
            "Epoch[1/2] Step[4822] Loss=0.6894 GradNorm=4.3140 StepSize=0.0431 RelImp=69.98%\n",
            "Epoch[1/2] Step[4823] Loss=0.1926 GradNorm=2.9234 StepSize=0.0292 RelImp=91.61%\n",
            "Epoch[1/2] Step[4824] Loss=0.5142 GradNorm=2.7704 StepSize=0.0277 RelImp=77.61%\n",
            "Epoch[1/2] Step[4825] Loss=0.2429 GradNorm=2.3382 StepSize=0.0234 RelImp=89.42%\n",
            "Epoch[1/2] Step[4826] Loss=0.2610 GradNorm=2.4871 StepSize=0.0249 RelImp=88.64%\n",
            "Epoch[1/2] Step[4827] Loss=0.3634 GradNorm=3.7100 StepSize=0.0371 RelImp=84.18%\n",
            "Epoch[1/2] Step[4828] Loss=0.2866 GradNorm=2.8130 StepSize=0.0281 RelImp=87.52%\n",
            "Epoch[1/2] Step[4829] Loss=0.5004 GradNorm=4.1391 StepSize=0.0414 RelImp=78.21%\n",
            "Epoch[1/2] Step[4830] Loss=0.6384 GradNorm=6.0330 StepSize=0.0603 RelImp=72.20%\n",
            "Epoch[1/2] Step[4831] Loss=0.2620 GradNorm=3.4174 StepSize=0.0342 RelImp=88.59%\n",
            "Epoch[1/2] Step[4832] Loss=0.1407 GradNorm=1.5915 StepSize=0.0159 RelImp=93.87%\n",
            "Epoch[1/2] Step[4833] Loss=0.0897 GradNorm=1.2921 StepSize=0.0129 RelImp=96.09%\n",
            "Epoch[1/2] Step[4834] Loss=0.1703 GradNorm=1.5028 StepSize=0.0150 RelImp=92.58%\n",
            "Epoch[1/2] Step[4835] Loss=0.2289 GradNorm=2.1810 StepSize=0.0218 RelImp=90.03%\n",
            "Epoch[1/2] Step[4836] Loss=0.2141 GradNorm=2.5919 StepSize=0.0259 RelImp=90.68%\n",
            "Epoch[1/2] Step[4837] Loss=0.1178 GradNorm=1.5421 StepSize=0.0154 RelImp=94.87%\n",
            "Epoch[1/2] Step[4838] Loss=0.3050 GradNorm=4.1551 StepSize=0.0416 RelImp=86.72%\n",
            "Epoch[1/2] Step[4839] Loss=0.5906 GradNorm=4.0791 StepSize=0.0408 RelImp=74.28%\n",
            "Epoch[1/2] Step[4840] Loss=0.3133 GradNorm=3.1479 StepSize=0.0315 RelImp=86.36%\n",
            "Epoch[1/2] Step[4841] Loss=0.5685 GradNorm=4.1028 StepSize=0.0410 RelImp=75.25%\n",
            "Epoch[1/2] Step[4842] Loss=0.7035 GradNorm=2.5906 StepSize=0.0259 RelImp=69.37%\n",
            "Epoch[1/2] Step[4843] Loss=0.2484 GradNorm=3.9955 StepSize=0.0400 RelImp=89.18%\n",
            "Epoch[1/2] Step[4844] Loss=0.5536 GradNorm=4.4228 StepSize=0.0442 RelImp=75.90%\n",
            "Epoch[1/2] Step[4845] Loss=0.0878 GradNorm=1.3315 StepSize=0.0133 RelImp=96.18%\n",
            "Epoch[1/2] Step[4846] Loss=0.3368 GradNorm=2.3406 StepSize=0.0234 RelImp=85.33%\n",
            "Epoch[1/2] Step[4847] Loss=0.5494 GradNorm=3.5836 StepSize=0.0358 RelImp=76.08%\n",
            "Epoch[1/2] Step[4848] Loss=0.3169 GradNorm=3.4020 StepSize=0.0340 RelImp=86.20%\n",
            "Epoch[1/2] Step[4849] Loss=0.1921 GradNorm=2.4128 StepSize=0.0241 RelImp=91.64%\n",
            "Epoch[1/2] Step[4850] Loss=0.7209 GradNorm=3.4920 StepSize=0.0349 RelImp=68.61%\n",
            "Epoch[1/2] Step[4851] Loss=0.4623 GradNorm=4.3125 StepSize=0.0431 RelImp=79.87%\n",
            "Epoch[1/2] Step[4852] Loss=0.5358 GradNorm=4.2479 StepSize=0.0425 RelImp=76.67%\n",
            "Epoch[1/2] Step[4853] Loss=0.4280 GradNorm=5.1500 StepSize=0.0515 RelImp=81.36%\n",
            "Epoch[1/2] Step[4854] Loss=0.9632 GradNorm=4.3952 StepSize=0.0440 RelImp=58.06%\n",
            "Epoch[1/2] Step[4855] Loss=0.2437 GradNorm=2.6502 StepSize=0.0265 RelImp=89.39%\n",
            "Epoch[1/2] Step[4856] Loss=0.1343 GradNorm=2.5567 StepSize=0.0256 RelImp=94.15%\n",
            "Epoch[1/2] Step[4857] Loss=0.2082 GradNorm=1.4297 StepSize=0.0143 RelImp=90.94%\n",
            "Epoch[1/2] Step[4858] Loss=0.1497 GradNorm=1.6824 StepSize=0.0168 RelImp=93.48%\n",
            "Epoch[1/2] Step[4859] Loss=0.3188 GradNorm=3.2529 StepSize=0.0325 RelImp=86.12%\n",
            "Epoch[1/2] Step[4860] Loss=0.2639 GradNorm=2.7565 StepSize=0.0276 RelImp=88.51%\n",
            "Epoch[1/2] Step[4861] Loss=0.3968 GradNorm=4.1126 StepSize=0.0411 RelImp=82.72%\n",
            "Epoch[1/2] Step[4862] Loss=0.1306 GradNorm=1.3284 StepSize=0.0133 RelImp=94.31%\n",
            "Epoch[1/2] Step[4863] Loss=0.3351 GradNorm=2.9139 StepSize=0.0291 RelImp=85.41%\n",
            "Epoch[1/2] Step[4864] Loss=0.2222 GradNorm=3.0167 StepSize=0.0302 RelImp=90.32%\n",
            "Epoch[1/2] Step[4865] Loss=0.5639 GradNorm=3.4155 StepSize=0.0342 RelImp=75.45%\n",
            "Epoch[1/2] Step[4866] Loss=0.4372 GradNorm=3.9738 StepSize=0.0397 RelImp=80.96%\n",
            "Epoch[1/2] Step[4867] Loss=0.5374 GradNorm=4.1801 StepSize=0.0418 RelImp=76.60%\n",
            "Epoch[1/2] Step[4868] Loss=0.5547 GradNorm=5.3843 StepSize=0.0538 RelImp=75.85%\n",
            "Epoch[1/2] Step[4869] Loss=0.1539 GradNorm=2.7379 StepSize=0.0274 RelImp=93.30%\n",
            "Epoch[1/2] Step[4870] Loss=0.1086 GradNorm=0.8034 StepSize=0.0080 RelImp=95.27%\n",
            "Epoch[1/2] Step[4871] Loss=0.2781 GradNorm=2.6687 StepSize=0.0267 RelImp=87.89%\n",
            "Epoch[1/2] Step[4872] Loss=0.4708 GradNorm=4.0323 StepSize=0.0403 RelImp=79.50%\n",
            "Epoch[1/2] Step[4873] Loss=0.6492 GradNorm=2.8906 StepSize=0.0289 RelImp=71.73%\n",
            "Epoch[1/2] Step[4874] Loss=0.2728 GradNorm=2.6940 StepSize=0.0269 RelImp=88.12%\n",
            "Epoch[1/2] Step[4875] Loss=0.3488 GradNorm=2.6388 StepSize=0.0264 RelImp=84.81%\n",
            "Epoch[1/2] Step[4876] Loss=0.5848 GradNorm=5.6495 StepSize=0.0565 RelImp=74.54%\n",
            "Epoch[1/2] Step[4877] Loss=0.2763 GradNorm=3.8719 StepSize=0.0387 RelImp=87.97%\n",
            "Epoch[1/2] Step[4878] Loss=0.3112 GradNorm=1.9321 StepSize=0.0193 RelImp=86.45%\n",
            "Epoch[1/2] Step[4879] Loss=0.1370 GradNorm=1.1721 StepSize=0.0117 RelImp=94.03%\n",
            "Epoch[1/2] Step[4880] Loss=0.4768 GradNorm=2.6706 StepSize=0.0267 RelImp=79.24%\n",
            "Epoch[1/2] Step[4881] Loss=0.2874 GradNorm=3.4388 StepSize=0.0344 RelImp=87.49%\n",
            "Epoch[1/2] Step[4882] Loss=0.2002 GradNorm=2.5397 StepSize=0.0254 RelImp=91.28%\n",
            "Epoch[1/2] Step[4883] Loss=0.2390 GradNorm=2.3982 StepSize=0.0240 RelImp=89.60%\n",
            "Epoch[1/2] Step[4884] Loss=0.1285 GradNorm=1.8376 StepSize=0.0184 RelImp=94.41%\n",
            "Epoch[1/2] Step[4885] Loss=0.1923 GradNorm=2.1317 StepSize=0.0213 RelImp=91.63%\n",
            "Epoch[1/2] Step[4886] Loss=0.6818 GradNorm=5.4985 StepSize=0.0550 RelImp=70.31%\n",
            "Epoch[1/2] Step[4887] Loss=0.2184 GradNorm=2.5206 StepSize=0.0252 RelImp=90.49%\n",
            "Epoch[1/2] Step[4888] Loss=0.5214 GradNorm=4.8647 StepSize=0.0486 RelImp=77.30%\n",
            "Epoch[1/2] Step[4889] Loss=0.5497 GradNorm=4.3958 StepSize=0.0440 RelImp=76.07%\n",
            "Epoch[1/2] Step[4890] Loss=0.0360 GradNorm=0.5405 StepSize=0.0054 RelImp=98.43%\n",
            "Epoch[1/2] Step[4891] Loss=0.3871 GradNorm=4.1271 StepSize=0.0413 RelImp=83.15%\n",
            "Epoch[1/2] Step[4892] Loss=0.1270 GradNorm=1.7930 StepSize=0.0179 RelImp=94.47%\n",
            "Epoch[1/2] Step[4893] Loss=0.1818 GradNorm=2.1144 StepSize=0.0211 RelImp=92.08%\n",
            "Epoch[1/2] Step[4894] Loss=0.0429 GradNorm=0.5974 StepSize=0.0060 RelImp=98.13%\n",
            "Epoch[1/2] Step[4895] Loss=0.3658 GradNorm=3.8771 StepSize=0.0388 RelImp=84.07%\n",
            "Epoch[1/2] Step[4896] Loss=0.1217 GradNorm=1.5041 StepSize=0.0150 RelImp=94.70%\n",
            "Epoch[1/2] Step[4897] Loss=0.1468 GradNorm=2.0672 StepSize=0.0207 RelImp=93.61%\n",
            "Epoch[1/2] Step[4898] Loss=0.1434 GradNorm=1.6994 StepSize=0.0170 RelImp=93.76%\n",
            "Epoch[1/2] Step[4899] Loss=0.5567 GradNorm=3.2606 StepSize=0.0326 RelImp=75.76%\n",
            "Epoch[1/2] Step[4900] Loss=0.4948 GradNorm=3.1365 StepSize=0.0314 RelImp=78.46%\n",
            "Epoch[1/2] Step[4901] Loss=0.0665 GradNorm=0.8240 StepSize=0.0082 RelImp=97.11%\n",
            "Epoch[1/2] Step[4902] Loss=0.2474 GradNorm=3.1409 StepSize=0.0314 RelImp=89.23%\n",
            "Epoch[1/2] Step[4903] Loss=0.6087 GradNorm=4.0085 StepSize=0.0401 RelImp=73.50%\n",
            "Epoch[1/2] Step[4904] Loss=0.2256 GradNorm=3.5966 StepSize=0.0360 RelImp=90.18%\n",
            "Epoch[1/2] Step[4905] Loss=0.2998 GradNorm=2.8424 StepSize=0.0284 RelImp=86.95%\n",
            "Epoch[1/2] Step[4906] Loss=0.2214 GradNorm=2.9026 StepSize=0.0290 RelImp=90.36%\n",
            "Epoch[1/2] Step[4907] Loss=0.6095 GradNorm=4.1494 StepSize=0.0415 RelImp=73.46%\n",
            "Epoch[1/2] Step[4908] Loss=1.0118 GradNorm=4.7317 StepSize=0.0473 RelImp=55.94%\n",
            "Epoch[1/2] Step[4909] Loss=0.1822 GradNorm=2.0737 StepSize=0.0207 RelImp=92.07%\n",
            "Epoch[1/2] Step[4910] Loss=0.1432 GradNorm=1.9874 StepSize=0.0199 RelImp=93.77%\n",
            "Epoch[1/2] Step[4911] Loss=0.3618 GradNorm=3.4798 StepSize=0.0348 RelImp=84.24%\n",
            "Epoch[1/2] Step[4912] Loss=0.2160 GradNorm=2.2936 StepSize=0.0229 RelImp=90.60%\n",
            "Epoch[1/2] Step[4913] Loss=0.2954 GradNorm=2.9107 StepSize=0.0291 RelImp=87.14%\n",
            "Epoch[1/2] Step[4914] Loss=0.1781 GradNorm=1.7199 StepSize=0.0172 RelImp=92.25%\n",
            "Epoch[1/2] Step[4915] Loss=0.1585 GradNorm=2.3322 StepSize=0.0233 RelImp=93.10%\n",
            "Epoch[1/2] Step[4916] Loss=0.5164 GradNorm=3.3674 StepSize=0.0337 RelImp=77.51%\n",
            "Epoch[1/2] Step[4917] Loss=0.3329 GradNorm=2.8088 StepSize=0.0281 RelImp=85.50%\n",
            "Epoch[1/2] Step[4918] Loss=0.2358 GradNorm=3.3767 StepSize=0.0338 RelImp=89.73%\n",
            "Epoch[1/2] Step[4919] Loss=0.1288 GradNorm=1.5765 StepSize=0.0158 RelImp=94.39%\n",
            "Epoch[1/2] Step[4920] Loss=0.1013 GradNorm=1.6372 StepSize=0.0164 RelImp=95.59%\n",
            "Epoch[1/2] Step[4921] Loss=0.3268 GradNorm=3.9486 StepSize=0.0395 RelImp=85.77%\n",
            "Epoch[1/2] Step[4922] Loss=0.3686 GradNorm=3.4744 StepSize=0.0347 RelImp=83.95%\n",
            "Epoch[1/2] Step[4923] Loss=0.5440 GradNorm=3.5311 StepSize=0.0353 RelImp=76.31%\n",
            "Epoch[1/2] Step[4924] Loss=0.2522 GradNorm=1.9344 StepSize=0.0193 RelImp=89.02%\n",
            "Epoch[1/2] Step[4925] Loss=0.1487 GradNorm=1.7839 StepSize=0.0178 RelImp=93.53%\n",
            "Epoch[1/2] Step[4926] Loss=0.4655 GradNorm=3.9342 StepSize=0.0393 RelImp=79.73%\n",
            "Epoch[1/2] Step[4927] Loss=0.2808 GradNorm=3.0684 StepSize=0.0307 RelImp=87.77%\n",
            "Epoch[1/2] Step[4928] Loss=0.1003 GradNorm=1.3996 StepSize=0.0140 RelImp=95.63%\n",
            "Epoch[1/2] Step[4929] Loss=0.3168 GradNorm=3.9463 StepSize=0.0395 RelImp=86.20%\n",
            "Epoch[1/2] Step[4930] Loss=0.3762 GradNorm=2.6545 StepSize=0.0265 RelImp=83.62%\n",
            "Epoch[1/2] Step[4931] Loss=0.0990 GradNorm=1.1386 StepSize=0.0114 RelImp=95.69%\n",
            "Epoch[1/2] Step[4932] Loss=0.4597 GradNorm=3.9507 StepSize=0.0395 RelImp=79.98%\n",
            "Epoch[1/2] Step[4933] Loss=0.2751 GradNorm=3.5046 StepSize=0.0350 RelImp=88.02%\n",
            "Epoch[1/2] Step[4934] Loss=0.1191 GradNorm=1.7290 StepSize=0.0173 RelImp=94.82%\n",
            "Epoch[1/2] Step[4935] Loss=0.5239 GradNorm=3.4347 StepSize=0.0343 RelImp=77.19%\n",
            "Epoch[1/2] Step[4936] Loss=0.6525 GradNorm=4.9204 StepSize=0.0492 RelImp=71.59%\n",
            "Epoch[1/2] Step[4937] Loss=0.8495 GradNorm=4.8695 StepSize=0.0487 RelImp=63.01%\n",
            "Epoch[1/2] Step[4938] Loss=0.1351 GradNorm=1.3140 StepSize=0.0131 RelImp=94.12%\n",
            "Epoch[1/2] Step[4939] Loss=0.1174 GradNorm=1.9729 StepSize=0.0197 RelImp=94.89%\n",
            "Epoch[1/2] Step[4940] Loss=0.1361 GradNorm=2.1132 StepSize=0.0211 RelImp=94.08%\n",
            "Epoch[1/2] Step[4941] Loss=0.2469 GradNorm=2.7206 StepSize=0.0272 RelImp=89.25%\n",
            "Epoch[1/2] Step[4942] Loss=0.0479 GradNorm=0.7412 StepSize=0.0074 RelImp=97.92%\n",
            "Epoch[1/2] Step[4943] Loss=0.4360 GradNorm=3.7650 StepSize=0.0376 RelImp=81.01%\n",
            "Epoch[1/2] Step[4944] Loss=0.1807 GradNorm=1.9827 StepSize=0.0198 RelImp=92.13%\n",
            "Epoch[1/2] Step[4945] Loss=0.1303 GradNorm=1.7963 StepSize=0.0180 RelImp=94.33%\n",
            "Epoch[1/2] Step[4946] Loss=0.2742 GradNorm=3.5298 StepSize=0.0353 RelImp=88.06%\n",
            "Epoch[1/2] Step[4947] Loss=0.0947 GradNorm=1.2221 StepSize=0.0122 RelImp=95.87%\n",
            "Epoch[1/2] Step[4948] Loss=0.4650 GradNorm=4.3052 StepSize=0.0431 RelImp=79.75%\n",
            "Epoch[1/2] Step[4949] Loss=0.3593 GradNorm=4.8652 StepSize=0.0487 RelImp=84.35%\n",
            "Epoch[1/2] Step[4950] Loss=0.1096 GradNorm=1.0656 StepSize=0.0107 RelImp=95.23%\n",
            "Epoch[1/2] Step[4951] Loss=0.1836 GradNorm=2.2617 StepSize=0.0226 RelImp=92.01%\n",
            "Epoch[1/2] Step[4952] Loss=0.6745 GradNorm=4.0354 StepSize=0.0404 RelImp=70.63%\n",
            "Epoch[1/2] Step[4953] Loss=0.4250 GradNorm=2.6813 StepSize=0.0268 RelImp=81.50%\n",
            "Epoch[1/2] Step[4954] Loss=0.0463 GradNorm=1.0857 StepSize=0.0109 RelImp=97.98%\n",
            "Epoch[1/2] Step[4955] Loss=0.0788 GradNorm=1.1439 StepSize=0.0114 RelImp=96.57%\n",
            "Epoch[1/2] Step[4956] Loss=0.3287 GradNorm=3.9082 StepSize=0.0391 RelImp=85.69%\n",
            "Epoch[1/2] Step[4957] Loss=0.6455 GradNorm=3.7981 StepSize=0.0380 RelImp=71.89%\n",
            "Epoch[1/2] Step[4958] Loss=0.6167 GradNorm=3.7909 StepSize=0.0379 RelImp=73.15%\n",
            "Epoch[1/2] Step[4959] Loss=0.2775 GradNorm=3.3284 StepSize=0.0333 RelImp=87.92%\n",
            "Epoch[1/2] Step[4960] Loss=0.2012 GradNorm=3.0209 StepSize=0.0302 RelImp=91.24%\n",
            "Epoch[1/2] Step[4961] Loss=0.1245 GradNorm=2.0299 StepSize=0.0203 RelImp=94.58%\n",
            "Epoch[1/2] Step[4962] Loss=0.1110 GradNorm=1.5499 StepSize=0.0155 RelImp=95.17%\n",
            "Epoch[1/2] Step[4963] Loss=0.3683 GradNorm=3.6104 StepSize=0.0361 RelImp=83.96%\n",
            "Epoch[1/2] Step[4964] Loss=0.3680 GradNorm=4.9378 StepSize=0.0494 RelImp=83.98%\n",
            "Epoch[1/2] Step[4965] Loss=0.5077 GradNorm=3.4996 StepSize=0.0350 RelImp=77.89%\n",
            "Epoch[1/2] Step[4966] Loss=0.4147 GradNorm=4.9967 StepSize=0.0500 RelImp=81.94%\n",
            "Epoch[1/2] Step[4967] Loss=0.3083 GradNorm=2.6752 StepSize=0.0268 RelImp=86.57%\n",
            "Epoch[1/2] Step[4968] Loss=0.0662 GradNorm=1.2790 StepSize=0.0128 RelImp=97.12%\n",
            "Epoch[1/2] Step[4969] Loss=0.2849 GradNorm=3.4794 StepSize=0.0348 RelImp=87.59%\n",
            "Epoch[1/2] Step[4970] Loss=0.1521 GradNorm=2.3567 StepSize=0.0236 RelImp=93.38%\n",
            "Epoch[1/2] Step[4971] Loss=0.1576 GradNorm=2.6263 StepSize=0.0263 RelImp=93.14%\n",
            "Epoch[1/2] Step[4972] Loss=0.2170 GradNorm=2.6648 StepSize=0.0266 RelImp=90.55%\n",
            "Epoch[1/2] Step[4973] Loss=0.4591 GradNorm=3.9000 StepSize=0.0390 RelImp=80.01%\n",
            "Epoch[1/2] Step[4974] Loss=0.1289 GradNorm=1.5842 StepSize=0.0158 RelImp=94.39%\n",
            "Epoch[1/2] Step[4975] Loss=0.2361 GradNorm=2.9384 StepSize=0.0294 RelImp=89.72%\n",
            "Epoch[1/2] Step[4976] Loss=0.0750 GradNorm=0.8282 StepSize=0.0083 RelImp=96.74%\n",
            "Epoch[1/2] Step[4977] Loss=0.5431 GradNorm=3.5901 StepSize=0.0359 RelImp=76.35%\n",
            "Epoch[1/2] Step[4978] Loss=0.3363 GradNorm=3.0946 StepSize=0.0309 RelImp=85.35%\n",
            "Epoch[1/2] Step[4979] Loss=0.1100 GradNorm=1.5595 StepSize=0.0156 RelImp=95.21%\n",
            "Epoch[1/2] Step[4980] Loss=0.2511 GradNorm=3.8207 StepSize=0.0382 RelImp=89.07%\n",
            "Epoch[1/2] Step[4981] Loss=0.4957 GradNorm=2.8407 StepSize=0.0284 RelImp=78.42%\n",
            "Epoch[1/2] Step[4982] Loss=0.4200 GradNorm=2.7884 StepSize=0.0279 RelImp=81.71%\n",
            "Epoch[1/2] Step[4983] Loss=0.2346 GradNorm=2.7294 StepSize=0.0273 RelImp=89.78%\n",
            "Epoch[1/2] Step[4984] Loss=0.3566 GradNorm=5.5981 StepSize=0.0560 RelImp=84.47%\n",
            "Epoch[1/2] Step[4985] Loss=0.5916 GradNorm=4.8620 StepSize=0.0486 RelImp=74.24%\n",
            "Epoch[1/2] Step[4986] Loss=0.7981 GradNorm=5.4569 StepSize=0.0546 RelImp=65.25%\n",
            "Epoch[1/2] Step[4987] Loss=0.2890 GradNorm=3.8265 StepSize=0.0383 RelImp=87.42%\n",
            "Epoch[1/2] Step[4988] Loss=0.2583 GradNorm=2.2385 StepSize=0.0224 RelImp=88.75%\n",
            "Epoch[1/2] Step[4989] Loss=0.1918 GradNorm=2.7172 StepSize=0.0272 RelImp=91.65%\n",
            "Epoch[1/2] Step[4990] Loss=0.1502 GradNorm=1.6870 StepSize=0.0169 RelImp=93.46%\n",
            "Epoch[1/2] Step[4991] Loss=0.2761 GradNorm=2.8409 StepSize=0.0284 RelImp=87.98%\n",
            "Epoch[1/2] Step[4992] Loss=0.1748 GradNorm=1.9963 StepSize=0.0200 RelImp=92.39%\n",
            "Epoch[1/2] Step[4993] Loss=0.1085 GradNorm=1.4272 StepSize=0.0143 RelImp=95.27%\n",
            "Epoch[1/2] Step[4994] Loss=0.1738 GradNorm=2.0911 StepSize=0.0209 RelImp=92.43%\n",
            "Epoch[1/2] Step[4995] Loss=0.5468 GradNorm=3.5943 StepSize=0.0359 RelImp=76.19%\n",
            "Epoch[1/2] Step[4996] Loss=0.2801 GradNorm=3.8231 StepSize=0.0382 RelImp=87.80%\n",
            "Epoch[1/2] Step[4997] Loss=0.2480 GradNorm=3.9576 StepSize=0.0396 RelImp=89.20%\n",
            "Epoch[1/2] Step[4998] Loss=0.4335 GradNorm=3.1010 StepSize=0.0310 RelImp=81.12%\n",
            "Epoch[1/2] Step[4999] Loss=0.4782 GradNorm=3.9210 StepSize=0.0392 RelImp=79.18%\n",
            "Epoch[1/2] Step[5000] Loss=0.1063 GradNorm=1.5445 StepSize=0.0154 RelImp=95.37%\n",
            "Epoch[1/2] Step[5001] Loss=0.5132 GradNorm=4.4287 StepSize=0.0443 RelImp=77.65%\n",
            "Epoch[1/2] Step[5002] Loss=0.4172 GradNorm=4.7738 StepSize=0.0477 RelImp=81.83%\n",
            "Epoch[1/2] Step[5003] Loss=0.0602 GradNorm=0.8189 StepSize=0.0082 RelImp=97.38%\n",
            "Epoch[1/2] Step[5004] Loss=0.2351 GradNorm=2.4984 StepSize=0.0250 RelImp=89.76%\n",
            "Epoch[1/2] Step[5005] Loss=0.1884 GradNorm=1.8378 StepSize=0.0184 RelImp=91.80%\n",
            "Epoch[1/2] Step[5006] Loss=0.6012 GradNorm=3.4396 StepSize=0.0344 RelImp=73.82%\n",
            "Epoch[1/2] Step[5007] Loss=0.0471 GradNorm=0.5115 StepSize=0.0051 RelImp=97.95%\n",
            "Epoch[1/2] Step[5008] Loss=0.1741 GradNorm=1.9798 StepSize=0.0198 RelImp=92.42%\n",
            "Epoch[1/2] Step[5009] Loss=0.3083 GradNorm=2.5420 StepSize=0.0254 RelImp=86.58%\n",
            "Epoch[1/2] Step[5010] Loss=0.1830 GradNorm=1.7902 StepSize=0.0179 RelImp=92.03%\n",
            "Epoch[1/2] Step[5011] Loss=0.2933 GradNorm=1.9652 StepSize=0.0197 RelImp=87.23%\n",
            "Epoch[1/2] Step[5012] Loss=0.2039 GradNorm=3.4209 StepSize=0.0342 RelImp=91.12%\n",
            "Epoch[1/2] Step[5013] Loss=0.2533 GradNorm=2.9142 StepSize=0.0291 RelImp=88.97%\n",
            "Epoch[1/2] Step[5014] Loss=0.4151 GradNorm=4.5061 StepSize=0.0451 RelImp=81.92%\n",
            "Epoch[1/2] Step[5015] Loss=0.1490 GradNorm=1.9570 StepSize=0.0196 RelImp=93.51%\n",
            "Epoch[1/2] Step[5016] Loss=0.3470 GradNorm=3.5352 StepSize=0.0354 RelImp=84.89%\n",
            "Epoch[1/2] Step[5017] Loss=0.5656 GradNorm=4.7294 StepSize=0.0473 RelImp=75.37%\n",
            "Epoch[1/2] Step[5018] Loss=0.1132 GradNorm=1.4562 StepSize=0.0146 RelImp=95.07%\n",
            "Epoch[1/2] Step[5019] Loss=0.2291 GradNorm=3.1756 StepSize=0.0318 RelImp=90.02%\n",
            "Epoch[1/2] Step[5020] Loss=0.3818 GradNorm=3.1050 StepSize=0.0311 RelImp=83.37%\n",
            "Epoch[1/2] Step[5021] Loss=0.6318 GradNorm=3.4979 StepSize=0.0350 RelImp=72.49%\n",
            "Epoch[1/2] Step[5022] Loss=0.6770 GradNorm=4.1399 StepSize=0.0414 RelImp=70.52%\n",
            "Epoch[1/2] Step[5023] Loss=0.4110 GradNorm=2.7443 StepSize=0.0274 RelImp=82.11%\n",
            "Epoch[1/2] Step[5024] Loss=0.1439 GradNorm=2.7960 StepSize=0.0280 RelImp=93.74%\n",
            "Epoch[1/2] Step[5025] Loss=0.0712 GradNorm=0.8364 StepSize=0.0084 RelImp=96.90%\n",
            "Epoch[1/2] Step[5026] Loss=0.3279 GradNorm=4.7711 StepSize=0.0477 RelImp=85.72%\n",
            "Epoch[1/2] Step[5027] Loss=0.0795 GradNorm=1.6064 StepSize=0.0161 RelImp=96.54%\n",
            "Epoch[1/2] Step[5028] Loss=0.4418 GradNorm=3.9259 StepSize=0.0393 RelImp=80.76%\n",
            "Epoch[1/2] Step[5029] Loss=0.3174 GradNorm=2.6568 StepSize=0.0266 RelImp=86.18%\n",
            "Epoch[1/2] Step[5030] Loss=0.4112 GradNorm=2.2231 StepSize=0.0222 RelImp=82.10%\n",
            "Epoch[1/2] Step[5031] Loss=0.1402 GradNorm=1.9952 StepSize=0.0200 RelImp=93.90%\n",
            "Epoch[1/2] Step[5032] Loss=0.3290 GradNorm=4.8459 StepSize=0.0485 RelImp=85.68%\n",
            "Epoch[1/2] Step[5033] Loss=0.5114 GradNorm=3.7676 StepSize=0.0377 RelImp=77.73%\n",
            "Epoch[1/2] Step[5034] Loss=0.1546 GradNorm=2.2173 StepSize=0.0222 RelImp=93.27%\n",
            "Epoch[1/2] Step[5035] Loss=0.3831 GradNorm=3.5048 StepSize=0.0350 RelImp=83.32%\n",
            "Epoch[1/2] Step[5036] Loss=0.1739 GradNorm=3.0996 StepSize=0.0310 RelImp=92.43%\n",
            "Epoch[1/2] Step[5037] Loss=0.4942 GradNorm=3.4036 StepSize=0.0340 RelImp=78.48%\n",
            "Epoch[1/2] Step[5038] Loss=0.3529 GradNorm=3.6277 StepSize=0.0363 RelImp=84.63%\n",
            "Epoch[1/2] Step[5039] Loss=0.1953 GradNorm=2.3327 StepSize=0.0233 RelImp=91.50%\n",
            "Epoch[1/2] Step[5040] Loss=0.5198 GradNorm=5.9534 StepSize=0.0595 RelImp=77.37%\n",
            "Epoch[1/2] Step[5041] Loss=0.3505 GradNorm=4.2214 StepSize=0.0422 RelImp=84.74%\n",
            "Epoch[1/2] Step[5042] Loss=0.7938 GradNorm=7.8602 StepSize=0.0786 RelImp=65.44%\n",
            "Epoch[1/2] Step[5043] Loss=0.2095 GradNorm=2.5665 StepSize=0.0257 RelImp=90.88%\n",
            "Epoch[1/2] Step[5044] Loss=0.4322 GradNorm=3.2784 StepSize=0.0328 RelImp=81.18%\n",
            "Epoch[1/2] Step[5045] Loss=0.3517 GradNorm=3.4678 StepSize=0.0347 RelImp=84.69%\n",
            "Epoch[1/2] Step[5046] Loss=0.1564 GradNorm=1.9475 StepSize=0.0195 RelImp=93.19%\n",
            "Epoch[1/2] Step[5047] Loss=0.3458 GradNorm=3.3922 StepSize=0.0339 RelImp=84.94%\n",
            "Epoch[1/2] Step[5048] Loss=0.4900 GradNorm=5.8235 StepSize=0.0582 RelImp=78.66%\n",
            "Epoch[1/2] Step[5049] Loss=0.5762 GradNorm=4.7425 StepSize=0.0474 RelImp=74.91%\n",
            "Epoch[1/2] Step[5050] Loss=0.3492 GradNorm=2.3212 StepSize=0.0232 RelImp=84.80%\n",
            "Epoch[1/2] Step[5051] Loss=0.1278 GradNorm=2.0569 StepSize=0.0206 RelImp=94.43%\n",
            "Epoch[1/2] Step[5052] Loss=0.3300 GradNorm=2.8855 StepSize=0.0289 RelImp=85.63%\n",
            "Epoch[1/2] Step[5053] Loss=0.3310 GradNorm=3.2359 StepSize=0.0324 RelImp=85.59%\n",
            "Epoch[1/2] Step[5054] Loss=0.0642 GradNorm=1.3248 StepSize=0.0132 RelImp=97.21%\n",
            "Epoch[1/2] Step[5055] Loss=0.0611 GradNorm=1.3734 StepSize=0.0137 RelImp=97.34%\n",
            "Epoch[1/2] Step[5056] Loss=0.7794 GradNorm=3.9709 StepSize=0.0397 RelImp=66.06%\n",
            "Epoch[1/2] Step[5057] Loss=0.4858 GradNorm=3.3589 StepSize=0.0336 RelImp=78.85%\n",
            "Epoch[1/2] Step[5058] Loss=0.3690 GradNorm=6.0843 StepSize=0.0608 RelImp=83.93%\n",
            "Epoch[1/2] Step[5059] Loss=0.1283 GradNorm=2.0357 StepSize=0.0204 RelImp=94.41%\n",
            "Epoch[1/2] Step[5060] Loss=0.6198 GradNorm=4.8392 StepSize=0.0484 RelImp=73.01%\n",
            "Epoch[1/2] Step[5061] Loss=0.1427 GradNorm=1.4484 StepSize=0.0145 RelImp=93.79%\n",
            "Epoch[1/2] Step[5062] Loss=0.1844 GradNorm=2.8880 StepSize=0.0289 RelImp=91.97%\n",
            "Epoch[1/2] Step[5063] Loss=0.3163 GradNorm=2.8548 StepSize=0.0285 RelImp=86.23%\n",
            "Epoch[1/2] Step[5064] Loss=0.6607 GradNorm=3.9164 StepSize=0.0392 RelImp=71.23%\n",
            "Epoch[1/2] Step[5065] Loss=0.0953 GradNorm=1.7325 StepSize=0.0173 RelImp=95.85%\n",
            "Epoch[1/2] Step[5066] Loss=0.9576 GradNorm=3.9876 StepSize=0.0399 RelImp=58.30%\n",
            "Epoch[1/2] Step[5067] Loss=0.2303 GradNorm=2.0607 StepSize=0.0206 RelImp=89.97%\n",
            "Epoch[1/2] Step[5068] Loss=0.1901 GradNorm=2.6073 StepSize=0.0261 RelImp=91.72%\n",
            "Epoch[1/2] Step[5069] Loss=0.4399 GradNorm=4.3286 StepSize=0.0433 RelImp=80.85%\n",
            "Epoch[1/2] Step[5070] Loss=0.1619 GradNorm=2.4963 StepSize=0.0250 RelImp=92.95%\n",
            "Epoch[1/2] Step[5071] Loss=0.1939 GradNorm=1.8543 StepSize=0.0185 RelImp=91.56%\n",
            "Epoch[1/2] Step[5072] Loss=0.4140 GradNorm=4.0851 StepSize=0.0409 RelImp=81.97%\n",
            "Epoch[1/2] Step[5073] Loss=0.4173 GradNorm=4.4315 StepSize=0.0443 RelImp=81.83%\n",
            "Epoch[1/2] Step[5074] Loss=0.2347 GradNorm=2.6058 StepSize=0.0261 RelImp=89.78%\n",
            "Epoch[1/2] Step[5075] Loss=0.1067 GradNorm=1.1439 StepSize=0.0114 RelImp=95.35%\n",
            "Epoch[1/2] Step[5076] Loss=1.0681 GradNorm=4.3110 StepSize=0.0431 RelImp=53.49%\n",
            "Epoch[1/2] Step[5077] Loss=0.4179 GradNorm=3.9644 StepSize=0.0396 RelImp=81.81%\n",
            "Epoch[1/2] Step[5078] Loss=0.3424 GradNorm=3.0391 StepSize=0.0304 RelImp=85.09%\n",
            "Epoch[1/2] Step[5079] Loss=0.2454 GradNorm=2.0838 StepSize=0.0208 RelImp=89.31%\n",
            "Epoch[1/2] Step[5080] Loss=0.1979 GradNorm=2.4269 StepSize=0.0243 RelImp=91.38%\n",
            "Epoch[1/2] Step[5081] Loss=0.4992 GradNorm=4.2866 StepSize=0.0429 RelImp=78.26%\n",
            "Epoch[1/2] Step[5082] Loss=0.2357 GradNorm=2.3041 StepSize=0.0230 RelImp=89.74%\n",
            "Epoch[1/2] Step[5083] Loss=0.7035 GradNorm=4.5732 StepSize=0.0457 RelImp=69.37%\n",
            "Epoch[1/2] Step[5084] Loss=0.9951 GradNorm=5.3822 StepSize=0.0538 RelImp=56.67%\n",
            "Epoch[1/2] Step[5085] Loss=0.1204 GradNorm=1.3760 StepSize=0.0138 RelImp=94.76%\n",
            "Epoch[1/2] Step[5086] Loss=0.3753 GradNorm=2.6127 StepSize=0.0261 RelImp=83.66%\n",
            "Epoch[1/2] Step[5087] Loss=0.1310 GradNorm=1.2222 StepSize=0.0122 RelImp=94.30%\n",
            "Epoch[1/2] Step[5088] Loss=0.3295 GradNorm=3.1846 StepSize=0.0318 RelImp=85.65%\n",
            "Epoch[1/2] Step[5089] Loss=0.3739 GradNorm=2.6604 StepSize=0.0266 RelImp=83.72%\n",
            "Epoch[1/2] Step[5090] Loss=0.5837 GradNorm=3.2899 StepSize=0.0329 RelImp=74.59%\n",
            "Epoch[1/2] Step[5091] Loss=0.2565 GradNorm=3.6091 StepSize=0.0361 RelImp=88.83%\n",
            "Epoch[1/2] Step[5092] Loss=0.5688 GradNorm=3.9477 StepSize=0.0395 RelImp=75.23%\n",
            "Epoch[1/2] Step[5093] Loss=0.2424 GradNorm=3.1504 StepSize=0.0315 RelImp=89.45%\n",
            "Epoch[1/2] Step[5094] Loss=0.5848 GradNorm=3.2525 StepSize=0.0325 RelImp=74.53%\n",
            "Epoch[1/2] Step[5095] Loss=0.2853 GradNorm=2.5072 StepSize=0.0251 RelImp=87.58%\n",
            "Epoch[1/2] Step[5096] Loss=0.6783 GradNorm=3.4513 StepSize=0.0345 RelImp=70.47%\n",
            "Epoch[1/2] Step[5097] Loss=0.2285 GradNorm=3.8255 StepSize=0.0383 RelImp=90.05%\n",
            "Epoch[1/2] Step[5098] Loss=0.6555 GradNorm=4.1072 StepSize=0.0411 RelImp=71.46%\n",
            "Epoch[1/2] Step[5099] Loss=0.4296 GradNorm=4.5390 StepSize=0.0454 RelImp=81.29%\n",
            "Epoch[1/2] Step[5100] Loss=0.5000 GradNorm=4.8232 StepSize=0.0482 RelImp=78.23%\n",
            "Epoch[1/2] Step[5101] Loss=0.3299 GradNorm=3.2706 StepSize=0.0327 RelImp=85.64%\n",
            "Epoch[1/2] Step[5102] Loss=0.3470 GradNorm=2.5155 StepSize=0.0252 RelImp=84.89%\n",
            "Epoch[1/2] Step[5103] Loss=0.4763 GradNorm=4.7554 StepSize=0.0476 RelImp=79.26%\n",
            "Epoch[1/2] Step[5104] Loss=0.1195 GradNorm=1.7795 StepSize=0.0178 RelImp=94.80%\n",
            "Epoch[1/2] Step[5105] Loss=0.8633 GradNorm=6.8805 StepSize=0.0688 RelImp=62.41%\n",
            "Epoch[1/2] Step[5106] Loss=0.1100 GradNorm=1.3712 StepSize=0.0137 RelImp=95.21%\n",
            "Epoch[1/2] Step[5107] Loss=0.1563 GradNorm=1.9561 StepSize=0.0196 RelImp=93.20%\n",
            "Epoch[1/2] Step[5108] Loss=0.1463 GradNorm=2.1717 StepSize=0.0217 RelImp=93.63%\n",
            "Epoch[1/2] Step[5109] Loss=0.5269 GradNorm=6.5012 StepSize=0.0650 RelImp=77.06%\n",
            "Epoch[1/2] Step[5110] Loss=0.5877 GradNorm=4.3199 StepSize=0.0432 RelImp=74.41%\n",
            "Epoch[1/2] Step[5111] Loss=0.5575 GradNorm=3.4692 StepSize=0.0347 RelImp=75.73%\n",
            "Epoch[1/2] Step[5112] Loss=0.2685 GradNorm=4.0952 StepSize=0.0410 RelImp=88.31%\n",
            "Epoch[1/2] Step[5113] Loss=0.3750 GradNorm=5.0088 StepSize=0.0501 RelImp=83.67%\n",
            "Epoch[1/2] Step[5114] Loss=0.1263 GradNorm=1.4580 StepSize=0.0146 RelImp=94.50%\n",
            "Epoch[1/2] Step[5115] Loss=0.1265 GradNorm=2.2285 StepSize=0.0223 RelImp=94.49%\n",
            "Epoch[1/2] Step[5116] Loss=0.2392 GradNorm=2.8498 StepSize=0.0285 RelImp=89.59%\n",
            "Epoch[1/2] Step[5117] Loss=0.0663 GradNorm=1.0658 StepSize=0.0107 RelImp=97.11%\n",
            "Epoch[1/2] Step[5118] Loss=0.2745 GradNorm=3.4304 StepSize=0.0343 RelImp=88.05%\n",
            "Epoch[1/2] Step[5119] Loss=0.2779 GradNorm=3.8527 StepSize=0.0385 RelImp=87.90%\n",
            "Epoch[1/2] Step[5120] Loss=0.1361 GradNorm=2.1444 StepSize=0.0214 RelImp=94.08%\n",
            "Epoch[1/2] Step[5121] Loss=0.7227 GradNorm=3.3968 StepSize=0.0340 RelImp=68.53%\n",
            "Epoch[1/2] Step[5122] Loss=0.3834 GradNorm=2.8577 StepSize=0.0286 RelImp=83.31%\n",
            "Epoch[1/2] Step[5123] Loss=0.2636 GradNorm=2.5709 StepSize=0.0257 RelImp=88.52%\n",
            "Epoch[1/2] Step[5124] Loss=0.3228 GradNorm=4.3604 StepSize=0.0436 RelImp=85.94%\n",
            "Epoch[1/2] Step[5125] Loss=0.3493 GradNorm=3.4432 StepSize=0.0344 RelImp=84.79%\n",
            "Epoch[1/2] Step[5126] Loss=0.1948 GradNorm=2.9043 StepSize=0.0290 RelImp=91.52%\n",
            "Epoch[1/2] Step[5127] Loss=0.4548 GradNorm=3.5580 StepSize=0.0356 RelImp=80.20%\n",
            "Epoch[1/2] Step[5128] Loss=0.1571 GradNorm=2.0216 StepSize=0.0202 RelImp=93.16%\n",
            "Epoch[1/2] Step[5129] Loss=0.4742 GradNorm=4.1456 StepSize=0.0415 RelImp=79.35%\n",
            "Epoch[1/2] Step[5130] Loss=0.7393 GradNorm=5.1716 StepSize=0.0517 RelImp=67.81%\n",
            "Epoch[1/2] Step[5131] Loss=0.2651 GradNorm=4.1317 StepSize=0.0413 RelImp=88.46%\n",
            "Epoch[1/2] Step[5132] Loss=0.3159 GradNorm=2.5148 StepSize=0.0251 RelImp=86.24%\n",
            "Epoch[1/2] Step[5133] Loss=0.2867 GradNorm=2.4779 StepSize=0.0248 RelImp=87.52%\n",
            "Epoch[1/2] Step[5134] Loss=0.3904 GradNorm=5.4098 StepSize=0.0541 RelImp=83.00%\n",
            "Epoch[1/2] Step[5135] Loss=0.4577 GradNorm=4.5552 StepSize=0.0456 RelImp=80.07%\n",
            "Epoch[1/2] Step[5136] Loss=0.1993 GradNorm=2.7226 StepSize=0.0272 RelImp=91.32%\n",
            "Epoch[1/2] Step[5137] Loss=0.2529 GradNorm=2.5743 StepSize=0.0257 RelImp=88.99%\n",
            "Epoch[1/2] Step[5138] Loss=0.3005 GradNorm=3.8754 StepSize=0.0388 RelImp=86.91%\n",
            "Epoch[1/2] Step[5139] Loss=0.1771 GradNorm=2.5248 StepSize=0.0252 RelImp=92.29%\n",
            "Epoch[1/2] Step[5140] Loss=0.3352 GradNorm=3.6192 StepSize=0.0362 RelImp=85.40%\n",
            "Epoch[1/2] Step[5141] Loss=0.3393 GradNorm=2.9060 StepSize=0.0291 RelImp=85.23%\n",
            "Epoch[1/2] Step[5142] Loss=0.1505 GradNorm=1.4468 StepSize=0.0145 RelImp=93.45%\n",
            "Epoch[1/2] Step[5143] Loss=0.6380 GradNorm=4.0338 StepSize=0.0403 RelImp=72.22%\n",
            "Epoch[1/2] Step[5144] Loss=0.6115 GradNorm=5.0274 StepSize=0.0503 RelImp=73.38%\n",
            "Epoch[1/2] Step[5145] Loss=0.1117 GradNorm=2.0288 StepSize=0.0203 RelImp=95.14%\n",
            "Epoch[1/2] Step[5146] Loss=0.2973 GradNorm=2.1548 StepSize=0.0215 RelImp=87.05%\n",
            "Epoch[1/2] Step[5147] Loss=0.1446 GradNorm=3.0093 StepSize=0.0301 RelImp=93.70%\n",
            "Epoch[1/2] Step[5148] Loss=0.3000 GradNorm=2.9194 StepSize=0.0292 RelImp=86.94%\n",
            "Epoch[1/2] Step[5149] Loss=0.6370 GradNorm=5.6961 StepSize=0.0570 RelImp=72.26%\n",
            "Epoch[1/2] Step[5150] Loss=0.1386 GradNorm=2.5955 StepSize=0.0260 RelImp=93.96%\n",
            "Epoch[1/2] Step[5151] Loss=0.2961 GradNorm=3.0565 StepSize=0.0306 RelImp=87.11%\n",
            "Epoch[1/2] Step[5152] Loss=0.8750 GradNorm=4.8507 StepSize=0.0485 RelImp=61.90%\n",
            "Epoch[1/2] Step[5153] Loss=0.6982 GradNorm=4.4419 StepSize=0.0444 RelImp=69.60%\n",
            "Epoch[1/2] Step[5154] Loss=0.7494 GradNorm=3.2613 StepSize=0.0326 RelImp=67.37%\n",
            "Epoch[1/2] Step[5155] Loss=0.4184 GradNorm=2.2208 StepSize=0.0222 RelImp=81.78%\n",
            "Epoch[1/2] Step[5156] Loss=0.1559 GradNorm=1.8667 StepSize=0.0187 RelImp=93.21%\n",
            "Epoch[1/2] Step[5157] Loss=0.0586 GradNorm=0.7702 StepSize=0.0077 RelImp=97.45%\n",
            "Epoch[1/2] Step[5158] Loss=0.8935 GradNorm=6.3134 StepSize=0.0631 RelImp=61.09%\n",
            "Epoch[1/2] Step[5159] Loss=0.2536 GradNorm=3.8814 StepSize=0.0388 RelImp=88.96%\n",
            "Epoch[1/2] Step[5160] Loss=0.4749 GradNorm=4.0680 StepSize=0.0407 RelImp=79.32%\n",
            "Epoch[1/2] Step[5161] Loss=0.5461 GradNorm=5.2983 StepSize=0.0530 RelImp=76.22%\n",
            "Epoch[1/2] Step[5162] Loss=0.2490 GradNorm=3.2643 StepSize=0.0326 RelImp=89.16%\n",
            "Epoch[1/2] Step[5163] Loss=0.2186 GradNorm=2.5301 StepSize=0.0253 RelImp=90.48%\n",
            "Epoch[1/2] Step[5164] Loss=0.2830 GradNorm=3.2110 StepSize=0.0321 RelImp=87.68%\n",
            "Epoch[1/2] Step[5165] Loss=0.7518 GradNorm=3.8303 StepSize=0.0383 RelImp=67.26%\n",
            "Epoch[1/2] Step[5166] Loss=0.1764 GradNorm=2.1102 StepSize=0.0211 RelImp=92.32%\n",
            "Epoch[1/2] Step[5167] Loss=0.2189 GradNorm=2.2758 StepSize=0.0228 RelImp=90.47%\n",
            "Epoch[1/2] Step[5168] Loss=0.4017 GradNorm=4.2983 StepSize=0.0430 RelImp=82.51%\n",
            "Epoch[1/2] Step[5169] Loss=0.1229 GradNorm=1.4735 StepSize=0.0147 RelImp=94.65%\n",
            "Epoch[1/2] Step[5170] Loss=0.5019 GradNorm=4.9385 StepSize=0.0494 RelImp=78.15%\n",
            "Epoch[1/2] Step[5171] Loss=0.2322 GradNorm=2.0827 StepSize=0.0208 RelImp=89.89%\n",
            "Epoch[1/2] Step[5172] Loss=0.6469 GradNorm=4.3771 StepSize=0.0438 RelImp=71.83%\n",
            "Epoch[1/2] Step[5173] Loss=0.3591 GradNorm=2.8626 StepSize=0.0286 RelImp=84.36%\n",
            "Epoch[1/2] Step[5174] Loss=0.2501 GradNorm=2.5443 StepSize=0.0254 RelImp=89.11%\n",
            "Epoch[1/2] Step[5175] Loss=0.2172 GradNorm=1.7020 StepSize=0.0170 RelImp=90.54%\n",
            "Epoch[1/2] Step[5176] Loss=0.3766 GradNorm=2.3161 StepSize=0.0232 RelImp=83.60%\n",
            "Epoch[1/2] Step[5177] Loss=0.2077 GradNorm=1.6516 StepSize=0.0165 RelImp=90.96%\n",
            "Epoch[1/2] Step[5178] Loss=0.7691 GradNorm=3.6264 StepSize=0.0363 RelImp=66.51%\n",
            "Epoch[1/2] Step[5179] Loss=0.2012 GradNorm=2.3778 StepSize=0.0238 RelImp=91.24%\n",
            "Epoch[1/2] Step[5180] Loss=0.2335 GradNorm=3.6964 StepSize=0.0370 RelImp=89.83%\n",
            "Epoch[1/2] Step[5181] Loss=0.0840 GradNorm=1.1384 StepSize=0.0114 RelImp=96.34%\n",
            "Epoch[1/2] Step[5182] Loss=0.1253 GradNorm=1.8807 StepSize=0.0188 RelImp=94.54%\n",
            "Epoch[1/2] Step[5183] Loss=0.4342 GradNorm=3.7855 StepSize=0.0379 RelImp=81.10%\n",
            "Epoch[1/2] Step[5184] Loss=0.1440 GradNorm=1.3982 StepSize=0.0140 RelImp=93.73%\n",
            "Epoch[1/2] Step[5185] Loss=0.2002 GradNorm=3.4996 StepSize=0.0350 RelImp=91.28%\n",
            "Epoch[1/2] Step[5186] Loss=0.0849 GradNorm=1.5804 StepSize=0.0158 RelImp=96.30%\n",
            "Epoch[1/2] Step[5187] Loss=0.3475 GradNorm=3.3822 StepSize=0.0338 RelImp=84.87%\n",
            "Epoch[1/2] Step[5188] Loss=0.3652 GradNorm=3.8519 StepSize=0.0385 RelImp=84.10%\n",
            "Epoch[1/2] Step[5189] Loss=0.2974 GradNorm=3.5607 StepSize=0.0356 RelImp=87.05%\n",
            "Epoch[1/2] Step[5190] Loss=0.1104 GradNorm=1.2915 StepSize=0.0129 RelImp=95.19%\n",
            "Epoch[1/2] Step[5191] Loss=0.2370 GradNorm=3.3690 StepSize=0.0337 RelImp=89.68%\n",
            "Epoch[1/2] Step[5192] Loss=0.2505 GradNorm=3.5431 StepSize=0.0354 RelImp=89.09%\n",
            "Epoch[1/2] Step[5193] Loss=0.1289 GradNorm=1.5169 StepSize=0.0152 RelImp=94.39%\n",
            "Epoch[1/2] Step[5194] Loss=0.1131 GradNorm=1.5573 StepSize=0.0156 RelImp=95.08%\n",
            "Epoch[1/2] Step[5195] Loss=0.2835 GradNorm=3.7549 StepSize=0.0375 RelImp=87.66%\n",
            "Epoch[1/2] Step[5196] Loss=0.1966 GradNorm=2.6080 StepSize=0.0261 RelImp=91.44%\n",
            "Epoch[1/2] Step[5197] Loss=0.0521 GradNorm=1.1528 StepSize=0.0115 RelImp=97.73%\n",
            "Epoch[1/2] Step[5198] Loss=0.1232 GradNorm=1.7173 StepSize=0.0172 RelImp=94.64%\n",
            "Epoch[1/2] Step[5199] Loss=0.5530 GradNorm=3.8887 StepSize=0.0389 RelImp=75.92%\n",
            "Epoch[1/2] Step[5200] Loss=0.2644 GradNorm=2.9479 StepSize=0.0295 RelImp=88.49%\n",
            "Epoch[1/2] Step[5201] Loss=0.1188 GradNorm=1.4983 StepSize=0.0150 RelImp=94.83%\n",
            "Epoch[1/2] Step[5202] Loss=0.4788 GradNorm=3.1546 StepSize=0.0315 RelImp=79.15%\n",
            "Epoch[1/2] Step[5203] Loss=0.2311 GradNorm=3.0031 StepSize=0.0300 RelImp=89.94%\n",
            "Epoch[1/2] Step[5204] Loss=0.5157 GradNorm=4.2920 StepSize=0.0429 RelImp=77.55%\n",
            "Epoch[1/2] Step[5205] Loss=0.6089 GradNorm=3.5412 StepSize=0.0354 RelImp=73.49%\n",
            "Epoch[1/2] Step[5206] Loss=0.4150 GradNorm=3.6876 StepSize=0.0369 RelImp=81.93%\n",
            "Epoch[1/2] Step[5207] Loss=0.2598 GradNorm=2.0991 StepSize=0.0210 RelImp=88.69%\n",
            "Epoch[1/2] Step[5208] Loss=0.1146 GradNorm=1.7134 StepSize=0.0171 RelImp=95.01%\n",
            "Epoch[1/2] Step[5209] Loss=0.6598 GradNorm=4.2846 StepSize=0.0428 RelImp=71.27%\n",
            "Epoch[1/2] Step[5210] Loss=0.5292 GradNorm=6.1512 StepSize=0.0615 RelImp=76.96%\n",
            "Epoch[1/2] Step[5211] Loss=0.1026 GradNorm=1.7945 StepSize=0.0179 RelImp=95.53%\n",
            "Epoch[1/2] Step[5212] Loss=0.1018 GradNorm=1.5109 StepSize=0.0151 RelImp=95.57%\n",
            "Epoch[1/2] Step[5213] Loss=0.1234 GradNorm=1.6628 StepSize=0.0166 RelImp=94.63%\n",
            "Epoch[1/2] Step[5214] Loss=0.4876 GradNorm=3.4840 StepSize=0.0348 RelImp=78.77%\n",
            "Epoch[1/2] Step[5215] Loss=0.2000 GradNorm=2.5520 StepSize=0.0255 RelImp=91.29%\n",
            "Epoch[1/2] Step[5216] Loss=0.5437 GradNorm=3.6162 StepSize=0.0362 RelImp=76.33%\n",
            "Epoch[1/2] Step[5217] Loss=0.1728 GradNorm=2.0487 StepSize=0.0205 RelImp=92.48%\n",
            "Epoch[1/2] Step[5218] Loss=0.5530 GradNorm=4.6693 StepSize=0.0467 RelImp=75.92%\n",
            "Epoch[1/2] Step[5219] Loss=0.3469 GradNorm=2.8085 StepSize=0.0281 RelImp=84.89%\n",
            "Epoch[1/2] Step[5220] Loss=0.3087 GradNorm=3.8537 StepSize=0.0385 RelImp=86.56%\n",
            "Epoch[1/2] Step[5221] Loss=0.0991 GradNorm=1.6300 StepSize=0.0163 RelImp=95.68%\n",
            "Epoch[1/2] Step[5222] Loss=0.2944 GradNorm=2.5288 StepSize=0.0253 RelImp=87.18%\n",
            "Epoch[1/2] Step[5223] Loss=0.1290 GradNorm=1.7772 StepSize=0.0178 RelImp=94.38%\n",
            "Epoch[1/2] Step[5224] Loss=0.5259 GradNorm=3.4047 StepSize=0.0340 RelImp=77.10%\n",
            "Epoch[1/2] Step[5225] Loss=0.5285 GradNorm=5.5864 StepSize=0.0559 RelImp=76.99%\n",
            "Epoch[1/2] Step[5226] Loss=0.8092 GradNorm=7.6129 StepSize=0.0761 RelImp=64.77%\n",
            "Epoch[1/2] Step[5227] Loss=0.4065 GradNorm=3.8252 StepSize=0.0383 RelImp=82.30%\n",
            "Epoch[1/2] Step[5228] Loss=0.1669 GradNorm=2.3661 StepSize=0.0237 RelImp=92.73%\n",
            "Epoch[1/2] Step[5229] Loss=0.4205 GradNorm=3.9712 StepSize=0.0397 RelImp=81.69%\n",
            "Epoch[1/2] Step[5230] Loss=0.6448 GradNorm=5.8554 StepSize=0.0586 RelImp=71.93%\n",
            "Epoch[1/2] Step[5231] Loss=0.1724 GradNorm=2.8297 StepSize=0.0283 RelImp=92.49%\n",
            "Epoch[1/2] Step[5232] Loss=0.1796 GradNorm=1.8723 StepSize=0.0187 RelImp=92.18%\n",
            "Epoch[1/2] Step[5233] Loss=0.1717 GradNorm=2.2725 StepSize=0.0227 RelImp=92.52%\n",
            "Epoch[1/2] Step[5234] Loss=0.6403 GradNorm=3.2711 StepSize=0.0327 RelImp=72.12%\n",
            "Epoch[1/2] Step[5235] Loss=0.2060 GradNorm=3.1106 StepSize=0.0311 RelImp=91.03%\n",
            "Epoch[1/2] Step[5236] Loss=0.4946 GradNorm=2.4301 StepSize=0.0243 RelImp=78.46%\n",
            "Epoch[1/2] Step[5237] Loss=0.4308 GradNorm=4.0682 StepSize=0.0407 RelImp=81.24%\n",
            "Epoch[1/2] Step[5238] Loss=0.6005 GradNorm=3.2849 StepSize=0.0328 RelImp=73.85%\n",
            "Epoch[1/2] Step[5239] Loss=0.4208 GradNorm=4.5013 StepSize=0.0450 RelImp=81.68%\n",
            "Epoch[1/2] Step[5240] Loss=0.1083 GradNorm=1.5092 StepSize=0.0151 RelImp=95.28%\n",
            "Epoch[1/2] Step[5241] Loss=0.2203 GradNorm=2.9243 StepSize=0.0292 RelImp=90.41%\n",
            "Epoch[1/2] Step[5242] Loss=0.0777 GradNorm=0.9404 StepSize=0.0094 RelImp=96.62%\n",
            "Epoch[1/2] Step[5243] Loss=0.6682 GradNorm=3.6391 StepSize=0.0364 RelImp=70.90%\n",
            "Epoch[1/2] Step[5244] Loss=1.1273 GradNorm=4.0780 StepSize=0.0408 RelImp=50.92%\n",
            "Epoch[1/2] Step[5245] Loss=0.5083 GradNorm=4.5509 StepSize=0.0455 RelImp=77.87%\n",
            "Epoch[1/2] Step[5246] Loss=0.0795 GradNorm=1.3823 StepSize=0.0138 RelImp=96.54%\n",
            "Epoch[1/2] Step[5247] Loss=0.4840 GradNorm=3.1732 StepSize=0.0317 RelImp=78.92%\n",
            "Epoch[1/2] Step[5248] Loss=0.6382 GradNorm=3.9805 StepSize=0.0398 RelImp=72.21%\n",
            "Epoch[1/2] Step[5249] Loss=0.1576 GradNorm=1.9796 StepSize=0.0198 RelImp=93.14%\n",
            "Epoch[1/2] Step[5250] Loss=0.2657 GradNorm=3.3720 StepSize=0.0337 RelImp=88.43%\n",
            "Epoch[1/2] Step[5251] Loss=0.3304 GradNorm=2.8005 StepSize=0.0280 RelImp=85.61%\n",
            "Epoch[1/2] Step[5252] Loss=0.3496 GradNorm=3.3756 StepSize=0.0338 RelImp=84.78%\n",
            "Epoch[1/2] Step[5253] Loss=0.1189 GradNorm=0.9037 StepSize=0.0090 RelImp=94.82%\n",
            "Epoch[1/2] Step[5254] Loss=0.0833 GradNorm=0.8756 StepSize=0.0088 RelImp=96.37%\n",
            "Epoch[1/2] Step[5255] Loss=0.7916 GradNorm=6.2547 StepSize=0.0625 RelImp=65.53%\n",
            "Epoch[1/2] Step[5256] Loss=1.0198 GradNorm=4.6896 StepSize=0.0469 RelImp=55.60%\n",
            "Epoch[1/2] Step[5257] Loss=0.3600 GradNorm=3.6594 StepSize=0.0366 RelImp=84.32%\n",
            "Epoch[1/2] Step[5258] Loss=0.3581 GradNorm=3.2430 StepSize=0.0324 RelImp=84.41%\n",
            "Epoch[1/2] Step[5259] Loss=0.5801 GradNorm=5.0991 StepSize=0.0510 RelImp=74.74%\n",
            "Epoch[1/2] Step[5260] Loss=0.2869 GradNorm=3.2831 StepSize=0.0328 RelImp=87.51%\n",
            "Epoch[1/2] Step[5261] Loss=0.1338 GradNorm=1.2499 StepSize=0.0125 RelImp=94.17%\n",
            "Epoch[1/2] Step[5262] Loss=0.1645 GradNorm=1.4973 StepSize=0.0150 RelImp=92.84%\n",
            "Epoch[1/2] Step[5263] Loss=0.4454 GradNorm=3.3833 StepSize=0.0338 RelImp=80.61%\n",
            "Epoch[1/2] Step[5264] Loss=0.3013 GradNorm=3.5971 StepSize=0.0360 RelImp=86.88%\n",
            "Epoch[1/2] Step[5265] Loss=0.2309 GradNorm=4.7677 StepSize=0.0477 RelImp=89.95%\n",
            "Epoch[1/2] Step[5266] Loss=0.1975 GradNorm=2.4688 StepSize=0.0247 RelImp=91.40%\n",
            "Epoch[1/2] Step[5267] Loss=0.3064 GradNorm=2.0299 StepSize=0.0203 RelImp=86.66%\n",
            "Epoch[1/2] Step[5268] Loss=0.3501 GradNorm=2.8981 StepSize=0.0290 RelImp=84.76%\n",
            "Epoch[1/2] Step[5269] Loss=0.4287 GradNorm=3.5178 StepSize=0.0352 RelImp=81.33%\n",
            "Epoch[1/2] Step[5270] Loss=0.0904 GradNorm=1.5785 StepSize=0.0158 RelImp=96.06%\n",
            "Epoch[1/2] Step[5271] Loss=0.6343 GradNorm=6.0294 StepSize=0.0603 RelImp=72.38%\n",
            "Epoch[1/2] Step[5272] Loss=0.2835 GradNorm=3.0985 StepSize=0.0310 RelImp=87.66%\n",
            "Epoch[1/2] Step[5273] Loss=0.2284 GradNorm=2.1721 StepSize=0.0217 RelImp=90.06%\n",
            "Epoch[1/2] Step[5274] Loss=0.6566 GradNorm=5.0314 StepSize=0.0503 RelImp=71.41%\n",
            "Epoch[1/2] Step[5275] Loss=0.0998 GradNorm=1.4779 StepSize=0.0148 RelImp=95.65%\n",
            "Epoch[1/2] Step[5276] Loss=0.6948 GradNorm=4.9754 StepSize=0.0498 RelImp=69.75%\n",
            "Epoch[1/2] Step[5277] Loss=0.4315 GradNorm=3.6644 StepSize=0.0366 RelImp=81.21%\n",
            "Epoch[1/2] Step[5278] Loss=0.1304 GradNorm=2.1633 StepSize=0.0216 RelImp=94.32%\n",
            "Epoch[1/2] Step[5279] Loss=0.1948 GradNorm=2.2863 StepSize=0.0229 RelImp=91.52%\n",
            "Epoch[1/2] Step[5280] Loss=0.2157 GradNorm=2.1566 StepSize=0.0216 RelImp=90.61%\n",
            "Epoch[1/2] Step[5281] Loss=0.3243 GradNorm=2.1435 StepSize=0.0214 RelImp=85.88%\n",
            "Epoch[1/2] Step[5282] Loss=0.1709 GradNorm=4.0650 StepSize=0.0407 RelImp=92.56%\n",
            "Epoch[1/2] Step[5283] Loss=0.5428 GradNorm=3.5945 StepSize=0.0359 RelImp=76.37%\n",
            "Epoch[1/2] Step[5284] Loss=0.2673 GradNorm=3.4326 StepSize=0.0343 RelImp=88.36%\n",
            "Epoch[1/2] Step[5285] Loss=0.1966 GradNorm=2.7691 StepSize=0.0277 RelImp=91.44%\n",
            "Epoch[1/2] Step[5286] Loss=0.4336 GradNorm=3.9865 StepSize=0.0399 RelImp=81.12%\n",
            "Epoch[1/2] Step[5287] Loss=0.5681 GradNorm=3.6980 StepSize=0.0370 RelImp=75.27%\n",
            "Epoch[1/2] Step[5288] Loss=0.2220 GradNorm=2.7548 StepSize=0.0275 RelImp=90.33%\n",
            "Epoch[1/2] Step[5289] Loss=0.3858 GradNorm=4.1101 StepSize=0.0411 RelImp=83.20%\n",
            "Epoch[1/2] Step[5290] Loss=0.2226 GradNorm=2.2935 StepSize=0.0229 RelImp=90.31%\n",
            "Epoch[1/2] Step[5291] Loss=0.3362 GradNorm=3.5708 StepSize=0.0357 RelImp=85.36%\n",
            "Epoch[1/2] Step[5292] Loss=0.4934 GradNorm=2.3175 StepSize=0.0232 RelImp=78.52%\n",
            "Epoch[1/2] Step[5293] Loss=0.2173 GradNorm=2.2555 StepSize=0.0226 RelImp=90.54%\n",
            "Epoch[1/2] Step[5294] Loss=0.2676 GradNorm=2.8911 StepSize=0.0289 RelImp=88.35%\n",
            "Epoch[1/2] Step[5295] Loss=0.2052 GradNorm=2.7372 StepSize=0.0274 RelImp=91.07%\n",
            "Epoch[1/2] Step[5296] Loss=0.4049 GradNorm=3.0669 StepSize=0.0307 RelImp=82.37%\n",
            "Epoch[1/2] Step[5297] Loss=0.3703 GradNorm=4.0519 StepSize=0.0405 RelImp=83.87%\n",
            "Epoch[1/2] Step[5298] Loss=0.1303 GradNorm=2.2598 StepSize=0.0226 RelImp=94.33%\n",
            "Epoch[1/2] Step[5299] Loss=0.8109 GradNorm=4.3397 StepSize=0.0434 RelImp=64.69%\n",
            "Epoch[1/2] Step[5300] Loss=0.2974 GradNorm=3.8730 StepSize=0.0387 RelImp=87.05%\n",
            "Epoch[1/2] Step[5301] Loss=0.2887 GradNorm=2.1239 StepSize=0.0212 RelImp=87.43%\n",
            "Epoch[1/2] Step[5302] Loss=0.3183 GradNorm=3.1946 StepSize=0.0319 RelImp=86.14%\n",
            "Epoch[1/2] Step[5303] Loss=0.0952 GradNorm=1.2768 StepSize=0.0128 RelImp=95.86%\n",
            "Epoch[1/2] Step[5304] Loss=0.5024 GradNorm=2.9606 StepSize=0.0296 RelImp=78.13%\n",
            "Epoch[1/2] Step[5305] Loss=0.0789 GradNorm=0.6978 StepSize=0.0070 RelImp=96.56%\n",
            "Epoch[1/2] Step[5306] Loss=0.2073 GradNorm=2.5429 StepSize=0.0254 RelImp=90.97%\n",
            "Epoch[1/2] Step[5307] Loss=0.1168 GradNorm=1.2793 StepSize=0.0128 RelImp=94.91%\n",
            "Epoch[1/2] Step[5308] Loss=0.0906 GradNorm=1.1649 StepSize=0.0116 RelImp=96.05%\n",
            "Epoch[1/2] Step[5309] Loss=0.3931 GradNorm=3.6664 StepSize=0.0367 RelImp=82.88%\n",
            "Epoch[1/2] Step[5310] Loss=0.1983 GradNorm=2.5289 StepSize=0.0253 RelImp=91.37%\n",
            "Epoch[1/2] Step[5311] Loss=0.6266 GradNorm=2.6508 StepSize=0.0265 RelImp=72.72%\n",
            "Epoch[1/2] Step[5312] Loss=0.2325 GradNorm=3.1202 StepSize=0.0312 RelImp=89.88%\n",
            "Epoch[1/2] Step[5313] Loss=0.8614 GradNorm=3.6888 StepSize=0.0369 RelImp=62.49%\n",
            "Epoch[1/2] Step[5314] Loss=1.0213 GradNorm=7.3448 StepSize=0.0734 RelImp=55.53%\n",
            "Epoch[1/2] Step[5315] Loss=0.1834 GradNorm=2.0915 StepSize=0.0209 RelImp=92.02%\n",
            "Epoch[1/2] Step[5316] Loss=1.1211 GradNorm=4.1991 StepSize=0.0420 RelImp=51.18%\n",
            "Epoch[1/2] Step[5317] Loss=0.5575 GradNorm=4.5678 StepSize=0.0457 RelImp=75.73%\n",
            "Epoch[1/2] Step[5318] Loss=0.2022 GradNorm=2.3010 StepSize=0.0230 RelImp=91.20%\n",
            "Epoch[1/2] Step[5319] Loss=0.3134 GradNorm=4.1414 StepSize=0.0414 RelImp=86.35%\n",
            "Epoch[1/2] Step[5320] Loss=0.0923 GradNorm=0.8221 StepSize=0.0082 RelImp=95.98%\n",
            "Epoch[1/2] Step[5321] Loss=0.2418 GradNorm=2.8150 StepSize=0.0282 RelImp=89.47%\n",
            "Epoch[1/2] Step[5322] Loss=0.4311 GradNorm=3.9786 StepSize=0.0398 RelImp=81.23%\n",
            "Epoch[1/2] Step[5323] Loss=0.3392 GradNorm=4.4648 StepSize=0.0446 RelImp=85.23%\n",
            "Epoch[1/2] Step[5324] Loss=0.2327 GradNorm=2.5281 StepSize=0.0253 RelImp=89.87%\n",
            "Epoch[1/2] Step[5325] Loss=0.2083 GradNorm=2.4578 StepSize=0.0246 RelImp=90.93%\n",
            "Epoch[1/2] Step[5326] Loss=0.4564 GradNorm=3.9864 StepSize=0.0399 RelImp=80.13%\n",
            "Epoch[1/2] Step[5327] Loss=0.4292 GradNorm=3.6465 StepSize=0.0365 RelImp=81.31%\n",
            "Epoch[1/2] Step[5328] Loss=0.2149 GradNorm=2.6711 StepSize=0.0267 RelImp=90.64%\n",
            "Epoch[1/2] Step[5329] Loss=0.5427 GradNorm=4.1998 StepSize=0.0420 RelImp=76.37%\n",
            "Epoch[1/2] Step[5330] Loss=0.4105 GradNorm=3.4354 StepSize=0.0344 RelImp=82.13%\n",
            "Epoch[1/2] Step[5331] Loss=0.8271 GradNorm=4.2461 StepSize=0.0425 RelImp=63.99%\n",
            "Epoch[1/2] Step[5332] Loss=0.1620 GradNorm=2.3961 StepSize=0.0240 RelImp=92.95%\n",
            "Epoch[1/2] Step[5333] Loss=0.1081 GradNorm=2.5623 StepSize=0.0256 RelImp=95.29%\n",
            "Epoch[1/2] Step[5334] Loss=0.3659 GradNorm=3.3815 StepSize=0.0338 RelImp=84.07%\n",
            "Epoch[1/2] Step[5335] Loss=0.1526 GradNorm=2.2380 StepSize=0.0224 RelImp=93.36%\n",
            "Epoch[1/2] Step[5336] Loss=0.3798 GradNorm=2.5383 StepSize=0.0254 RelImp=83.46%\n",
            "Epoch[1/2] Step[5337] Loss=0.1346 GradNorm=1.7945 StepSize=0.0179 RelImp=94.14%\n",
            "Epoch[1/2] Step[5338] Loss=0.1582 GradNorm=2.8598 StepSize=0.0286 RelImp=93.11%\n",
            "Epoch[1/2] Step[5339] Loss=0.2183 GradNorm=2.8908 StepSize=0.0289 RelImp=90.49%\n",
            "Epoch[1/2] Step[5340] Loss=0.9146 GradNorm=5.5294 StepSize=0.0553 RelImp=60.18%\n",
            "Epoch[1/2] Step[5341] Loss=0.1419 GradNorm=2.2886 StepSize=0.0229 RelImp=93.82%\n",
            "Epoch[1/2] Step[5342] Loss=0.3261 GradNorm=4.5644 StepSize=0.0456 RelImp=85.80%\n",
            "Epoch[1/2] Step[5343] Loss=0.1217 GradNorm=1.5903 StepSize=0.0159 RelImp=94.70%\n",
            "Epoch[1/2] Step[5344] Loss=0.0825 GradNorm=1.0969 StepSize=0.0110 RelImp=96.41%\n",
            "Epoch[1/2] Step[5345] Loss=0.5957 GradNorm=4.6946 StepSize=0.0469 RelImp=74.06%\n",
            "Epoch[1/2] Step[5346] Loss=0.7243 GradNorm=4.5043 StepSize=0.0450 RelImp=68.46%\n",
            "Epoch[1/2] Step[5347] Loss=0.2261 GradNorm=2.0634 StepSize=0.0206 RelImp=90.15%\n",
            "Epoch[1/2] Step[5348] Loss=0.3228 GradNorm=3.9384 StepSize=0.0394 RelImp=85.94%\n",
            "Epoch[1/2] Step[5349] Loss=0.1904 GradNorm=3.0523 StepSize=0.0305 RelImp=91.71%\n",
            "Epoch[1/2] Step[5350] Loss=0.1454 GradNorm=1.8971 StepSize=0.0190 RelImp=93.67%\n",
            "Epoch[1/2] Step[5351] Loss=0.1355 GradNorm=1.4155 StepSize=0.0142 RelImp=94.10%\n",
            "Epoch[1/2] Step[5352] Loss=0.7249 GradNorm=3.9320 StepSize=0.0393 RelImp=68.43%\n",
            "Epoch[1/2] Step[5353] Loss=0.4845 GradNorm=3.2991 StepSize=0.0330 RelImp=78.90%\n",
            "Epoch[1/2] Step[5354] Loss=0.4225 GradNorm=3.9234 StepSize=0.0392 RelImp=81.60%\n",
            "Epoch[1/2] Step[5355] Loss=0.4995 GradNorm=4.4410 StepSize=0.0444 RelImp=78.25%\n",
            "Epoch[1/2] Step[5356] Loss=0.0773 GradNorm=1.1918 StepSize=0.0119 RelImp=96.64%\n",
            "Epoch[1/2] Step[5357] Loss=0.1963 GradNorm=2.5191 StepSize=0.0252 RelImp=91.45%\n",
            "Epoch[1/2] Step[5358] Loss=0.2591 GradNorm=2.5106 StepSize=0.0251 RelImp=88.72%\n",
            "Epoch[1/2] Step[5359] Loss=0.4630 GradNorm=5.0262 StepSize=0.0503 RelImp=79.84%\n",
            "Epoch[1/2] Step[5360] Loss=0.3354 GradNorm=3.5782 StepSize=0.0358 RelImp=85.40%\n",
            "Epoch[1/2] Step[5361] Loss=0.3100 GradNorm=3.7897 StepSize=0.0379 RelImp=86.50%\n",
            "Epoch[1/2] Step[5362] Loss=0.2692 GradNorm=2.3095 StepSize=0.0231 RelImp=88.28%\n",
            "Epoch[1/2] Step[5363] Loss=0.3947 GradNorm=4.0846 StepSize=0.0408 RelImp=82.82%\n",
            "Epoch[1/2] Step[5364] Loss=0.3827 GradNorm=3.2629 StepSize=0.0326 RelImp=83.34%\n",
            "Epoch[1/2] Step[5365] Loss=0.0786 GradNorm=1.1689 StepSize=0.0117 RelImp=96.58%\n",
            "Epoch[1/2] Step[5366] Loss=0.3616 GradNorm=3.0787 StepSize=0.0308 RelImp=84.25%\n",
            "Epoch[1/2] Step[5367] Loss=0.1972 GradNorm=1.6926 StepSize=0.0169 RelImp=91.41%\n",
            "Epoch[1/2] Step[5368] Loss=0.2687 GradNorm=3.0027 StepSize=0.0300 RelImp=88.30%\n",
            "Epoch[1/2] Step[5369] Loss=0.7131 GradNorm=5.2225 StepSize=0.0522 RelImp=68.95%\n",
            "Epoch[1/2] Step[5370] Loss=0.1631 GradNorm=2.7992 StepSize=0.0280 RelImp=92.90%\n",
            "Epoch[1/2] Step[5371] Loss=0.2087 GradNorm=2.6802 StepSize=0.0268 RelImp=90.91%\n",
            "Epoch[1/2] Step[5372] Loss=0.4394 GradNorm=4.6574 StepSize=0.0466 RelImp=80.87%\n",
            "Epoch[1/2] Step[5373] Loss=0.3489 GradNorm=2.9142 StepSize=0.0291 RelImp=84.81%\n",
            "Epoch[1/2] Step[5374] Loss=0.0698 GradNorm=1.1787 StepSize=0.0118 RelImp=96.96%\n",
            "Epoch[1/2] Step[5375] Loss=0.6817 GradNorm=4.4166 StepSize=0.0442 RelImp=70.32%\n",
            "Epoch[1/2] Step[5376] Loss=0.3751 GradNorm=2.8395 StepSize=0.0284 RelImp=83.67%\n",
            "Epoch[1/2] Step[5377] Loss=0.7127 GradNorm=4.9617 StepSize=0.0496 RelImp=68.97%\n",
            "Epoch[1/2] Step[5378] Loss=0.9599 GradNorm=4.4549 StepSize=0.0445 RelImp=58.20%\n",
            "Epoch[1/2] Step[5379] Loss=0.2849 GradNorm=2.3067 StepSize=0.0231 RelImp=87.60%\n",
            "Epoch[1/2] Step[5380] Loss=0.4465 GradNorm=2.3822 StepSize=0.0238 RelImp=80.56%\n",
            "Epoch[1/2] Step[5381] Loss=0.1842 GradNorm=2.7438 StepSize=0.0274 RelImp=91.98%\n",
            "Epoch[1/2] Step[5382] Loss=0.3702 GradNorm=3.0603 StepSize=0.0306 RelImp=83.88%\n",
            "Epoch[1/2] Step[5383] Loss=0.2195 GradNorm=1.8357 StepSize=0.0184 RelImp=90.44%\n",
            "Epoch[1/2] Step[5384] Loss=0.8665 GradNorm=6.6054 StepSize=0.0661 RelImp=62.27%\n",
            "Epoch[1/2] Step[5385] Loss=0.3964 GradNorm=2.7012 StepSize=0.0270 RelImp=82.74%\n",
            "Epoch[1/2] Step[5386] Loss=0.1433 GradNorm=2.5700 StepSize=0.0257 RelImp=93.76%\n",
            "Epoch[1/2] Step[5387] Loss=0.5770 GradNorm=4.1797 StepSize=0.0418 RelImp=74.88%\n",
            "Epoch[1/2] Step[5388] Loss=0.3297 GradNorm=2.8931 StepSize=0.0289 RelImp=85.64%\n",
            "Epoch[1/2] Step[5389] Loss=0.0939 GradNorm=0.9353 StepSize=0.0094 RelImp=95.91%\n",
            "Epoch[1/2] Step[5390] Loss=0.0994 GradNorm=1.4564 StepSize=0.0146 RelImp=95.67%\n",
            "Epoch[1/2] Step[5391] Loss=0.3281 GradNorm=2.9424 StepSize=0.0294 RelImp=85.72%\n",
            "Epoch[1/2] Step[5392] Loss=0.5832 GradNorm=5.3709 StepSize=0.0537 RelImp=74.60%\n",
            "Epoch[1/2] Step[5393] Loss=0.1211 GradNorm=2.4535 StepSize=0.0245 RelImp=94.73%\n",
            "Epoch[1/2] Step[5394] Loss=0.5807 GradNorm=3.9001 StepSize=0.0390 RelImp=74.71%\n",
            "Epoch[1/2] Step[5395] Loss=0.7270 GradNorm=4.6277 StepSize=0.0463 RelImp=68.34%\n",
            "Epoch[1/2] Step[5396] Loss=0.2805 GradNorm=3.0074 StepSize=0.0301 RelImp=87.79%\n",
            "Epoch[1/2] Step[5397] Loss=0.3658 GradNorm=4.6107 StepSize=0.0461 RelImp=84.07%\n",
            "Epoch[1/2] Step[5398] Loss=0.4755 GradNorm=2.4280 StepSize=0.0243 RelImp=79.30%\n",
            "Epoch[1/2] Step[5399] Loss=1.0444 GradNorm=3.7739 StepSize=0.0377 RelImp=54.52%\n",
            "Epoch[1/2] Step[5400] Loss=0.1593 GradNorm=1.8461 StepSize=0.0185 RelImp=93.06%\n",
            "Epoch[1/2] Step[5401] Loss=0.2329 GradNorm=3.9774 StepSize=0.0398 RelImp=89.86%\n",
            "Epoch[1/2] Step[5402] Loss=0.8470 GradNorm=4.2022 StepSize=0.0420 RelImp=63.12%\n",
            "Epoch[1/2] Step[5403] Loss=0.3921 GradNorm=4.5974 StepSize=0.0460 RelImp=82.93%\n",
            "Epoch[1/2] Step[5404] Loss=0.4311 GradNorm=1.9789 StepSize=0.0198 RelImp=81.23%\n",
            "Epoch[1/2] Step[5405] Loss=0.3255 GradNorm=3.0048 StepSize=0.0300 RelImp=85.83%\n",
            "Epoch[1/2] Step[5406] Loss=0.6510 GradNorm=7.4201 StepSize=0.0742 RelImp=71.65%\n",
            "Epoch[1/2] Step[5407] Loss=0.2715 GradNorm=2.4607 StepSize=0.0246 RelImp=88.18%\n",
            "Epoch[1/2] Step[5408] Loss=0.3019 GradNorm=3.2504 StepSize=0.0325 RelImp=86.85%\n",
            "Epoch[1/2] Step[5409] Loss=0.5230 GradNorm=3.7744 StepSize=0.0377 RelImp=77.23%\n",
            "Epoch[1/2] Step[5410] Loss=0.3484 GradNorm=2.4759 StepSize=0.0248 RelImp=84.83%\n",
            "Epoch[1/2] Step[5411] Loss=0.3352 GradNorm=2.8845 StepSize=0.0288 RelImp=85.41%\n",
            "Epoch[1/2] Step[5412] Loss=0.2728 GradNorm=2.6536 StepSize=0.0265 RelImp=88.12%\n",
            "Epoch[1/2] Step[5413] Loss=0.7321 GradNorm=3.2111 StepSize=0.0321 RelImp=68.12%\n",
            "Epoch[1/2] Step[5414] Loss=0.7176 GradNorm=5.1372 StepSize=0.0514 RelImp=68.75%\n",
            "Epoch[1/2] Step[5415] Loss=0.1956 GradNorm=1.4520 StepSize=0.0145 RelImp=91.48%\n",
            "Epoch[1/2] Step[5416] Loss=0.2371 GradNorm=3.1765 StepSize=0.0318 RelImp=89.68%\n",
            "Epoch[1/2] Step[5417] Loss=0.3257 GradNorm=2.4276 StepSize=0.0243 RelImp=85.82%\n",
            "Epoch[1/2] Step[5418] Loss=0.0795 GradNorm=1.4784 StepSize=0.0148 RelImp=96.54%\n",
            "Epoch[1/2] Step[5419] Loss=0.1505 GradNorm=2.0291 StepSize=0.0203 RelImp=93.45%\n",
            "Epoch[1/2] Step[5420] Loss=0.1236 GradNorm=1.1083 StepSize=0.0111 RelImp=94.62%\n",
            "Epoch[1/2] Step[5421] Loss=0.1080 GradNorm=1.3854 StepSize=0.0139 RelImp=95.30%\n",
            "Epoch[1/2] Step[5422] Loss=0.2640 GradNorm=2.2282 StepSize=0.0223 RelImp=88.50%\n",
            "Epoch[1/2] Step[5423] Loss=0.2549 GradNorm=2.3112 StepSize=0.0231 RelImp=88.90%\n",
            "Epoch[1/2] Step[5424] Loss=0.5068 GradNorm=3.6306 StepSize=0.0363 RelImp=77.93%\n",
            "Epoch[1/2] Step[5425] Loss=0.8035 GradNorm=5.6729 StepSize=0.0567 RelImp=65.01%\n",
            "Epoch[1/2] Step[5426] Loss=0.5551 GradNorm=4.6238 StepSize=0.0462 RelImp=75.83%\n",
            "Epoch[1/2] Step[5427] Loss=0.3796 GradNorm=3.0056 StepSize=0.0301 RelImp=83.47%\n",
            "Epoch[1/2] Step[5428] Loss=0.1701 GradNorm=1.5427 StepSize=0.0154 RelImp=92.59%\n",
            "Epoch[1/2] Step[5429] Loss=0.2421 GradNorm=2.8418 StepSize=0.0284 RelImp=89.46%\n",
            "Epoch[1/2] Step[5430] Loss=0.3882 GradNorm=4.0824 StepSize=0.0408 RelImp=83.10%\n",
            "Epoch[1/2] Step[5431] Loss=0.1753 GradNorm=1.9301 StepSize=0.0193 RelImp=92.37%\n",
            "Epoch[1/2] Step[5432] Loss=0.3647 GradNorm=3.1246 StepSize=0.0312 RelImp=84.12%\n",
            "Epoch[1/2] Step[5433] Loss=0.3351 GradNorm=2.9198 StepSize=0.0292 RelImp=85.41%\n",
            "Epoch[1/2] Step[5434] Loss=0.0395 GradNorm=0.4486 StepSize=0.0045 RelImp=98.28%\n",
            "Epoch[1/2] Step[5435] Loss=0.6400 GradNorm=4.5957 StepSize=0.0460 RelImp=72.13%\n",
            "Epoch[1/2] Step[5436] Loss=0.3124 GradNorm=3.3126 StepSize=0.0331 RelImp=86.40%\n",
            "Epoch[1/2] Step[5437] Loss=0.2426 GradNorm=2.2137 StepSize=0.0221 RelImp=89.44%\n",
            "Epoch[1/2] Step[5438] Loss=0.2729 GradNorm=2.2600 StepSize=0.0226 RelImp=88.12%\n",
            "Epoch[1/2] Step[5439] Loss=0.2545 GradNorm=2.9940 StepSize=0.0299 RelImp=88.92%\n",
            "Epoch[1/2] Step[5440] Loss=0.1818 GradNorm=2.9966 StepSize=0.0300 RelImp=92.08%\n",
            "Epoch[1/2] Step[5441] Loss=0.5093 GradNorm=3.8871 StepSize=0.0389 RelImp=77.82%\n",
            "Epoch[1/2] Step[5442] Loss=0.4944 GradNorm=5.6184 StepSize=0.0562 RelImp=78.47%\n",
            "Epoch[1/2] Step[5443] Loss=0.3738 GradNorm=3.0210 StepSize=0.0302 RelImp=83.72%\n",
            "Epoch[1/2] Step[5444] Loss=0.4341 GradNorm=3.2821 StepSize=0.0328 RelImp=81.10%\n",
            "Epoch[1/2] Step[5445] Loss=0.1847 GradNorm=3.1203 StepSize=0.0312 RelImp=91.96%\n",
            "Epoch[1/2] Step[5446] Loss=0.2740 GradNorm=2.4350 StepSize=0.0244 RelImp=88.07%\n",
            "Epoch[1/2] Step[5447] Loss=0.4019 GradNorm=3.3909 StepSize=0.0339 RelImp=82.50%\n",
            "Epoch[1/2] Step[5448] Loss=0.2967 GradNorm=3.5573 StepSize=0.0356 RelImp=87.08%\n",
            "Epoch[1/2] Step[5449] Loss=0.1436 GradNorm=2.0722 StepSize=0.0207 RelImp=93.75%\n",
            "Epoch[1/2] Step[5450] Loss=0.1591 GradNorm=1.9586 StepSize=0.0196 RelImp=93.07%\n",
            "Epoch[1/2] Step[5451] Loss=0.2828 GradNorm=2.7986 StepSize=0.0280 RelImp=87.69%\n",
            "Epoch[1/2] Step[5452] Loss=0.2004 GradNorm=2.0783 StepSize=0.0208 RelImp=91.27%\n",
            "Epoch[1/2] Step[5453] Loss=0.2197 GradNorm=2.6097 StepSize=0.0261 RelImp=90.44%\n",
            "Epoch[1/2] Step[5454] Loss=0.0925 GradNorm=0.8763 StepSize=0.0088 RelImp=95.97%\n",
            "Epoch[1/2] Step[5455] Loss=0.3139 GradNorm=2.7398 StepSize=0.0274 RelImp=86.33%\n",
            "Epoch[1/2] Step[5456] Loss=0.2252 GradNorm=1.6780 StepSize=0.0168 RelImp=90.19%\n",
            "Epoch[1/2] Step[5457] Loss=0.3960 GradNorm=4.6266 StepSize=0.0463 RelImp=82.76%\n",
            "Epoch[1/2] Step[5458] Loss=0.2366 GradNorm=2.6538 StepSize=0.0265 RelImp=89.70%\n",
            "Epoch[1/2] Step[5459] Loss=0.7104 GradNorm=5.5469 StepSize=0.0555 RelImp=69.07%\n",
            "Epoch[1/2] Step[5460] Loss=0.8150 GradNorm=3.0396 StepSize=0.0304 RelImp=64.51%\n",
            "Epoch[1/2] Step[5461] Loss=0.4273 GradNorm=2.6146 StepSize=0.0261 RelImp=81.39%\n",
            "Epoch[1/2] Step[5462] Loss=1.0218 GradNorm=6.2238 StepSize=0.0622 RelImp=55.51%\n",
            "Epoch[1/2] Step[5463] Loss=0.4570 GradNorm=4.3183 StepSize=0.0432 RelImp=80.10%\n",
            "Epoch[1/2] Step[5464] Loss=1.2225 GradNorm=3.6822 StepSize=0.0368 RelImp=46.77%\n",
            "Epoch[1/2] Step[5465] Loss=0.9941 GradNorm=5.0507 StepSize=0.0505 RelImp=56.72%\n",
            "Epoch[1/2] Step[5466] Loss=0.3438 GradNorm=5.6487 StepSize=0.0565 RelImp=85.03%\n",
            "Epoch[1/2] Step[5467] Loss=0.4823 GradNorm=3.5845 StepSize=0.0358 RelImp=79.00%\n",
            "Epoch[1/2] Step[5468] Loss=0.4615 GradNorm=2.8198 StepSize=0.0282 RelImp=79.91%\n",
            "Epoch[1/2] Step[5469] Loss=0.4682 GradNorm=3.0899 StepSize=0.0309 RelImp=79.61%\n",
            "Epoch[1/2] Step[5470] Loss=0.0935 GradNorm=1.6846 StepSize=0.0168 RelImp=95.93%\n",
            "Epoch[1/2] Step[5471] Loss=0.2631 GradNorm=3.0576 StepSize=0.0306 RelImp=88.54%\n",
            "Epoch[1/2] Step[5472] Loss=0.2868 GradNorm=2.4651 StepSize=0.0247 RelImp=87.51%\n",
            "Epoch[1/2] Step[5473] Loss=0.2378 GradNorm=3.5144 StepSize=0.0351 RelImp=89.65%\n",
            "Epoch[1/2] Step[5474] Loss=0.3297 GradNorm=2.0298 StepSize=0.0203 RelImp=85.65%\n",
            "Epoch[1/2] Step[5475] Loss=0.2788 GradNorm=2.6858 StepSize=0.0269 RelImp=87.86%\n",
            "Epoch[1/2] Step[5476] Loss=0.2886 GradNorm=2.1330 StepSize=0.0213 RelImp=87.43%\n",
            "Epoch[1/2] Step[5477] Loss=0.7671 GradNorm=3.8981 StepSize=0.0390 RelImp=66.60%\n",
            "Epoch[1/2] Step[5478] Loss=0.3614 GradNorm=4.3329 StepSize=0.0433 RelImp=84.26%\n",
            "Epoch[1/2] Step[5479] Loss=0.4041 GradNorm=3.5174 StepSize=0.0352 RelImp=82.41%\n",
            "Epoch[1/2] Step[5480] Loss=0.1611 GradNorm=1.7181 StepSize=0.0172 RelImp=92.99%\n",
            "Epoch[1/2] Step[5481] Loss=0.1883 GradNorm=2.3962 StepSize=0.0240 RelImp=91.80%\n",
            "Epoch[1/2] Step[5482] Loss=0.1541 GradNorm=2.3502 StepSize=0.0235 RelImp=93.29%\n",
            "Epoch[1/2] Step[5483] Loss=0.4713 GradNorm=3.0911 StepSize=0.0309 RelImp=79.48%\n",
            "Epoch[1/2] Step[5484] Loss=0.1249 GradNorm=1.1150 StepSize=0.0111 RelImp=94.56%\n",
            "Epoch[1/2] Step[5485] Loss=0.1304 GradNorm=2.0492 StepSize=0.0205 RelImp=94.32%\n",
            "Epoch[1/2] Step[5486] Loss=0.7015 GradNorm=4.5924 StepSize=0.0459 RelImp=69.45%\n",
            "Epoch[1/2] Step[5487] Loss=0.2651 GradNorm=2.3521 StepSize=0.0235 RelImp=88.46%\n",
            "Epoch[1/2] Step[5488] Loss=0.6449 GradNorm=3.5694 StepSize=0.0357 RelImp=71.92%\n",
            "Epoch[1/2] Step[5489] Loss=0.5411 GradNorm=3.9583 StepSize=0.0396 RelImp=76.44%\n",
            "Epoch[1/2] Step[5490] Loss=0.3845 GradNorm=3.6884 StepSize=0.0369 RelImp=83.26%\n",
            "Epoch[1/2] Step[5491] Loss=0.2294 GradNorm=2.5479 StepSize=0.0255 RelImp=90.01%\n",
            "Epoch[1/2] Step[5492] Loss=0.1648 GradNorm=2.0369 StepSize=0.0204 RelImp=92.82%\n",
            "Epoch[1/2] Step[5493] Loss=0.1627 GradNorm=1.6219 StepSize=0.0162 RelImp=92.91%\n",
            "Epoch[1/2] Step[5494] Loss=0.2064 GradNorm=2.6638 StepSize=0.0266 RelImp=91.01%\n",
            "Epoch[1/2] Step[5495] Loss=0.2385 GradNorm=3.3066 StepSize=0.0331 RelImp=89.62%\n",
            "Epoch[1/2] Step[5496] Loss=0.4707 GradNorm=3.8332 StepSize=0.0383 RelImp=79.50%\n",
            "Epoch[1/2] Step[5497] Loss=0.1818 GradNorm=2.7633 StepSize=0.0276 RelImp=92.09%\n",
            "Epoch[1/2] Step[5498] Loss=0.2293 GradNorm=2.9456 StepSize=0.0295 RelImp=90.02%\n",
            "Epoch[1/2] Step[5499] Loss=1.0252 GradNorm=5.4910 StepSize=0.0549 RelImp=55.36%\n",
            "Epoch[1/2] Step[5500] Loss=0.3382 GradNorm=3.4274 StepSize=0.0343 RelImp=85.27%\n",
            "Epoch[1/2] Step[5501] Loss=0.6388 GradNorm=3.4289 StepSize=0.0343 RelImp=72.18%\n",
            "Epoch[1/2] Step[5502] Loss=0.1081 GradNorm=1.7464 StepSize=0.0175 RelImp=95.29%\n",
            "Epoch[1/2] Step[5503] Loss=0.1293 GradNorm=1.2226 StepSize=0.0122 RelImp=94.37%\n",
            "Epoch[1/2] Step[5504] Loss=0.0653 GradNorm=0.9154 StepSize=0.0092 RelImp=97.16%\n",
            "Epoch[1/2] Step[5505] Loss=0.0473 GradNorm=0.6162 StepSize=0.0062 RelImp=97.94%\n",
            "Epoch[1/2] Step[5506] Loss=0.2813 GradNorm=2.7434 StepSize=0.0274 RelImp=87.75%\n",
            "Epoch[1/2] Step[5507] Loss=0.5041 GradNorm=3.4703 StepSize=0.0347 RelImp=78.05%\n",
            "Epoch[1/2] Step[5508] Loss=0.1197 GradNorm=1.3377 StepSize=0.0134 RelImp=94.79%\n",
            "Epoch[1/2] Step[5509] Loss=0.1621 GradNorm=2.2597 StepSize=0.0226 RelImp=92.94%\n",
            "Epoch[1/2] Step[5510] Loss=0.2911 GradNorm=3.5918 StepSize=0.0359 RelImp=87.33%\n",
            "Epoch[1/2] Step[5511] Loss=0.3220 GradNorm=2.6958 StepSize=0.0270 RelImp=85.98%\n",
            "Epoch[1/2] Step[5512] Loss=0.2048 GradNorm=2.2999 StepSize=0.0230 RelImp=91.08%\n",
            "Epoch[1/2] Step[5513] Loss=0.5127 GradNorm=2.9316 StepSize=0.0293 RelImp=77.68%\n",
            "Epoch[1/2] Step[5514] Loss=0.1781 GradNorm=1.5292 StepSize=0.0153 RelImp=92.24%\n",
            "Epoch[1/2] Step[5515] Loss=0.3634 GradNorm=4.5351 StepSize=0.0454 RelImp=84.18%\n",
            "Epoch[1/2] Step[5516] Loss=0.3773 GradNorm=4.1618 StepSize=0.0416 RelImp=83.57%\n",
            "Epoch[1/2] Step[5517] Loss=0.2802 GradNorm=2.7501 StepSize=0.0275 RelImp=87.80%\n",
            "Epoch[1/2] Step[5518] Loss=0.3978 GradNorm=4.2908 StepSize=0.0429 RelImp=82.68%\n",
            "Epoch[1/2] Step[5519] Loss=0.5291 GradNorm=5.4939 StepSize=0.0549 RelImp=76.96%\n",
            "Epoch[1/2] Step[5520] Loss=0.4685 GradNorm=4.3622 StepSize=0.0436 RelImp=79.60%\n",
            "Epoch[1/2] Step[5521] Loss=0.2972 GradNorm=3.1282 StepSize=0.0313 RelImp=87.06%\n",
            "Epoch[1/2] Step[5522] Loss=0.1622 GradNorm=1.8873 StepSize=0.0189 RelImp=92.94%\n",
            "Epoch[1/2] Step[5523] Loss=0.1729 GradNorm=1.2973 StepSize=0.0130 RelImp=92.47%\n",
            "Epoch[1/2] Step[5524] Loss=0.4405 GradNorm=3.4667 StepSize=0.0347 RelImp=80.82%\n",
            "Epoch[1/2] Step[5525] Loss=0.4375 GradNorm=3.5905 StepSize=0.0359 RelImp=80.95%\n",
            "Epoch[1/2] Step[5526] Loss=0.3271 GradNorm=3.3304 StepSize=0.0333 RelImp=85.76%\n",
            "Epoch[1/2] Step[5527] Loss=0.7960 GradNorm=2.2879 StepSize=0.0229 RelImp=65.34%\n",
            "Epoch[1/2] Step[5528] Loss=0.3593 GradNorm=3.3207 StepSize=0.0332 RelImp=84.36%\n",
            "Epoch[1/2] Step[5529] Loss=0.1700 GradNorm=2.3578 StepSize=0.0236 RelImp=92.60%\n",
            "Epoch[1/2] Step[5530] Loss=0.2627 GradNorm=2.9740 StepSize=0.0297 RelImp=88.56%\n",
            "Epoch[1/2] Step[5531] Loss=0.7217 GradNorm=4.5292 StepSize=0.0453 RelImp=68.58%\n",
            "Epoch[1/2] Step[5532] Loss=0.2518 GradNorm=2.6169 StepSize=0.0262 RelImp=89.04%\n",
            "Epoch[1/2] Step[5533] Loss=0.2090 GradNorm=2.1995 StepSize=0.0220 RelImp=90.90%\n",
            "Epoch[1/2] Step[5534] Loss=0.3437 GradNorm=3.5540 StepSize=0.0355 RelImp=85.03%\n",
            "Epoch[1/2] Step[5535] Loss=0.4691 GradNorm=3.2833 StepSize=0.0328 RelImp=79.58%\n",
            "Epoch[1/2] Step[5536] Loss=0.4194 GradNorm=3.4679 StepSize=0.0347 RelImp=81.74%\n",
            "Epoch[1/2] Step[5537] Loss=0.1440 GradNorm=1.7305 StepSize=0.0173 RelImp=93.73%\n",
            "Epoch[1/2] Step[5538] Loss=0.1337 GradNorm=1.9654 StepSize=0.0197 RelImp=94.18%\n",
            "Epoch[1/2] Step[5539] Loss=0.8340 GradNorm=3.4934 StepSize=0.0349 RelImp=63.68%\n",
            "Epoch[1/2] Step[5540] Loss=0.1352 GradNorm=2.2983 StepSize=0.0230 RelImp=94.11%\n",
            "Epoch[1/2] Step[5541] Loss=0.3402 GradNorm=3.4354 StepSize=0.0344 RelImp=85.19%\n",
            "Epoch[1/2] Step[5542] Loss=0.4491 GradNorm=3.2009 StepSize=0.0320 RelImp=80.44%\n",
            "Epoch[1/2] Step[5543] Loss=0.2267 GradNorm=3.1891 StepSize=0.0319 RelImp=90.13%\n",
            "Epoch[1/2] Step[5544] Loss=0.1623 GradNorm=2.4201 StepSize=0.0242 RelImp=92.93%\n",
            "Epoch[1/2] Step[5545] Loss=0.1049 GradNorm=1.4315 StepSize=0.0143 RelImp=95.43%\n",
            "Epoch[1/2] Step[5546] Loss=1.0933 GradNorm=3.8606 StepSize=0.0386 RelImp=52.40%\n",
            "Epoch[1/2] Step[5547] Loss=0.2552 GradNorm=2.7285 StepSize=0.0273 RelImp=88.89%\n",
            "Epoch[1/2] Step[5548] Loss=0.3579 GradNorm=4.5910 StepSize=0.0459 RelImp=84.42%\n",
            "Epoch[1/2] Step[5549] Loss=0.2398 GradNorm=2.6200 StepSize=0.0262 RelImp=89.56%\n",
            "Epoch[1/2] Step[5550] Loss=0.1746 GradNorm=2.6478 StepSize=0.0265 RelImp=92.40%\n",
            "Epoch[1/2] Step[5551] Loss=0.0735 GradNorm=0.6291 StepSize=0.0063 RelImp=96.80%\n",
            "Epoch[1/2] Step[5552] Loss=0.9677 GradNorm=6.2584 StepSize=0.0626 RelImp=57.86%\n",
            "Epoch[1/2] Step[5553] Loss=0.4410 GradNorm=4.0059 StepSize=0.0401 RelImp=80.80%\n",
            "Epoch[1/2] Step[5554] Loss=0.7954 GradNorm=4.7396 StepSize=0.0474 RelImp=65.37%\n",
            "Epoch[1/2] Step[5555] Loss=0.1185 GradNorm=1.6019 StepSize=0.0160 RelImp=94.84%\n",
            "Epoch[1/2] Step[5556] Loss=0.3453 GradNorm=4.2853 StepSize=0.0429 RelImp=84.97%\n",
            "Epoch[1/2] Step[5557] Loss=0.6663 GradNorm=4.2116 StepSize=0.0421 RelImp=70.99%\n",
            "Epoch[1/2] Step[5558] Loss=0.2798 GradNorm=3.1349 StepSize=0.0313 RelImp=87.82%\n",
            "Epoch[1/2] Step[5559] Loss=0.3137 GradNorm=4.1252 StepSize=0.0413 RelImp=86.34%\n",
            "Epoch[1/2] Step[5560] Loss=0.4911 GradNorm=4.3514 StepSize=0.0435 RelImp=78.62%\n",
            "Epoch[1/2] Step[5561] Loss=0.7310 GradNorm=3.6147 StepSize=0.0361 RelImp=68.17%\n",
            "Epoch[1/2] Step[5562] Loss=0.2411 GradNorm=3.0717 StepSize=0.0307 RelImp=89.50%\n",
            "Epoch[1/2] Step[5563] Loss=0.5269 GradNorm=3.9610 StepSize=0.0396 RelImp=77.06%\n",
            "Epoch[1/2] Step[5564] Loss=0.1368 GradNorm=2.3093 StepSize=0.0231 RelImp=94.04%\n",
            "Epoch[1/2] Step[5565] Loss=0.1672 GradNorm=2.3587 StepSize=0.0236 RelImp=92.72%\n",
            "Epoch[1/2] Step[5566] Loss=0.0756 GradNorm=1.2816 StepSize=0.0128 RelImp=96.71%\n",
            "Epoch[1/2] Step[5567] Loss=0.1631 GradNorm=1.8097 StepSize=0.0181 RelImp=92.90%\n",
            "Epoch[1/2] Step[5568] Loss=0.1919 GradNorm=3.0118 StepSize=0.0301 RelImp=91.64%\n",
            "Epoch[1/2] Step[5569] Loss=0.3831 GradNorm=3.0581 StepSize=0.0306 RelImp=83.32%\n",
            "Epoch[1/2] Step[5570] Loss=0.0525 GradNorm=0.6025 StepSize=0.0060 RelImp=97.71%\n",
            "Epoch[1/2] Step[5571] Loss=0.3441 GradNorm=3.1429 StepSize=0.0314 RelImp=85.02%\n",
            "Epoch[1/2] Step[5572] Loss=0.3685 GradNorm=4.5040 StepSize=0.0450 RelImp=83.96%\n",
            "Epoch[1/2] Step[5573] Loss=0.5734 GradNorm=5.0100 StepSize=0.0501 RelImp=75.03%\n",
            "Epoch[1/2] Step[5574] Loss=0.1235 GradNorm=1.7161 StepSize=0.0172 RelImp=94.62%\n",
            "Epoch[1/2] Step[5575] Loss=0.3521 GradNorm=2.8296 StepSize=0.0283 RelImp=84.67%\n",
            "Epoch[1/2] Step[5576] Loss=0.3476 GradNorm=2.5519 StepSize=0.0255 RelImp=84.87%\n",
            "Epoch[1/2] Step[5577] Loss=0.9680 GradNorm=3.7972 StepSize=0.0380 RelImp=57.85%\n",
            "Epoch[1/2] Step[5578] Loss=0.3633 GradNorm=3.9908 StepSize=0.0399 RelImp=84.18%\n",
            "Epoch[1/2] Step[5579] Loss=0.7424 GradNorm=4.7523 StepSize=0.0475 RelImp=67.67%\n",
            "Epoch[1/2] Step[5580] Loss=0.3102 GradNorm=4.0504 StepSize=0.0405 RelImp=86.49%\n",
            "Epoch[1/2] Step[5581] Loss=0.1251 GradNorm=1.7308 StepSize=0.0173 RelImp=94.55%\n",
            "Epoch[1/2] Step[5582] Loss=0.6773 GradNorm=3.8040 StepSize=0.0380 RelImp=70.51%\n",
            "Epoch[1/2] Step[5583] Loss=0.5540 GradNorm=4.4334 StepSize=0.0443 RelImp=75.88%\n",
            "Epoch[1/2] Step[5584] Loss=0.5221 GradNorm=3.9676 StepSize=0.0397 RelImp=77.27%\n",
            "Epoch[1/2] Step[5585] Loss=0.2218 GradNorm=2.0312 StepSize=0.0203 RelImp=90.34%\n",
            "Epoch[1/2] Step[5586] Loss=0.2374 GradNorm=2.6681 StepSize=0.0267 RelImp=89.66%\n",
            "Epoch[1/2] Step[5587] Loss=0.3357 GradNorm=3.7812 StepSize=0.0378 RelImp=85.38%\n",
            "Epoch[1/2] Step[5588] Loss=0.6819 GradNorm=3.3939 StepSize=0.0339 RelImp=70.31%\n",
            "Epoch[1/2] Step[5589] Loss=0.2671 GradNorm=2.2537 StepSize=0.0225 RelImp=88.37%\n",
            "Epoch[1/2] Step[5590] Loss=0.3823 GradNorm=5.3893 StepSize=0.0539 RelImp=83.35%\n",
            "Epoch[1/2] Step[5591] Loss=0.4736 GradNorm=3.7199 StepSize=0.0372 RelImp=79.38%\n",
            "Epoch[1/2] Step[5592] Loss=0.3145 GradNorm=3.6086 StepSize=0.0361 RelImp=86.31%\n",
            "Epoch[1/2] Step[5593] Loss=0.2462 GradNorm=3.1222 StepSize=0.0312 RelImp=89.28%\n",
            "Epoch[1/2] Step[5594] Loss=0.2192 GradNorm=2.8826 StepSize=0.0288 RelImp=90.45%\n",
            "Epoch[1/2] Step[5595] Loss=0.7104 GradNorm=5.2894 StepSize=0.0529 RelImp=69.07%\n",
            "Epoch[1/2] Step[5596] Loss=0.0929 GradNorm=1.8010 StepSize=0.0180 RelImp=95.96%\n",
            "Epoch[1/2] Step[5597] Loss=0.2056 GradNorm=1.3308 StepSize=0.0133 RelImp=91.05%\n",
            "Epoch[1/2] Step[5598] Loss=0.4345 GradNorm=5.0090 StepSize=0.0501 RelImp=81.08%\n",
            "Epoch[1/2] Step[5599] Loss=0.2161 GradNorm=2.8294 StepSize=0.0283 RelImp=90.59%\n",
            "Epoch[1/2] Step[5600] Loss=0.5501 GradNorm=4.1781 StepSize=0.0418 RelImp=76.05%\n",
            "Epoch[1/2] Step[5601] Loss=0.1089 GradNorm=1.3469 StepSize=0.0135 RelImp=95.26%\n",
            "Epoch[1/2] Step[5602] Loss=0.1140 GradNorm=1.7583 StepSize=0.0176 RelImp=95.04%\n",
            "Epoch[1/2] Step[5603] Loss=0.3504 GradNorm=3.6171 StepSize=0.0362 RelImp=84.74%\n",
            "Epoch[1/2] Step[5604] Loss=0.5967 GradNorm=3.1289 StepSize=0.0313 RelImp=74.02%\n",
            "Epoch[1/2] Step[5605] Loss=0.1453 GradNorm=2.1197 StepSize=0.0212 RelImp=93.67%\n",
            "Epoch[1/2] Step[5606] Loss=0.1914 GradNorm=2.4593 StepSize=0.0246 RelImp=91.67%\n",
            "Epoch[1/2] Step[5607] Loss=0.1073 GradNorm=1.5045 StepSize=0.0150 RelImp=95.33%\n",
            "Epoch[1/2] Step[5608] Loss=0.2281 GradNorm=2.7878 StepSize=0.0279 RelImp=90.07%\n",
            "Epoch[1/2] Step[5609] Loss=0.3302 GradNorm=3.2538 StepSize=0.0325 RelImp=85.62%\n",
            "Epoch[1/2] Step[5610] Loss=0.1683 GradNorm=1.8840 StepSize=0.0188 RelImp=92.67%\n",
            "Epoch[1/2] Step[5611] Loss=0.0731 GradNorm=0.9978 StepSize=0.0100 RelImp=96.82%\n",
            "Epoch[1/2] Step[5612] Loss=0.5076 GradNorm=3.3265 StepSize=0.0333 RelImp=77.90%\n",
            "Epoch[1/2] Step[5613] Loss=0.1328 GradNorm=1.3937 StepSize=0.0139 RelImp=94.22%\n",
            "Epoch[1/2] Step[5614] Loss=0.2530 GradNorm=3.1585 StepSize=0.0316 RelImp=88.98%\n",
            "Epoch[1/2] Step[5615] Loss=0.3152 GradNorm=3.9811 StepSize=0.0398 RelImp=86.28%\n",
            "Epoch[1/2] Step[5616] Loss=0.4855 GradNorm=4.0473 StepSize=0.0405 RelImp=78.86%\n",
            "Epoch[1/2] Step[5617] Loss=0.2173 GradNorm=2.2266 StepSize=0.0223 RelImp=90.54%\n",
            "Epoch[1/2] Step[5618] Loss=0.4457 GradNorm=3.3836 StepSize=0.0338 RelImp=80.59%\n",
            "Epoch[1/2] Step[5619] Loss=1.0886 GradNorm=5.2720 StepSize=0.0527 RelImp=52.60%\n",
            "Epoch[1/2] Step[5620] Loss=0.5424 GradNorm=3.2452 StepSize=0.0325 RelImp=76.38%\n",
            "Epoch[1/2] Step[5621] Loss=0.0841 GradNorm=1.1350 StepSize=0.0113 RelImp=96.34%\n",
            "Epoch[1/2] Step[5622] Loss=0.0692 GradNorm=1.2598 StepSize=0.0126 RelImp=96.99%\n",
            "Epoch[1/2] Step[5623] Loss=0.2379 GradNorm=3.3431 StepSize=0.0334 RelImp=89.64%\n",
            "Epoch[1/2] Step[5624] Loss=0.2428 GradNorm=3.2960 StepSize=0.0330 RelImp=89.43%\n",
            "Epoch[1/2] Step[5625] Loss=0.3894 GradNorm=2.8849 StepSize=0.0288 RelImp=83.05%\n",
            "Epoch[1/2] Step[5626] Loss=0.3321 GradNorm=3.3855 StepSize=0.0339 RelImp=85.54%\n",
            "Epoch[1/2] Step[5627] Loss=0.1318 GradNorm=2.1532 StepSize=0.0215 RelImp=94.26%\n",
            "Epoch[1/2] Step[5628] Loss=0.2684 GradNorm=2.5543 StepSize=0.0255 RelImp=88.31%\n",
            "Epoch[1/2] Step[5629] Loss=0.1552 GradNorm=1.9075 StepSize=0.0191 RelImp=93.24%\n",
            "Epoch[1/2] Step[5630] Loss=0.7692 GradNorm=3.0551 StepSize=0.0306 RelImp=66.51%\n",
            "Epoch[1/2] Step[5631] Loss=0.5379 GradNorm=3.8869 StepSize=0.0389 RelImp=76.58%\n",
            "Epoch[1/2] Step[5632] Loss=0.2057 GradNorm=1.9767 StepSize=0.0198 RelImp=91.04%\n",
            "Epoch[1/2] Step[5633] Loss=0.2245 GradNorm=3.4235 StepSize=0.0342 RelImp=90.23%\n",
            "Epoch[1/2] Step[5634] Loss=0.2761 GradNorm=3.9882 StepSize=0.0399 RelImp=87.98%\n",
            "Epoch[1/2] Step[5635] Loss=0.3297 GradNorm=2.7053 StepSize=0.0271 RelImp=85.64%\n",
            "Epoch[1/2] Step[5636] Loss=0.6983 GradNorm=5.7164 StepSize=0.0572 RelImp=69.59%\n",
            "Epoch[1/2] Step[5637] Loss=0.1965 GradNorm=1.9781 StepSize=0.0198 RelImp=91.44%\n",
            "Epoch[1/2] Step[5638] Loss=0.0584 GradNorm=0.9640 StepSize=0.0096 RelImp=97.46%\n",
            "Epoch[1/2] Step[5639] Loss=0.2999 GradNorm=5.0074 StepSize=0.0501 RelImp=86.94%\n",
            "Epoch[1/2] Step[5640] Loss=0.1556 GradNorm=1.9176 StepSize=0.0192 RelImp=93.22%\n",
            "Epoch[1/2] Step[5641] Loss=0.2852 GradNorm=3.4904 StepSize=0.0349 RelImp=87.58%\n",
            "Epoch[1/2] Step[5642] Loss=0.4451 GradNorm=4.1901 StepSize=0.0419 RelImp=80.62%\n",
            "Epoch[1/2] Step[5643] Loss=0.0929 GradNorm=1.8704 StepSize=0.0187 RelImp=95.95%\n",
            "Epoch[1/2] Step[5644] Loss=0.6016 GradNorm=5.6711 StepSize=0.0567 RelImp=73.80%\n",
            "Epoch[1/2] Step[5645] Loss=0.3631 GradNorm=3.2035 StepSize=0.0320 RelImp=84.19%\n",
            "Epoch[1/2] Step[5646] Loss=0.3323 GradNorm=2.4247 StepSize=0.0242 RelImp=85.53%\n",
            "Epoch[1/2] Step[5647] Loss=0.4182 GradNorm=3.4101 StepSize=0.0341 RelImp=81.79%\n",
            "Epoch[1/2] Step[5648] Loss=0.4772 GradNorm=4.2863 StepSize=0.0429 RelImp=79.22%\n",
            "Epoch[1/2] Step[5649] Loss=0.1931 GradNorm=2.0020 StepSize=0.0200 RelImp=91.59%\n",
            "Epoch[1/2] Step[5650] Loss=0.2792 GradNorm=2.6395 StepSize=0.0264 RelImp=87.84%\n",
            "Epoch[1/2] Step[5651] Loss=0.5211 GradNorm=4.9100 StepSize=0.0491 RelImp=77.31%\n",
            "Epoch[1/2] Step[5652] Loss=0.3215 GradNorm=3.9516 StepSize=0.0395 RelImp=86.00%\n",
            "Epoch[1/2] Step[5653] Loss=0.1404 GradNorm=2.1310 StepSize=0.0213 RelImp=93.89%\n",
            "Epoch[1/2] Step[5654] Loss=0.4856 GradNorm=3.4419 StepSize=0.0344 RelImp=78.86%\n",
            "Epoch[1/2] Step[5655] Loss=0.1260 GradNorm=1.5427 StepSize=0.0154 RelImp=94.51%\n",
            "Epoch[1/2] Step[5656] Loss=0.3893 GradNorm=2.4725 StepSize=0.0247 RelImp=83.05%\n",
            "Epoch[1/2] Step[5657] Loss=0.2449 GradNorm=3.2806 StepSize=0.0328 RelImp=89.34%\n",
            "Epoch[1/2] Step[5658] Loss=0.6031 GradNorm=3.3419 StepSize=0.0334 RelImp=73.74%\n",
            "Epoch[1/2] Step[5659] Loss=0.2587 GradNorm=2.7344 StepSize=0.0273 RelImp=88.74%\n",
            "Epoch[1/2] Step[5660] Loss=0.2140 GradNorm=2.5532 StepSize=0.0255 RelImp=90.68%\n",
            "Epoch[1/2] Step[5661] Loss=0.1520 GradNorm=1.3032 StepSize=0.0130 RelImp=93.38%\n",
            "Epoch[1/2] Step[5662] Loss=0.1386 GradNorm=1.6349 StepSize=0.0163 RelImp=93.97%\n",
            "Epoch[1/2] Step[5663] Loss=0.2956 GradNorm=2.9587 StepSize=0.0296 RelImp=87.13%\n",
            "Epoch[1/2] Step[5664] Loss=0.1108 GradNorm=1.5344 StepSize=0.0153 RelImp=95.17%\n",
            "Epoch[1/2] Step[5665] Loss=0.3435 GradNorm=3.9472 StepSize=0.0395 RelImp=85.04%\n",
            "Epoch[1/2] Step[5666] Loss=0.2447 GradNorm=3.0441 StepSize=0.0304 RelImp=89.35%\n",
            "Epoch[1/2] Step[5667] Loss=0.1251 GradNorm=2.1787 StepSize=0.0218 RelImp=94.55%\n",
            "Epoch[1/2] Step[5668] Loss=0.5399 GradNorm=4.4974 StepSize=0.0450 RelImp=76.49%\n",
            "Epoch[1/2] Step[5669] Loss=0.2875 GradNorm=2.3824 StepSize=0.0238 RelImp=87.48%\n",
            "Epoch[1/2] Step[5670] Loss=0.2414 GradNorm=3.0097 StepSize=0.0301 RelImp=89.49%\n",
            "Epoch[1/2] Step[5671] Loss=0.2187 GradNorm=2.4020 StepSize=0.0240 RelImp=90.48%\n",
            "Epoch[1/2] Step[5672] Loss=0.3815 GradNorm=3.5401 StepSize=0.0354 RelImp=83.39%\n",
            "Epoch[1/2] Step[5673] Loss=0.0884 GradNorm=1.3048 StepSize=0.0130 RelImp=96.15%\n",
            "Epoch[1/2] Step[5674] Loss=0.6309 GradNorm=3.9901 StepSize=0.0399 RelImp=72.53%\n",
            "Epoch[1/2] Step[5675] Loss=0.1745 GradNorm=2.2224 StepSize=0.0222 RelImp=92.40%\n",
            "Epoch[1/2] Step[5676] Loss=0.3047 GradNorm=3.6842 StepSize=0.0368 RelImp=86.73%\n",
            "Epoch[1/2] Step[5677] Loss=0.8603 GradNorm=2.8791 StepSize=0.0288 RelImp=62.54%\n",
            "Epoch[1/2] Step[5678] Loss=0.1770 GradNorm=2.7537 StepSize=0.0275 RelImp=92.29%\n",
            "Epoch[1/2] Step[5679] Loss=0.2951 GradNorm=3.3651 StepSize=0.0337 RelImp=87.15%\n",
            "Epoch[1/2] Step[5680] Loss=0.2469 GradNorm=2.9562 StepSize=0.0296 RelImp=89.25%\n",
            "Epoch[1/2] Step[5681] Loss=0.3929 GradNorm=3.3930 StepSize=0.0339 RelImp=82.89%\n",
            "Epoch[1/2] Step[5682] Loss=0.5701 GradNorm=3.3839 StepSize=0.0338 RelImp=75.18%\n",
            "Epoch[1/2] Step[5683] Loss=0.7322 GradNorm=4.2444 StepSize=0.0424 RelImp=68.12%\n",
            "Epoch[1/2] Step[5684] Loss=0.4099 GradNorm=3.6318 StepSize=0.0363 RelImp=82.15%\n",
            "Epoch[1/2] Step[5685] Loss=0.2020 GradNorm=1.9039 StepSize=0.0190 RelImp=91.20%\n",
            "Epoch[1/2] Step[5686] Loss=0.2072 GradNorm=2.5773 StepSize=0.0258 RelImp=90.98%\n",
            "Epoch[1/2] Step[5687] Loss=0.2936 GradNorm=2.7821 StepSize=0.0278 RelImp=87.21%\n",
            "Epoch[1/2] Step[5688] Loss=0.1777 GradNorm=2.5150 StepSize=0.0251 RelImp=92.26%\n",
            "Epoch[1/2] Step[5689] Loss=0.4949 GradNorm=4.1502 StepSize=0.0415 RelImp=78.45%\n",
            "Epoch[1/2] Step[5690] Loss=0.6697 GradNorm=5.9017 StepSize=0.0590 RelImp=70.84%\n",
            "Epoch[1/2] Step[5691] Loss=0.0958 GradNorm=1.1358 StepSize=0.0114 RelImp=95.83%\n",
            "Epoch[1/2] Step[5692] Loss=0.6380 GradNorm=5.7291 StepSize=0.0573 RelImp=72.22%\n",
            "Epoch[1/2] Step[5693] Loss=0.0970 GradNorm=1.6241 StepSize=0.0162 RelImp=95.77%\n",
            "Epoch[1/2] Step[5694] Loss=0.5369 GradNorm=3.0554 StepSize=0.0306 RelImp=76.62%\n",
            "Epoch[1/2] Step[5695] Loss=0.1395 GradNorm=1.7663 StepSize=0.0177 RelImp=93.93%\n",
            "Epoch[1/2] Step[5696] Loss=0.6320 GradNorm=4.9669 StepSize=0.0497 RelImp=72.48%\n",
            "Epoch[1/2] Step[5697] Loss=0.1299 GradNorm=1.7224 StepSize=0.0172 RelImp=94.34%\n",
            "Epoch[1/2] Step[5698] Loss=0.1580 GradNorm=2.1123 StepSize=0.0211 RelImp=93.12%\n",
            "Epoch[1/2] Step[5699] Loss=0.4025 GradNorm=2.9537 StepSize=0.0295 RelImp=82.48%\n",
            "Epoch[1/2] Step[5700] Loss=0.4016 GradNorm=4.6318 StepSize=0.0463 RelImp=82.51%\n",
            "Epoch[1/2] Step[5701] Loss=0.5214 GradNorm=4.6227 StepSize=0.0462 RelImp=77.30%\n",
            "Epoch[1/2] Step[5702] Loss=0.3875 GradNorm=3.1813 StepSize=0.0318 RelImp=83.13%\n",
            "Epoch[1/2] Step[5703] Loss=0.3803 GradNorm=4.0606 StepSize=0.0406 RelImp=83.44%\n",
            "Epoch[1/2] Step[5704] Loss=0.3456 GradNorm=2.6429 StepSize=0.0264 RelImp=84.95%\n",
            "Epoch[1/2] Step[5705] Loss=0.6093 GradNorm=4.2823 StepSize=0.0428 RelImp=73.47%\n",
            "Epoch[1/2] Step[5706] Loss=0.1864 GradNorm=2.3421 StepSize=0.0234 RelImp=91.88%\n",
            "Epoch[1/2] Step[5707] Loss=0.3510 GradNorm=3.1879 StepSize=0.0319 RelImp=84.72%\n",
            "Epoch[1/2] Step[5708] Loss=0.2236 GradNorm=2.4760 StepSize=0.0248 RelImp=90.26%\n",
            "Epoch[1/2] Step[5709] Loss=0.0920 GradNorm=1.2484 StepSize=0.0125 RelImp=95.99%\n",
            "Epoch[1/2] Step[5710] Loss=0.1410 GradNorm=1.7495 StepSize=0.0175 RelImp=93.86%\n",
            "Epoch[1/2] Step[5711] Loss=0.3665 GradNorm=3.7821 StepSize=0.0378 RelImp=84.04%\n",
            "Epoch[1/2] Step[5712] Loss=0.0650 GradNorm=0.8086 StepSize=0.0081 RelImp=97.17%\n",
            "Epoch[1/2] Step[5713] Loss=0.5457 GradNorm=3.6367 StepSize=0.0364 RelImp=76.24%\n",
            "Epoch[1/2] Step[5714] Loss=0.0487 GradNorm=0.5519 StepSize=0.0055 RelImp=97.88%\n",
            "Epoch[1/2] Step[5715] Loss=0.1578 GradNorm=2.3316 StepSize=0.0233 RelImp=93.13%\n",
            "Epoch[1/2] Step[5716] Loss=0.0958 GradNorm=1.1845 StepSize=0.0118 RelImp=95.83%\n",
            "Epoch[1/2] Step[5717] Loss=0.5801 GradNorm=3.8664 StepSize=0.0387 RelImp=74.74%\n",
            "Epoch[1/2] Step[5718] Loss=0.3667 GradNorm=4.3272 StepSize=0.0433 RelImp=84.03%\n",
            "Epoch[1/2] Step[5719] Loss=0.2918 GradNorm=3.6806 StepSize=0.0368 RelImp=87.29%\n",
            "Epoch[1/2] Step[5720] Loss=0.4431 GradNorm=2.9028 StepSize=0.0290 RelImp=80.71%\n",
            "Epoch[1/2] Step[5721] Loss=0.4353 GradNorm=3.4081 StepSize=0.0341 RelImp=81.05%\n",
            "Epoch[1/2] Step[5722] Loss=0.5899 GradNorm=5.6368 StepSize=0.0564 RelImp=74.31%\n",
            "Epoch[1/2] Step[5723] Loss=0.0798 GradNorm=1.2559 StepSize=0.0126 RelImp=96.52%\n",
            "Epoch[1/2] Step[5724] Loss=0.0761 GradNorm=0.7855 StepSize=0.0079 RelImp=96.69%\n",
            "Epoch[1/2] Step[5725] Loss=0.3748 GradNorm=3.8449 StepSize=0.0384 RelImp=83.68%\n",
            "Epoch[1/2] Step[5726] Loss=0.7367 GradNorm=4.8003 StepSize=0.0480 RelImp=67.92%\n",
            "Epoch[1/2] Step[5727] Loss=0.3382 GradNorm=3.0711 StepSize=0.0307 RelImp=85.27%\n",
            "Epoch[1/2] Step[5728] Loss=0.3330 GradNorm=3.3690 StepSize=0.0337 RelImp=85.50%\n",
            "Epoch[1/2] Step[5729] Loss=0.7378 GradNorm=4.4605 StepSize=0.0446 RelImp=67.87%\n",
            "Epoch[1/2] Step[5730] Loss=0.1810 GradNorm=2.7380 StepSize=0.0274 RelImp=92.12%\n",
            "Epoch[1/2] Step[5731] Loss=0.2680 GradNorm=2.1007 StepSize=0.0210 RelImp=88.33%\n",
            "Epoch[1/2] Step[5732] Loss=0.1959 GradNorm=2.4803 StepSize=0.0248 RelImp=91.47%\n",
            "Epoch[1/2] Step[5733] Loss=0.0918 GradNorm=1.8995 StepSize=0.0190 RelImp=96.00%\n",
            "Epoch[1/2] Step[5734] Loss=0.3628 GradNorm=4.2752 StepSize=0.0428 RelImp=84.20%\n",
            "Epoch[1/2] Step[5735] Loss=0.2155 GradNorm=2.6609 StepSize=0.0266 RelImp=90.62%\n",
            "Epoch[1/2] Step[5736] Loss=0.3079 GradNorm=2.6416 StepSize=0.0264 RelImp=86.59%\n",
            "Epoch[1/2] Step[5737] Loss=0.0924 GradNorm=1.5184 StepSize=0.0152 RelImp=95.98%\n",
            "Epoch[1/2] Step[5738] Loss=0.1612 GradNorm=1.6824 StepSize=0.0168 RelImp=92.98%\n",
            "Epoch[1/2] Step[5739] Loss=0.3791 GradNorm=4.1173 StepSize=0.0412 RelImp=83.49%\n",
            "Epoch[1/2] Step[5740] Loss=0.1377 GradNorm=2.1440 StepSize=0.0214 RelImp=94.00%\n",
            "Epoch[1/2] Step[5741] Loss=0.4107 GradNorm=4.0166 StepSize=0.0402 RelImp=82.12%\n",
            "Epoch[1/2] Step[5742] Loss=0.1523 GradNorm=2.1629 StepSize=0.0216 RelImp=93.37%\n",
            "Epoch[1/2] Step[5743] Loss=0.1199 GradNorm=1.8572 StepSize=0.0186 RelImp=94.78%\n",
            "Epoch[1/2] Step[5744] Loss=1.0216 GradNorm=5.2821 StepSize=0.0528 RelImp=55.52%\n",
            "Epoch[1/2] Step[5745] Loss=0.6496 GradNorm=2.7873 StepSize=0.0279 RelImp=71.71%\n",
            "Epoch[1/2] Step[5746] Loss=0.1304 GradNorm=1.5937 StepSize=0.0159 RelImp=94.32%\n",
            "Epoch[1/2] Step[5747] Loss=0.4012 GradNorm=4.6810 StepSize=0.0468 RelImp=82.53%\n",
            "Epoch[1/2] Step[5748] Loss=0.3137 GradNorm=4.0393 StepSize=0.0404 RelImp=86.34%\n",
            "Epoch[1/2] Step[5749] Loss=0.5590 GradNorm=4.4500 StepSize=0.0445 RelImp=75.66%\n",
            "Epoch[1/2] Step[5750] Loss=0.2454 GradNorm=2.3834 StepSize=0.0238 RelImp=89.31%\n",
            "Epoch[1/2] Step[5751] Loss=0.2613 GradNorm=2.7655 StepSize=0.0277 RelImp=88.62%\n",
            "Epoch[1/2] Step[5752] Loss=0.3898 GradNorm=3.7813 StepSize=0.0378 RelImp=83.03%\n",
            "Epoch[1/2] Step[5753] Loss=0.1400 GradNorm=2.0232 StepSize=0.0202 RelImp=93.91%\n",
            "Epoch[1/2] Step[5754] Loss=0.3061 GradNorm=2.6632 StepSize=0.0266 RelImp=86.67%\n",
            "Epoch[1/2] Step[5755] Loss=0.2693 GradNorm=2.5463 StepSize=0.0255 RelImp=88.27%\n",
            "Epoch[1/2] Step[5756] Loss=0.0805 GradNorm=1.5204 StepSize=0.0152 RelImp=96.50%\n",
            "Epoch[1/2] Step[5757] Loss=0.3469 GradNorm=3.1998 StepSize=0.0320 RelImp=84.90%\n",
            "Epoch[1/2] Step[5758] Loss=0.2129 GradNorm=2.5072 StepSize=0.0251 RelImp=90.73%\n",
            "Epoch[1/2] Step[5759] Loss=0.1277 GradNorm=1.5019 StepSize=0.0150 RelImp=94.44%\n",
            "Epoch[1/2] Step[5760] Loss=0.1167 GradNorm=1.5097 StepSize=0.0151 RelImp=94.92%\n",
            "Epoch[1/2] Step[5761] Loss=0.1236 GradNorm=0.8917 StepSize=0.0089 RelImp=94.62%\n",
            "Epoch[1/2] Step[5762] Loss=0.3246 GradNorm=2.8326 StepSize=0.0283 RelImp=85.87%\n",
            "Epoch[1/2] Step[5763] Loss=0.1854 GradNorm=2.6665 StepSize=0.0267 RelImp=91.93%\n",
            "Epoch[1/2] Step[5764] Loss=0.1531 GradNorm=2.7431 StepSize=0.0274 RelImp=93.33%\n",
            "Epoch[1/2] Step[5765] Loss=0.7925 GradNorm=4.5519 StepSize=0.0455 RelImp=65.49%\n",
            "Epoch[1/2] Step[5766] Loss=0.0969 GradNorm=1.7287 StepSize=0.0173 RelImp=95.78%\n",
            "Epoch[1/2] Step[5767] Loss=1.0349 GradNorm=3.6202 StepSize=0.0362 RelImp=54.94%\n",
            "Epoch[1/2] Step[5768] Loss=0.3627 GradNorm=3.5724 StepSize=0.0357 RelImp=84.21%\n",
            "Epoch[1/2] Step[5769] Loss=0.6580 GradNorm=5.3623 StepSize=0.0536 RelImp=71.35%\n",
            "Epoch[1/2] Step[5770] Loss=0.1742 GradNorm=2.5218 StepSize=0.0252 RelImp=92.42%\n",
            "Epoch[1/2] Step[5771] Loss=0.0604 GradNorm=1.2810 StepSize=0.0128 RelImp=97.37%\n",
            "Epoch[1/2] Step[5772] Loss=0.5542 GradNorm=4.2081 StepSize=0.0421 RelImp=75.87%\n",
            "Epoch[1/2] Step[5773] Loss=0.0951 GradNorm=1.6347 StepSize=0.0163 RelImp=95.86%\n",
            "Epoch[1/2] Step[5774] Loss=0.0927 GradNorm=1.1770 StepSize=0.0118 RelImp=95.96%\n",
            "Epoch[1/2] Step[5775] Loss=0.0477 GradNorm=0.6482 StepSize=0.0065 RelImp=97.92%\n",
            "Epoch[1/2] Step[5776] Loss=0.5288 GradNorm=2.7596 StepSize=0.0276 RelImp=76.98%\n",
            "Epoch[1/2] Step[5777] Loss=0.4250 GradNorm=3.6599 StepSize=0.0366 RelImp=81.50%\n",
            "Epoch[1/2] Step[5778] Loss=0.4706 GradNorm=3.9592 StepSize=0.0396 RelImp=79.51%\n",
            "Epoch[1/2] Step[5779] Loss=0.3445 GradNorm=2.9182 StepSize=0.0292 RelImp=85.00%\n",
            "Epoch[1/2] Step[5780] Loss=0.1055 GradNorm=2.1482 StepSize=0.0215 RelImp=95.41%\n",
            "Epoch[1/2] Step[5781] Loss=0.2563 GradNorm=3.6471 StepSize=0.0365 RelImp=88.84%\n",
            "Epoch[1/2] Step[5782] Loss=0.0848 GradNorm=1.2890 StepSize=0.0129 RelImp=96.31%\n",
            "Epoch[1/2] Step[5783] Loss=0.3066 GradNorm=4.2542 StepSize=0.0425 RelImp=86.65%\n",
            "Epoch[1/2] Step[5784] Loss=0.2985 GradNorm=2.9743 StepSize=0.0297 RelImp=87.00%\n",
            "Epoch[1/2] Step[5785] Loss=0.2596 GradNorm=4.5543 StepSize=0.0455 RelImp=88.70%\n",
            "Epoch[1/2] Step[5786] Loss=0.2839 GradNorm=3.1268 StepSize=0.0313 RelImp=87.64%\n",
            "Epoch[1/2] Step[5787] Loss=0.4522 GradNorm=6.0997 StepSize=0.0610 RelImp=80.31%\n",
            "Epoch[1/2] Step[5788] Loss=0.5018 GradNorm=3.1937 StepSize=0.0319 RelImp=78.15%\n",
            "Epoch[1/2] Step[5789] Loss=0.2011 GradNorm=3.0282 StepSize=0.0303 RelImp=91.24%\n",
            "Epoch[1/2] Step[5790] Loss=0.1421 GradNorm=2.2566 StepSize=0.0226 RelImp=93.81%\n",
            "Epoch[1/2] Step[5791] Loss=0.2688 GradNorm=2.8736 StepSize=0.0287 RelImp=88.29%\n",
            "Epoch[1/2] Step[5792] Loss=0.5183 GradNorm=2.6126 StepSize=0.0261 RelImp=77.43%\n",
            "Epoch[1/2] Step[5793] Loss=0.4078 GradNorm=2.2354 StepSize=0.0224 RelImp=82.24%\n",
            "Epoch[1/2] Step[5794] Loss=0.5124 GradNorm=3.5480 StepSize=0.0355 RelImp=77.69%\n",
            "Epoch[1/2] Step[5795] Loss=0.0905 GradNorm=1.4566 StepSize=0.0146 RelImp=96.06%\n",
            "Epoch[1/2] Step[5796] Loss=0.5897 GradNorm=4.6347 StepSize=0.0463 RelImp=74.32%\n",
            "Epoch[1/2] Step[5797] Loss=1.0588 GradNorm=5.2000 StepSize=0.0520 RelImp=53.90%\n",
            "Epoch[1/2] Step[5798] Loss=0.2950 GradNorm=3.8790 StepSize=0.0388 RelImp=87.15%\n",
            "Epoch[1/2] Step[5799] Loss=0.4869 GradNorm=3.1240 StepSize=0.0312 RelImp=78.80%\n",
            "Epoch[1/2] Step[5800] Loss=0.6814 GradNorm=5.1315 StepSize=0.0513 RelImp=70.33%\n",
            "Epoch[1/2] Step[5801] Loss=0.3260 GradNorm=3.2783 StepSize=0.0328 RelImp=85.81%\n",
            "Epoch[1/2] Step[5802] Loss=0.1423 GradNorm=1.6216 StepSize=0.0162 RelImp=93.80%\n",
            "Epoch[1/2] Step[5803] Loss=0.5792 GradNorm=3.6643 StepSize=0.0366 RelImp=74.78%\n",
            "Epoch[1/2] Step[5804] Loss=0.3553 GradNorm=3.7563 StepSize=0.0376 RelImp=84.53%\n",
            "Epoch[1/2] Step[5805] Loss=0.5211 GradNorm=5.1350 StepSize=0.0514 RelImp=77.31%\n",
            "Epoch[1/2] Step[5806] Loss=0.1426 GradNorm=2.0908 StepSize=0.0209 RelImp=93.79%\n",
            "Epoch[1/2] Step[5807] Loss=0.5899 GradNorm=3.4770 StepSize=0.0348 RelImp=74.31%\n",
            "Epoch[1/2] Step[5808] Loss=0.3585 GradNorm=4.5732 StepSize=0.0457 RelImp=84.39%\n",
            "Epoch[1/2] Step[5809] Loss=0.2015 GradNorm=2.3368 StepSize=0.0234 RelImp=91.23%\n",
            "Epoch[1/2] Step[5810] Loss=0.2344 GradNorm=2.4786 StepSize=0.0248 RelImp=89.80%\n",
            "Epoch[1/2] Step[5811] Loss=0.2778 GradNorm=4.1990 StepSize=0.0420 RelImp=87.90%\n",
            "Epoch[1/2] Step[5812] Loss=0.4531 GradNorm=4.6920 StepSize=0.0469 RelImp=80.27%\n",
            "Epoch[1/2] Step[5813] Loss=0.2286 GradNorm=2.1026 StepSize=0.0210 RelImp=90.05%\n",
            "Epoch[1/2] Step[5814] Loss=0.5444 GradNorm=4.0689 StepSize=0.0407 RelImp=76.30%\n",
            "Epoch[1/2] Step[5815] Loss=0.1228 GradNorm=2.1492 StepSize=0.0215 RelImp=94.65%\n",
            "Epoch[1/2] Step[5816] Loss=0.2126 GradNorm=3.1387 StepSize=0.0314 RelImp=90.74%\n",
            "Epoch[1/2] Step[5817] Loss=0.2589 GradNorm=2.8260 StepSize=0.0283 RelImp=88.72%\n",
            "Epoch[1/2] Step[5818] Loss=0.4214 GradNorm=2.6922 StepSize=0.0269 RelImp=81.65%\n",
            "Epoch[1/2] Step[5819] Loss=0.4034 GradNorm=4.3675 StepSize=0.0437 RelImp=82.44%\n",
            "Epoch[1/2] Step[5820] Loss=0.2153 GradNorm=3.2116 StepSize=0.0321 RelImp=90.63%\n",
            "Epoch[1/2] Step[5821] Loss=0.9688 GradNorm=3.7826 StepSize=0.0378 RelImp=57.82%\n",
            "Epoch[1/2] Step[5822] Loss=0.0877 GradNorm=1.7121 StepSize=0.0171 RelImp=96.18%\n",
            "Epoch[1/2] Step[5823] Loss=0.3336 GradNorm=3.1822 StepSize=0.0318 RelImp=85.47%\n",
            "Epoch[1/2] Step[5824] Loss=0.6379 GradNorm=5.1747 StepSize=0.0517 RelImp=72.22%\n",
            "Epoch[1/2] Step[5825] Loss=0.7083 GradNorm=5.5079 StepSize=0.0551 RelImp=69.16%\n",
            "Epoch[1/2] Step[5826] Loss=0.2861 GradNorm=3.9267 StepSize=0.0393 RelImp=87.54%\n",
            "Epoch[1/2] Step[5827] Loss=0.3321 GradNorm=2.8643 StepSize=0.0286 RelImp=85.54%\n",
            "Epoch[1/2] Step[5828] Loss=0.3789 GradNorm=3.9380 StepSize=0.0394 RelImp=83.50%\n",
            "Epoch[1/2] Step[5829] Loss=0.1602 GradNorm=2.0325 StepSize=0.0203 RelImp=93.02%\n",
            "Epoch[1/2] Step[5830] Loss=0.2998 GradNorm=3.9805 StepSize=0.0398 RelImp=86.94%\n",
            "Epoch[1/2] Step[5831] Loss=1.1070 GradNorm=4.0335 StepSize=0.0403 RelImp=51.80%\n",
            "Epoch[1/2] Step[5832] Loss=1.2126 GradNorm=4.4444 StepSize=0.0444 RelImp=47.20%\n",
            "Epoch[1/2] Step[5833] Loss=0.3196 GradNorm=3.2487 StepSize=0.0325 RelImp=86.08%\n",
            "Epoch[1/2] Step[5834] Loss=0.6956 GradNorm=6.0948 StepSize=0.0609 RelImp=69.71%\n",
            "Epoch[1/2] Step[5835] Loss=0.2681 GradNorm=3.4477 StepSize=0.0345 RelImp=88.33%\n",
            "Epoch[1/2] Step[5836] Loss=0.6318 GradNorm=3.6154 StepSize=0.0362 RelImp=72.49%\n",
            "Epoch[1/2] Step[5837] Loss=0.2098 GradNorm=2.1709 StepSize=0.0217 RelImp=90.87%\n",
            "Epoch[1/2] Step[5838] Loss=0.5021 GradNorm=3.8017 StepSize=0.0380 RelImp=78.14%\n",
            "Epoch[1/2] Step[5839] Loss=0.2186 GradNorm=3.0113 StepSize=0.0301 RelImp=90.48%\n",
            "Epoch[1/2] Step[5840] Loss=0.4899 GradNorm=3.6415 StepSize=0.0364 RelImp=78.67%\n",
            "Epoch[1/2] Step[5841] Loss=0.5673 GradNorm=3.2868 StepSize=0.0329 RelImp=75.30%\n",
            "Epoch[1/2] Step[5842] Loss=0.0666 GradNorm=1.2426 StepSize=0.0124 RelImp=97.10%\n",
            "Epoch[1/2] Step[5843] Loss=0.2404 GradNorm=2.6214 StepSize=0.0262 RelImp=89.53%\n",
            "Epoch[1/2] Step[5844] Loss=0.1526 GradNorm=1.5707 StepSize=0.0157 RelImp=93.36%\n",
            "Epoch[1/2] Step[5845] Loss=0.4389 GradNorm=4.0024 StepSize=0.0400 RelImp=80.89%\n",
            "Epoch[1/2] Step[5846] Loss=0.2852 GradNorm=3.1919 StepSize=0.0319 RelImp=87.58%\n",
            "Epoch[1/2] Step[5847] Loss=0.3187 GradNorm=2.2342 StepSize=0.0223 RelImp=86.12%\n",
            "Epoch[1/2] Step[5848] Loss=0.3800 GradNorm=4.2450 StepSize=0.0425 RelImp=83.45%\n",
            "Epoch[1/2] Step[5849] Loss=0.2634 GradNorm=2.7308 StepSize=0.0273 RelImp=88.53%\n",
            "Epoch[1/2] Step[5850] Loss=0.2107 GradNorm=3.9782 StepSize=0.0398 RelImp=90.83%\n",
            "Epoch[1/2] Step[5851] Loss=0.1795 GradNorm=2.3936 StepSize=0.0239 RelImp=92.18%\n",
            "Epoch[1/2] Step[5852] Loss=0.3900 GradNorm=4.0579 StepSize=0.0406 RelImp=83.02%\n",
            "Epoch[1/2] Step[5853] Loss=0.2461 GradNorm=2.9935 StepSize=0.0299 RelImp=89.28%\n",
            "Epoch[1/2] Step[5854] Loss=0.1421 GradNorm=1.7484 StepSize=0.0175 RelImp=93.81%\n",
            "Epoch[1/2] Step[5855] Loss=0.8319 GradNorm=3.7099 StepSize=0.0371 RelImp=63.78%\n",
            "Epoch[1/2] Step[5856] Loss=0.5153 GradNorm=3.9988 StepSize=0.0400 RelImp=77.56%\n",
            "Epoch[1/2] Step[5857] Loss=0.3660 GradNorm=2.1260 StepSize=0.0213 RelImp=84.06%\n",
            "Epoch[1/2] Step[5858] Loss=0.2989 GradNorm=2.2583 StepSize=0.0226 RelImp=86.98%\n",
            "Epoch[1/2] Step[5859] Loss=0.2559 GradNorm=1.8827 StepSize=0.0188 RelImp=88.86%\n",
            "Epoch[1/2] Step[5860] Loss=0.3642 GradNorm=3.5045 StepSize=0.0350 RelImp=84.14%\n",
            "Epoch[1/2] Step[5861] Loss=0.1977 GradNorm=2.3406 StepSize=0.0234 RelImp=91.39%\n",
            "Epoch[1/2] Step[5862] Loss=0.3004 GradNorm=3.6779 StepSize=0.0368 RelImp=86.92%\n",
            "Epoch[1/2] Step[5863] Loss=0.5854 GradNorm=5.2597 StepSize=0.0526 RelImp=74.51%\n",
            "Epoch[1/2] Step[5864] Loss=0.3192 GradNorm=2.8760 StepSize=0.0288 RelImp=86.10%\n",
            "Epoch[1/2] Step[5865] Loss=0.2259 GradNorm=3.0372 StepSize=0.0304 RelImp=90.17%\n",
            "Epoch[1/2] Step[5866] Loss=0.5194 GradNorm=4.8117 StepSize=0.0481 RelImp=77.38%\n",
            "Epoch[1/2] Step[5867] Loss=0.2847 GradNorm=2.6441 StepSize=0.0264 RelImp=87.60%\n",
            "Epoch[1/2] Step[5868] Loss=0.1602 GradNorm=1.5467 StepSize=0.0155 RelImp=93.03%\n",
            "Epoch[1/2] Step[5869] Loss=0.3573 GradNorm=3.1394 StepSize=0.0314 RelImp=84.44%\n",
            "Epoch[1/2] Step[5870] Loss=0.1752 GradNorm=2.0728 StepSize=0.0207 RelImp=92.37%\n",
            "Epoch[1/2] Step[5871] Loss=0.0642 GradNorm=0.8746 StepSize=0.0087 RelImp=97.20%\n",
            "Epoch[1/2] Step[5872] Loss=0.7808 GradNorm=4.6714 StepSize=0.0467 RelImp=66.00%\n",
            "Epoch[1/2] Step[5873] Loss=0.3758 GradNorm=3.5298 StepSize=0.0353 RelImp=83.64%\n",
            "Epoch[1/2] Step[5874] Loss=0.1329 GradNorm=2.0905 StepSize=0.0209 RelImp=94.22%\n",
            "Epoch[1/2] Step[5875] Loss=0.4184 GradNorm=2.7545 StepSize=0.0275 RelImp=81.78%\n",
            "Epoch[1/2] Step[5876] Loss=0.3769 GradNorm=2.5609 StepSize=0.0256 RelImp=83.59%\n",
            "Epoch[1/2] Step[5877] Loss=0.0807 GradNorm=1.0400 StepSize=0.0104 RelImp=96.49%\n",
            "Epoch[1/2] Step[5878] Loss=0.9193 GradNorm=5.3145 StepSize=0.0531 RelImp=59.97%\n",
            "Epoch[1/2] Step[5879] Loss=0.2235 GradNorm=2.6101 StepSize=0.0261 RelImp=90.27%\n",
            "Epoch[1/2] Step[5880] Loss=0.2838 GradNorm=3.0252 StepSize=0.0303 RelImp=87.64%\n",
            "Epoch[1/2] Step[5881] Loss=0.5930 GradNorm=4.7919 StepSize=0.0479 RelImp=74.18%\n",
            "Epoch[1/2] Step[5882] Loss=0.3447 GradNorm=3.3880 StepSize=0.0339 RelImp=84.99%\n",
            "Epoch[1/2] Step[5883] Loss=0.1467 GradNorm=1.4748 StepSize=0.0147 RelImp=93.61%\n",
            "Epoch[1/2] Step[5884] Loss=0.1822 GradNorm=2.2021 StepSize=0.0220 RelImp=92.07%\n",
            "Epoch[1/2] Step[5885] Loss=0.2329 GradNorm=2.5651 StepSize=0.0257 RelImp=89.86%\n",
            "Epoch[1/2] Step[5886] Loss=0.2168 GradNorm=1.7661 StepSize=0.0177 RelImp=90.56%\n",
            "Epoch[1/2] Step[5887] Loss=0.2576 GradNorm=2.3309 StepSize=0.0233 RelImp=88.78%\n",
            "Epoch[1/2] Step[5888] Loss=0.2039 GradNorm=1.6450 StepSize=0.0164 RelImp=91.12%\n",
            "Epoch[1/2] Step[5889] Loss=0.3968 GradNorm=3.5309 StepSize=0.0353 RelImp=82.72%\n",
            "Epoch[1/2] Step[5890] Loss=0.1534 GradNorm=2.2159 StepSize=0.0222 RelImp=93.32%\n",
            "Epoch[1/2] Step[5891] Loss=0.2260 GradNorm=2.6810 StepSize=0.0268 RelImp=90.16%\n",
            "Epoch[1/2] Step[5892] Loss=0.6761 GradNorm=4.4831 StepSize=0.0448 RelImp=70.56%\n",
            "Epoch[1/2] Step[5893] Loss=0.1256 GradNorm=2.2076 StepSize=0.0221 RelImp=94.53%\n",
            "Epoch[1/2] Step[5894] Loss=0.7557 GradNorm=3.9244 StepSize=0.0392 RelImp=67.09%\n",
            "Epoch[1/2] Step[5895] Loss=0.3245 GradNorm=2.5183 StepSize=0.0252 RelImp=85.87%\n",
            "Epoch[1/2] Step[5896] Loss=0.1306 GradNorm=1.9763 StepSize=0.0198 RelImp=94.31%\n",
            "Epoch[1/2] Step[5897] Loss=0.4350 GradNorm=4.0146 StepSize=0.0401 RelImp=81.06%\n",
            "Epoch[1/2] Step[5898] Loss=0.1751 GradNorm=2.1347 StepSize=0.0213 RelImp=92.38%\n",
            "Epoch[1/2] Step[5899] Loss=0.6034 GradNorm=4.3710 StepSize=0.0437 RelImp=73.72%\n",
            "Epoch[1/2] Step[5900] Loss=0.8725 GradNorm=4.8359 StepSize=0.0484 RelImp=62.01%\n",
            "Epoch[1/2] Step[5901] Loss=0.3360 GradNorm=3.6460 StepSize=0.0365 RelImp=85.37%\n",
            "Epoch[1/2] Step[5902] Loss=0.5216 GradNorm=3.2488 StepSize=0.0325 RelImp=77.29%\n",
            "Epoch[1/2] Step[5903] Loss=0.3512 GradNorm=4.4171 StepSize=0.0442 RelImp=84.71%\n",
            "Epoch[1/2] Step[5904] Loss=0.0981 GradNorm=1.3558 StepSize=0.0136 RelImp=95.73%\n",
            "Epoch[1/2] Step[5905] Loss=0.1210 GradNorm=1.3815 StepSize=0.0138 RelImp=94.73%\n",
            "Epoch[1/2] Step[5906] Loss=0.9171 GradNorm=5.6380 StepSize=0.0564 RelImp=60.07%\n",
            "Epoch[1/2] Step[5907] Loss=0.0691 GradNorm=0.6488 StepSize=0.0065 RelImp=96.99%\n",
            "Epoch[1/2] Step[5908] Loss=0.2796 GradNorm=2.6501 StepSize=0.0265 RelImp=87.83%\n",
            "Epoch[1/2] Step[5909] Loss=0.8763 GradNorm=4.8606 StepSize=0.0486 RelImp=61.85%\n",
            "Epoch[1/2] Step[5910] Loss=0.3729 GradNorm=5.0546 StepSize=0.0505 RelImp=83.76%\n",
            "Epoch[1/2] Step[5911] Loss=0.3799 GradNorm=3.3005 StepSize=0.0330 RelImp=83.46%\n",
            "Epoch[1/2] Step[5912] Loss=0.7380 GradNorm=4.6376 StepSize=0.0464 RelImp=67.87%\n",
            "Epoch[1/2] Step[5913] Loss=0.6162 GradNorm=2.6174 StepSize=0.0262 RelImp=73.17%\n",
            "Epoch[1/2] Step[5914] Loss=0.5796 GradNorm=5.8504 StepSize=0.0585 RelImp=74.76%\n",
            "Epoch[1/2] Step[5915] Loss=0.2651 GradNorm=3.3288 StepSize=0.0333 RelImp=88.46%\n",
            "Epoch[1/2] Step[5916] Loss=0.1804 GradNorm=2.1463 StepSize=0.0215 RelImp=92.15%\n",
            "Epoch[1/2] Step[5917] Loss=0.3148 GradNorm=4.0387 StepSize=0.0404 RelImp=86.29%\n",
            "Epoch[1/2] Step[5918] Loss=0.1713 GradNorm=2.5732 StepSize=0.0257 RelImp=92.54%\n",
            "Epoch[1/2] Step[5919] Loss=0.4025 GradNorm=2.7242 StepSize=0.0272 RelImp=82.47%\n",
            "Epoch[1/2] Step[5920] Loss=0.3700 GradNorm=2.5502 StepSize=0.0255 RelImp=83.89%\n",
            "Epoch[1/2] Step[5921] Loss=0.1066 GradNorm=1.8093 StepSize=0.0181 RelImp=95.36%\n",
            "Epoch[1/2] Step[5922] Loss=0.5894 GradNorm=4.2307 StepSize=0.0423 RelImp=74.34%\n",
            "Epoch[1/2] Step[5923] Loss=0.2273 GradNorm=2.5815 StepSize=0.0258 RelImp=90.10%\n",
            "Epoch[1/2] Step[5924] Loss=0.4131 GradNorm=3.1634 StepSize=0.0316 RelImp=82.01%\n",
            "Epoch[1/2] Step[5925] Loss=0.3982 GradNorm=3.8929 StepSize=0.0389 RelImp=82.66%\n",
            "Epoch[1/2] Step[5926] Loss=0.1715 GradNorm=2.9398 StepSize=0.0294 RelImp=92.53%\n",
            "Epoch[1/2] Step[5927] Loss=0.1745 GradNorm=1.6536 StepSize=0.0165 RelImp=92.40%\n",
            "Epoch[1/2] Step[5928] Loss=0.2503 GradNorm=2.5285 StepSize=0.0253 RelImp=89.10%\n",
            "Epoch[1/2] Step[5929] Loss=0.2236 GradNorm=2.2580 StepSize=0.0226 RelImp=90.26%\n",
            "Epoch[1/2] Step[5930] Loss=0.2632 GradNorm=2.4329 StepSize=0.0243 RelImp=88.54%\n",
            "Epoch[1/2] Step[5931] Loss=0.5683 GradNorm=4.0210 StepSize=0.0402 RelImp=75.26%\n",
            "Epoch[1/2] Step[5932] Loss=0.3846 GradNorm=3.5046 StepSize=0.0350 RelImp=83.25%\n",
            "Epoch[1/2] Step[5933] Loss=0.1717 GradNorm=1.8675 StepSize=0.0187 RelImp=92.52%\n",
            "Epoch[1/2] Step[5934] Loss=0.9387 GradNorm=6.7881 StepSize=0.0679 RelImp=59.13%\n",
            "Epoch[1/2] Step[5935] Loss=0.1753 GradNorm=1.7807 StepSize=0.0178 RelImp=92.37%\n",
            "Epoch[1/2] Step[5936] Loss=0.5297 GradNorm=5.9179 StepSize=0.0592 RelImp=76.94%\n",
            "Epoch[1/2] Step[5937] Loss=0.7362 GradNorm=4.6438 StepSize=0.0464 RelImp=67.94%\n",
            "Epoch[1/2] Step[5938] Loss=0.7261 GradNorm=4.4389 StepSize=0.0444 RelImp=68.39%\n",
            "Epoch[1/2] Step[5939] Loss=0.1619 GradNorm=2.4890 StepSize=0.0249 RelImp=92.95%\n",
            "Epoch[1/2] Step[5940] Loss=0.2817 GradNorm=2.0632 StepSize=0.0206 RelImp=87.73%\n",
            "Epoch[1/2] Step[5941] Loss=0.3372 GradNorm=3.0721 StepSize=0.0307 RelImp=85.32%\n",
            "Epoch[1/2] Step[5942] Loss=0.3778 GradNorm=3.8472 StepSize=0.0385 RelImp=83.55%\n",
            "Epoch[1/2] Step[5943] Loss=0.1673 GradNorm=1.7806 StepSize=0.0178 RelImp=92.72%\n",
            "Epoch[1/2] Step[5944] Loss=0.3816 GradNorm=2.1778 StepSize=0.0218 RelImp=83.38%\n",
            "Epoch[1/2] Step[5945] Loss=0.6678 GradNorm=4.6227 StepSize=0.0462 RelImp=70.92%\n",
            "Epoch[1/2] Step[5946] Loss=0.7228 GradNorm=3.6380 StepSize=0.0364 RelImp=68.53%\n",
            "Epoch[1/2] Step[5947] Loss=0.3810 GradNorm=4.1862 StepSize=0.0419 RelImp=83.41%\n",
            "Epoch[1/2] Step[5948] Loss=0.3197 GradNorm=3.4650 StepSize=0.0346 RelImp=86.08%\n",
            "Epoch[1/2] Step[5949] Loss=0.3406 GradNorm=3.0791 StepSize=0.0308 RelImp=85.17%\n",
            "Epoch[1/2] Step[5950] Loss=0.2486 GradNorm=2.6676 StepSize=0.0267 RelImp=89.18%\n",
            "Epoch[1/2] Step[5951] Loss=0.0881 GradNorm=1.1294 StepSize=0.0113 RelImp=96.17%\n",
            "Epoch[1/2] Step[5952] Loss=0.3192 GradNorm=3.0664 StepSize=0.0307 RelImp=86.10%\n",
            "Epoch[1/2] Step[5953] Loss=0.2263 GradNorm=2.4821 StepSize=0.0248 RelImp=90.15%\n",
            "Epoch[1/2] Step[5954] Loss=0.2159 GradNorm=2.8033 StepSize=0.0280 RelImp=90.60%\n",
            "Epoch[1/2] Step[5955] Loss=0.2615 GradNorm=2.7735 StepSize=0.0277 RelImp=88.62%\n",
            "Epoch[1/2] Step[5956] Loss=0.0351 GradNorm=0.5387 StepSize=0.0054 RelImp=98.47%\n",
            "Epoch[1/2] Step[5957] Loss=0.3120 GradNorm=2.7796 StepSize=0.0278 RelImp=86.41%\n",
            "Epoch[1/2] Step[5958] Loss=0.0503 GradNorm=0.8904 StepSize=0.0089 RelImp=97.81%\n",
            "Epoch[1/2] Step[5959] Loss=0.2947 GradNorm=3.8322 StepSize=0.0383 RelImp=87.17%\n",
            "Epoch[1/2] Step[5960] Loss=0.2213 GradNorm=2.1803 StepSize=0.0218 RelImp=90.36%\n",
            "Epoch[1/2] Step[5961] Loss=0.2344 GradNorm=2.3216 StepSize=0.0232 RelImp=89.79%\n",
            "Epoch[1/2] Step[5962] Loss=0.1202 GradNorm=1.7570 StepSize=0.0176 RelImp=94.77%\n",
            "Epoch[1/2] Step[5963] Loss=0.3235 GradNorm=2.4021 StepSize=0.0240 RelImp=85.91%\n",
            "Epoch[1/2] Step[5964] Loss=0.4349 GradNorm=3.7532 StepSize=0.0375 RelImp=81.07%\n",
            "Epoch[1/2] Step[5965] Loss=0.3973 GradNorm=4.2433 StepSize=0.0424 RelImp=82.70%\n",
            "Epoch[1/2] Step[5966] Loss=0.2796 GradNorm=3.0872 StepSize=0.0309 RelImp=87.82%\n",
            "Epoch[1/2] Step[5967] Loss=0.5236 GradNorm=4.3335 StepSize=0.0433 RelImp=77.20%\n",
            "Epoch[1/2] Step[5968] Loss=0.1355 GradNorm=1.4393 StepSize=0.0144 RelImp=94.10%\n",
            "Epoch[1/2] Step[5969] Loss=0.5374 GradNorm=3.2905 StepSize=0.0329 RelImp=76.60%\n",
            "Epoch[1/2] Step[5970] Loss=0.3656 GradNorm=2.9254 StepSize=0.0293 RelImp=84.08%\n",
            "Epoch[1/2] Step[5971] Loss=0.1395 GradNorm=2.4433 StepSize=0.0244 RelImp=93.93%\n",
            "Epoch[1/2] Step[5972] Loss=0.1363 GradNorm=2.0579 StepSize=0.0206 RelImp=94.06%\n",
            "Epoch[1/2] Step[5973] Loss=0.2987 GradNorm=2.2801 StepSize=0.0228 RelImp=87.00%\n",
            "Epoch[1/2] Step[5974] Loss=0.6526 GradNorm=2.7884 StepSize=0.0279 RelImp=71.58%\n",
            "Epoch[1/2] Step[5975] Loss=0.4672 GradNorm=3.1032 StepSize=0.0310 RelImp=79.66%\n",
            "Epoch[1/2] Step[5976] Loss=0.1115 GradNorm=1.5395 StepSize=0.0154 RelImp=95.14%\n",
            "Epoch[1/2] Step[5977] Loss=0.5317 GradNorm=3.6824 StepSize=0.0368 RelImp=76.85%\n",
            "Epoch[1/2] Step[5978] Loss=0.1881 GradNorm=2.6061 StepSize=0.0261 RelImp=91.81%\n",
            "Epoch[1/2] Step[5979] Loss=0.2517 GradNorm=2.8019 StepSize=0.0280 RelImp=89.04%\n",
            "Epoch[1/2] Step[5980] Loss=0.1825 GradNorm=1.9029 StepSize=0.0190 RelImp=92.05%\n",
            "Epoch[1/2] Step[5981] Loss=0.3793 GradNorm=4.9631 StepSize=0.0496 RelImp=83.48%\n",
            "Epoch[1/2] Step[5982] Loss=0.2066 GradNorm=3.2767 StepSize=0.0328 RelImp=91.00%\n",
            "Epoch[1/2] Step[5983] Loss=0.3160 GradNorm=2.4231 StepSize=0.0242 RelImp=86.24%\n",
            "Epoch[1/2] Step[5984] Loss=0.5362 GradNorm=3.4251 StepSize=0.0343 RelImp=76.65%\n",
            "Epoch[1/2] Step[5985] Loss=0.2307 GradNorm=2.9194 StepSize=0.0292 RelImp=89.95%\n",
            "Epoch[1/2] Step[5986] Loss=0.3686 GradNorm=3.2188 StepSize=0.0322 RelImp=83.95%\n",
            "Epoch[1/2] Step[5987] Loss=0.1075 GradNorm=1.8028 StepSize=0.0180 RelImp=95.32%\n",
            "Epoch[1/2] Step[5988] Loss=0.1063 GradNorm=1.4684 StepSize=0.0147 RelImp=95.37%\n",
            "Epoch[1/2] Step[5989] Loss=0.2298 GradNorm=2.9577 StepSize=0.0296 RelImp=90.00%\n",
            "Epoch[1/2] Step[5990] Loss=0.0892 GradNorm=1.3233 StepSize=0.0132 RelImp=96.12%\n",
            "Epoch[1/2] Step[5991] Loss=0.1642 GradNorm=1.9252 StepSize=0.0193 RelImp=92.85%\n",
            "Epoch[1/2] Step[5992] Loss=0.7993 GradNorm=4.3812 StepSize=0.0438 RelImp=65.20%\n",
            "Epoch[1/2] Step[5993] Loss=0.5201 GradNorm=4.7535 StepSize=0.0475 RelImp=77.35%\n",
            "Epoch[1/2] Step[5994] Loss=0.1466 GradNorm=2.4923 StepSize=0.0249 RelImp=93.62%\n",
            "Epoch[1/2] Step[5995] Loss=0.2481 GradNorm=3.5066 StepSize=0.0351 RelImp=89.20%\n",
            "Epoch[1/2] Step[5996] Loss=0.9556 GradNorm=6.5712 StepSize=0.0657 RelImp=58.39%\n",
            "Epoch[1/2] Step[5997] Loss=0.1612 GradNorm=2.6553 StepSize=0.0266 RelImp=92.98%\n",
            "Epoch[1/2] Step[5998] Loss=0.1836 GradNorm=2.5209 StepSize=0.0252 RelImp=92.01%\n",
            "Epoch[1/2] Step[5999] Loss=0.5283 GradNorm=4.4662 StepSize=0.0447 RelImp=77.00%\n",
            "Epoch[1/2] Step[6000] Loss=0.2358 GradNorm=4.0647 StepSize=0.0406 RelImp=89.73%\n",
            "Epoch[1/2] Step[6001] Loss=0.2064 GradNorm=2.6319 StepSize=0.0263 RelImp=91.01%\n",
            "Epoch[1/2] Step[6002] Loss=0.2447 GradNorm=2.1833 StepSize=0.0218 RelImp=89.35%\n",
            "Epoch[1/2] Step[6003] Loss=0.5516 GradNorm=4.2616 StepSize=0.0426 RelImp=75.98%\n",
            "Epoch[1/2] Step[6004] Loss=0.7993 GradNorm=5.4117 StepSize=0.0541 RelImp=65.20%\n",
            "Epoch[1/2] Step[6005] Loss=0.4460 GradNorm=3.8650 StepSize=0.0387 RelImp=80.58%\n",
            "Epoch[1/2] Step[6006] Loss=0.1489 GradNorm=1.5730 StepSize=0.0157 RelImp=93.51%\n",
            "Epoch[1/2] Step[6007] Loss=0.4865 GradNorm=2.6556 StepSize=0.0266 RelImp=78.82%\n",
            "Epoch[1/2] Step[6008] Loss=0.2846 GradNorm=3.9767 StepSize=0.0398 RelImp=87.61%\n",
            "Epoch[1/2] Step[6009] Loss=0.3488 GradNorm=2.5347 StepSize=0.0253 RelImp=84.81%\n",
            "Epoch[1/2] Step[6010] Loss=0.5267 GradNorm=4.9798 StepSize=0.0498 RelImp=77.07%\n",
            "Epoch[1/2] Step[6011] Loss=0.4951 GradNorm=4.0457 StepSize=0.0405 RelImp=78.44%\n",
            "Epoch[1/2] Step[6012] Loss=0.3528 GradNorm=3.7299 StepSize=0.0373 RelImp=84.64%\n",
            "Epoch[1/2] Step[6013] Loss=0.2823 GradNorm=2.9925 StepSize=0.0299 RelImp=87.71%\n",
            "Epoch[1/2] Step[6014] Loss=0.1815 GradNorm=2.6763 StepSize=0.0268 RelImp=92.10%\n",
            "Epoch[1/2] Step[6015] Loss=0.5872 GradNorm=3.8664 StepSize=0.0387 RelImp=74.43%\n",
            "Epoch[1/2] Step[6016] Loss=0.0795 GradNorm=1.5063 StepSize=0.0151 RelImp=96.54%\n",
            "Epoch[1/2] Step[6017] Loss=0.4122 GradNorm=4.0688 StepSize=0.0407 RelImp=82.05%\n",
            "Epoch[1/2] Step[6018] Loss=0.1626 GradNorm=2.1712 StepSize=0.0217 RelImp=92.92%\n",
            "Epoch[1/2] Step[6019] Loss=0.2488 GradNorm=2.4535 StepSize=0.0245 RelImp=89.17%\n",
            "Epoch[1/2] Step[6020] Loss=0.7108 GradNorm=3.1936 StepSize=0.0319 RelImp=69.05%\n",
            "Epoch[1/2] Step[6021] Loss=0.0521 GradNorm=0.6325 StepSize=0.0063 RelImp=97.73%\n",
            "Epoch[1/2] Step[6022] Loss=0.7009 GradNorm=6.1270 StepSize=0.0613 RelImp=69.48%\n",
            "Epoch[1/2] Step[6023] Loss=0.0764 GradNorm=1.2856 StepSize=0.0129 RelImp=96.67%\n",
            "Epoch[1/2] Step[6024] Loss=0.1794 GradNorm=2.5234 StepSize=0.0252 RelImp=92.19%\n",
            "Epoch[1/2] Step[6025] Loss=0.2876 GradNorm=3.3329 StepSize=0.0333 RelImp=87.48%\n",
            "Epoch[1/2] Step[6026] Loss=0.5217 GradNorm=3.6239 StepSize=0.0362 RelImp=77.28%\n",
            "Epoch[1/2] Step[6027] Loss=0.3283 GradNorm=3.2328 StepSize=0.0323 RelImp=85.71%\n",
            "Epoch[1/2] Step[6028] Loss=0.3959 GradNorm=3.5300 StepSize=0.0353 RelImp=82.76%\n",
            "Epoch[1/2] Step[6029] Loss=0.1345 GradNorm=2.3615 StepSize=0.0236 RelImp=94.14%\n",
            "Epoch[1/2] Step[6030] Loss=0.6395 GradNorm=4.6628 StepSize=0.0466 RelImp=72.15%\n",
            "Epoch[1/2] Step[6031] Loss=0.9493 GradNorm=5.3458 StepSize=0.0535 RelImp=58.66%\n",
            "Epoch[1/2] Step[6032] Loss=0.5171 GradNorm=3.8785 StepSize=0.0388 RelImp=77.48%\n",
            "Epoch[1/2] Step[6033] Loss=0.2434 GradNorm=2.1700 StepSize=0.0217 RelImp=89.40%\n",
            "Epoch[1/2] Step[6034] Loss=0.4934 GradNorm=4.0500 StepSize=0.0405 RelImp=78.52%\n",
            "Epoch[1/2] Step[6035] Loss=0.2873 GradNorm=3.6641 StepSize=0.0366 RelImp=87.49%\n",
            "Epoch[1/2] Step[6036] Loss=0.2093 GradNorm=2.3914 StepSize=0.0239 RelImp=90.88%\n",
            "Epoch[1/2] Step[6037] Loss=0.4853 GradNorm=4.0132 StepSize=0.0401 RelImp=78.87%\n",
            "Epoch[1/2] Step[6038] Loss=1.2088 GradNorm=4.6609 StepSize=0.0466 RelImp=47.37%\n",
            "Epoch[1/2] Step[6039] Loss=0.3238 GradNorm=3.7910 StepSize=0.0379 RelImp=85.90%\n",
            "Epoch[1/2] Step[6040] Loss=0.2158 GradNorm=2.8231 StepSize=0.0282 RelImp=90.60%\n",
            "Epoch[1/2] Step[6041] Loss=0.3468 GradNorm=3.4068 StepSize=0.0341 RelImp=84.90%\n",
            "Epoch[1/2] Step[6042] Loss=0.3314 GradNorm=3.9592 StepSize=0.0396 RelImp=85.57%\n",
            "Epoch[1/2] Step[6043] Loss=0.2577 GradNorm=3.5743 StepSize=0.0357 RelImp=88.78%\n",
            "Epoch[1/2] Step[6044] Loss=0.0878 GradNorm=1.3403 StepSize=0.0134 RelImp=96.18%\n",
            "Epoch[1/2] Step[6045] Loss=0.2652 GradNorm=2.5671 StepSize=0.0257 RelImp=88.45%\n",
            "Epoch[1/2] Step[6046] Loss=0.1705 GradNorm=2.4456 StepSize=0.0245 RelImp=92.58%\n",
            "Epoch[1/2] Step[6047] Loss=0.2576 GradNorm=2.7142 StepSize=0.0271 RelImp=88.78%\n",
            "Epoch[1/2] Step[6048] Loss=0.2453 GradNorm=2.8816 StepSize=0.0288 RelImp=89.32%\n",
            "Epoch[1/2] Step[6049] Loss=0.4239 GradNorm=5.0435 StepSize=0.0504 RelImp=81.54%\n",
            "Epoch[1/2] Step[6050] Loss=0.1871 GradNorm=2.1470 StepSize=0.0215 RelImp=91.85%\n",
            "Epoch[1/2] Step[6051] Loss=0.5174 GradNorm=4.9345 StepSize=0.0493 RelImp=77.47%\n",
            "Epoch[1/2] Step[6052] Loss=0.3820 GradNorm=3.2939 StepSize=0.0329 RelImp=83.37%\n",
            "Epoch[1/2] Step[6053] Loss=0.5667 GradNorm=4.6956 StepSize=0.0470 RelImp=75.33%\n",
            "Epoch[1/2] Step[6054] Loss=0.2794 GradNorm=2.8901 StepSize=0.0289 RelImp=87.84%\n",
            "Epoch[1/2] Step[6055] Loss=0.4787 GradNorm=3.8597 StepSize=0.0386 RelImp=79.15%\n",
            "Epoch[1/2] Step[6056] Loss=0.3627 GradNorm=2.2054 StepSize=0.0221 RelImp=84.21%\n",
            "Epoch[1/2] Step[6057] Loss=0.5197 GradNorm=3.4605 StepSize=0.0346 RelImp=77.37%\n",
            "Epoch[1/2] Step[6058] Loss=0.0670 GradNorm=0.8582 StepSize=0.0086 RelImp=97.08%\n",
            "Epoch[1/2] Step[6059] Loss=0.1253 GradNorm=1.8721 StepSize=0.0187 RelImp=94.54%\n",
            "Epoch[1/2] Step[6060] Loss=0.1986 GradNorm=1.8126 StepSize=0.0181 RelImp=91.35%\n",
            "Epoch[1/2] Step[6061] Loss=0.5512 GradNorm=4.2738 StepSize=0.0427 RelImp=76.00%\n",
            "Epoch[1/2] Step[6062] Loss=0.3172 GradNorm=3.1045 StepSize=0.0310 RelImp=86.19%\n",
            "Epoch[1/2] Step[6063] Loss=0.5602 GradNorm=4.2666 StepSize=0.0427 RelImp=75.61%\n",
            "Epoch[1/2] Step[6064] Loss=0.2582 GradNorm=2.8215 StepSize=0.0282 RelImp=88.76%\n",
            "Epoch[1/2] Step[6065] Loss=0.3869 GradNorm=3.2796 StepSize=0.0328 RelImp=83.16%\n",
            "Epoch[1/2] Step[6066] Loss=0.1279 GradNorm=1.9557 StepSize=0.0196 RelImp=94.43%\n",
            "Epoch[1/2] Step[6067] Loss=0.9342 GradNorm=6.3536 StepSize=0.0635 RelImp=59.32%\n",
            "Epoch[1/2] Step[6068] Loss=0.1891 GradNorm=2.4806 StepSize=0.0248 RelImp=91.77%\n",
            "Epoch[1/2] Step[6069] Loss=0.4641 GradNorm=4.6259 StepSize=0.0463 RelImp=79.79%\n",
            "Epoch[1/2] Step[6070] Loss=0.4460 GradNorm=3.1284 StepSize=0.0313 RelImp=80.58%\n",
            "Epoch[1/2] Step[6071] Loss=0.3030 GradNorm=2.1390 StepSize=0.0214 RelImp=86.81%\n",
            "Epoch[1/2] Step[6072] Loss=0.5027 GradNorm=3.9668 StepSize=0.0397 RelImp=78.11%\n",
            "Epoch[1/2] Step[6073] Loss=0.2875 GradNorm=1.6566 StepSize=0.0166 RelImp=87.48%\n",
            "Epoch[1/2] Step[6074] Loss=0.1641 GradNorm=2.0692 StepSize=0.0207 RelImp=92.85%\n",
            "Epoch[1/2] Step[6075] Loss=0.1363 GradNorm=1.4757 StepSize=0.0148 RelImp=94.07%\n",
            "Epoch[1/2] Step[6076] Loss=0.1570 GradNorm=2.5686 StepSize=0.0257 RelImp=93.16%\n",
            "Epoch[1/2] Step[6077] Loss=0.3213 GradNorm=2.9348 StepSize=0.0293 RelImp=86.01%\n",
            "Epoch[1/2] Step[6078] Loss=0.3971 GradNorm=3.0987 StepSize=0.0310 RelImp=82.71%\n",
            "Epoch[1/2] Step[6079] Loss=0.0595 GradNorm=0.9010 StepSize=0.0090 RelImp=97.41%\n",
            "Epoch[1/2] Step[6080] Loss=0.2906 GradNorm=3.1426 StepSize=0.0314 RelImp=87.35%\n",
            "Epoch[1/2] Step[6081] Loss=0.1741 GradNorm=3.0107 StepSize=0.0301 RelImp=92.42%\n",
            "Epoch[1/2] Step[6082] Loss=0.1473 GradNorm=2.1819 StepSize=0.0218 RelImp=93.58%\n",
            "Epoch[1/2] Step[6083] Loss=0.2272 GradNorm=2.6849 StepSize=0.0268 RelImp=90.11%\n",
            "Epoch[1/2] Step[6084] Loss=0.4313 GradNorm=4.0230 StepSize=0.0402 RelImp=81.22%\n",
            "Epoch[1/2] Step[6085] Loss=0.3500 GradNorm=3.7547 StepSize=0.0375 RelImp=84.76%\n",
            "Epoch[1/2] Step[6086] Loss=0.4832 GradNorm=3.3887 StepSize=0.0339 RelImp=78.96%\n",
            "Epoch[1/2] Step[6087] Loss=0.4407 GradNorm=3.6367 StepSize=0.0364 RelImp=80.81%\n",
            "Epoch[1/2] Step[6088] Loss=0.1486 GradNorm=1.6732 StepSize=0.0167 RelImp=93.53%\n",
            "Epoch[1/2] Step[6089] Loss=0.4006 GradNorm=3.1836 StepSize=0.0318 RelImp=82.56%\n",
            "Epoch[1/2] Step[6090] Loss=0.2318 GradNorm=1.9016 StepSize=0.0190 RelImp=89.91%\n",
            "Epoch[1/2] Step[6091] Loss=0.4564 GradNorm=3.9869 StepSize=0.0399 RelImp=80.13%\n",
            "Epoch[1/2] Step[6092] Loss=0.2956 GradNorm=3.7911 StepSize=0.0379 RelImp=87.13%\n",
            "Epoch[1/2] Step[6093] Loss=0.2994 GradNorm=3.6091 StepSize=0.0361 RelImp=86.96%\n",
            "Epoch[1/2] Step[6094] Loss=0.3778 GradNorm=3.8599 StepSize=0.0386 RelImp=83.55%\n",
            "Epoch[1/2] Step[6095] Loss=0.1897 GradNorm=1.9914 StepSize=0.0199 RelImp=91.74%\n",
            "Epoch[1/2] Step[6096] Loss=0.4183 GradNorm=3.5499 StepSize=0.0355 RelImp=81.78%\n",
            "Epoch[1/2] Step[6097] Loss=0.1232 GradNorm=2.2526 StepSize=0.0225 RelImp=94.64%\n",
            "Epoch[1/2] Step[6098] Loss=0.4403 GradNorm=2.7842 StepSize=0.0278 RelImp=80.83%\n",
            "Epoch[1/2] Step[6099] Loss=0.6074 GradNorm=4.5506 StepSize=0.0455 RelImp=73.55%\n",
            "Epoch[1/2] Step[6100] Loss=0.5610 GradNorm=4.9419 StepSize=0.0494 RelImp=75.57%\n",
            "Epoch[1/2] Step[6101] Loss=0.2280 GradNorm=2.6300 StepSize=0.0263 RelImp=90.07%\n",
            "Epoch[1/2] Step[6102] Loss=0.1175 GradNorm=2.2868 StepSize=0.0229 RelImp=94.88%\n",
            "Epoch[1/2] Step[6103] Loss=0.5367 GradNorm=5.4417 StepSize=0.0544 RelImp=76.63%\n",
            "Epoch[1/2] Step[6104] Loss=0.2149 GradNorm=2.9998 StepSize=0.0300 RelImp=90.64%\n",
            "Epoch[1/2] Step[6105] Loss=0.7022 GradNorm=4.8993 StepSize=0.0490 RelImp=69.42%\n",
            "Epoch[1/2] Step[6106] Loss=0.1731 GradNorm=2.0863 StepSize=0.0209 RelImp=92.46%\n",
            "Epoch[1/2] Step[6107] Loss=0.1945 GradNorm=1.7988 StepSize=0.0180 RelImp=91.53%\n",
            "Epoch[1/2] Step[6108] Loss=0.2268 GradNorm=2.6079 StepSize=0.0261 RelImp=90.13%\n",
            "Epoch[1/2] Step[6109] Loss=0.7852 GradNorm=6.6201 StepSize=0.0662 RelImp=65.81%\n",
            "Epoch[1/2] Step[6110] Loss=0.3554 GradNorm=4.2349 StepSize=0.0423 RelImp=84.53%\n",
            "Epoch[1/2] Step[6111] Loss=0.2515 GradNorm=2.9650 StepSize=0.0296 RelImp=89.05%\n",
            "Epoch[1/2] Step[6112] Loss=0.8029 GradNorm=4.3725 StepSize=0.0437 RelImp=65.04%\n",
            "Epoch[1/2] Step[6113] Loss=0.6026 GradNorm=2.7352 StepSize=0.0274 RelImp=73.76%\n",
            "Epoch[1/2] Step[6114] Loss=0.5792 GradNorm=4.5360 StepSize=0.0454 RelImp=74.78%\n",
            "Epoch[1/2] Step[6115] Loss=0.2136 GradNorm=2.2195 StepSize=0.0222 RelImp=90.70%\n",
            "Epoch[1/2] Step[6116] Loss=0.1541 GradNorm=1.3373 StepSize=0.0134 RelImp=93.29%\n",
            "Epoch[1/2] Step[6117] Loss=0.2099 GradNorm=2.5240 StepSize=0.0252 RelImp=90.86%\n",
            "Epoch[1/2] Step[6118] Loss=0.1812 GradNorm=1.9878 StepSize=0.0199 RelImp=92.11%\n",
            "Epoch[1/2] Step[6119] Loss=0.1834 GradNorm=2.2715 StepSize=0.0227 RelImp=92.01%\n",
            "Epoch[1/2] Step[6120] Loss=0.6813 GradNorm=4.3667 StepSize=0.0437 RelImp=70.34%\n",
            "Epoch[1/2] Step[6121] Loss=0.1367 GradNorm=2.0922 StepSize=0.0209 RelImp=94.05%\n",
            "Epoch[1/2] Step[6122] Loss=0.3650 GradNorm=3.3377 StepSize=0.0334 RelImp=84.11%\n",
            "Epoch[1/2] Step[6123] Loss=0.3118 GradNorm=3.7176 StepSize=0.0372 RelImp=86.42%\n",
            "Epoch[1/2] Step[6124] Loss=0.4681 GradNorm=4.5306 StepSize=0.0453 RelImp=79.62%\n",
            "Epoch[1/2] Step[6125] Loss=0.2824 GradNorm=2.8164 StepSize=0.0282 RelImp=87.70%\n",
            "Epoch[1/2] Step[6126] Loss=0.4607 GradNorm=2.5380 StepSize=0.0254 RelImp=79.94%\n",
            "Epoch[1/2] Step[6127] Loss=0.3342 GradNorm=3.2866 StepSize=0.0329 RelImp=85.45%\n",
            "Epoch[1/2] Step[6128] Loss=0.5787 GradNorm=4.9504 StepSize=0.0495 RelImp=74.80%\n",
            "Epoch[1/2] Step[6129] Loss=0.3123 GradNorm=2.4277 StepSize=0.0243 RelImp=86.40%\n",
            "Epoch[1/2] Step[6130] Loss=0.5627 GradNorm=4.6904 StepSize=0.0469 RelImp=75.50%\n",
            "Epoch[1/2] Step[6131] Loss=0.2198 GradNorm=2.8162 StepSize=0.0282 RelImp=90.43%\n",
            "Epoch[1/2] Step[6132] Loss=0.1761 GradNorm=2.5664 StepSize=0.0257 RelImp=92.33%\n",
            "Epoch[1/2] Step[6133] Loss=0.2636 GradNorm=3.0135 StepSize=0.0301 RelImp=88.52%\n",
            "Epoch[1/2] Step[6134] Loss=0.3175 GradNorm=4.7905 StepSize=0.0479 RelImp=86.17%\n",
            "Epoch[1/2] Step[6135] Loss=0.3729 GradNorm=3.1221 StepSize=0.0312 RelImp=83.76%\n",
            "Epoch[1/2] Step[6136] Loss=0.5896 GradNorm=3.8739 StepSize=0.0387 RelImp=74.33%\n",
            "Epoch[1/2] Step[6137] Loss=0.1875 GradNorm=1.7828 StepSize=0.0178 RelImp=91.84%\n",
            "Epoch[1/2] Step[6138] Loss=0.2754 GradNorm=3.4569 StepSize=0.0346 RelImp=88.01%\n",
            "Epoch[1/2] Step[6139] Loss=0.1223 GradNorm=1.4634 StepSize=0.0146 RelImp=94.67%\n",
            "Epoch[1/2] Step[6140] Loss=0.0938 GradNorm=1.0198 StepSize=0.0102 RelImp=95.92%\n",
            "Epoch[1/2] Step[6141] Loss=0.4860 GradNorm=3.9411 StepSize=0.0394 RelImp=78.84%\n",
            "Epoch[1/2] Step[6142] Loss=0.1656 GradNorm=2.0271 StepSize=0.0203 RelImp=92.79%\n",
            "Epoch[1/2] Step[6143] Loss=0.2254 GradNorm=3.0517 StepSize=0.0305 RelImp=90.18%\n",
            "Epoch[1/2] Step[6144] Loss=0.0596 GradNorm=1.0681 StepSize=0.0107 RelImp=97.41%\n",
            "Epoch[1/2] Step[6145] Loss=0.2094 GradNorm=3.0626 StepSize=0.0306 RelImp=90.88%\n",
            "Epoch[1/2] Step[6146] Loss=0.3704 GradNorm=2.4878 StepSize=0.0249 RelImp=83.87%\n",
            "Epoch[1/2] Step[6147] Loss=0.4199 GradNorm=2.7324 StepSize=0.0273 RelImp=81.72%\n",
            "Epoch[1/2] Step[6148] Loss=0.1755 GradNorm=2.1242 StepSize=0.0212 RelImp=92.36%\n",
            "Epoch[1/2] Step[6149] Loss=0.2815 GradNorm=2.5967 StepSize=0.0260 RelImp=87.74%\n",
            "Epoch[1/2] Step[6150] Loss=0.0277 GradNorm=0.4662 StepSize=0.0047 RelImp=98.79%\n",
            "Epoch[1/2] Step[6151] Loss=0.5751 GradNorm=4.5218 StepSize=0.0452 RelImp=74.96%\n",
            "Epoch[1/2] Step[6152] Loss=0.1004 GradNorm=1.7295 StepSize=0.0173 RelImp=95.63%\n",
            "Epoch[1/2] Step[6153] Loss=0.0797 GradNorm=0.9033 StepSize=0.0090 RelImp=96.53%\n",
            "Epoch[1/2] Step[6154] Loss=0.1906 GradNorm=2.5114 StepSize=0.0251 RelImp=91.70%\n",
            "Epoch[1/2] Step[6155] Loss=0.2348 GradNorm=2.0514 StepSize=0.0205 RelImp=89.77%\n",
            "Epoch[1/2] Step[6156] Loss=0.6048 GradNorm=4.6345 StepSize=0.0463 RelImp=73.66%\n",
            "Epoch[1/2] Step[6157] Loss=0.2482 GradNorm=2.7904 StepSize=0.0279 RelImp=89.19%\n",
            "Epoch[1/2] Step[6158] Loss=0.2462 GradNorm=2.6467 StepSize=0.0265 RelImp=89.28%\n",
            "Epoch[1/2] Step[6159] Loss=0.1097 GradNorm=1.6777 StepSize=0.0168 RelImp=95.22%\n",
            "Epoch[1/2] Step[6160] Loss=0.1149 GradNorm=1.3802 StepSize=0.0138 RelImp=95.00%\n",
            "Epoch[1/2] Step[6161] Loss=0.3782 GradNorm=3.8567 StepSize=0.0386 RelImp=83.53%\n",
            "Epoch[1/2] Step[6162] Loss=0.2273 GradNorm=3.1354 StepSize=0.0314 RelImp=90.10%\n",
            "Epoch[1/2] Step[6163] Loss=0.3551 GradNorm=3.8039 StepSize=0.0380 RelImp=84.54%\n",
            "Epoch[1/2] Step[6164] Loss=0.7156 GradNorm=4.9651 StepSize=0.0497 RelImp=68.84%\n",
            "Epoch[1/2] Step[6165] Loss=0.1907 GradNorm=2.6092 StepSize=0.0261 RelImp=91.70%\n",
            "Epoch[1/2] Step[6166] Loss=0.2706 GradNorm=3.2981 StepSize=0.0330 RelImp=88.22%\n",
            "Epoch[1/2] Step[6167] Loss=0.6319 GradNorm=3.3162 StepSize=0.0332 RelImp=72.49%\n",
            "Epoch[1/2] Step[6168] Loss=0.7988 GradNorm=6.0381 StepSize=0.0604 RelImp=65.22%\n",
            "Epoch[1/2] Step[6169] Loss=0.1976 GradNorm=1.8564 StepSize=0.0186 RelImp=91.40%\n",
            "Epoch[1/2] Step[6170] Loss=0.7590 GradNorm=3.4000 StepSize=0.0340 RelImp=66.95%\n",
            "Epoch[1/2] Step[6171] Loss=0.5803 GradNorm=3.8948 StepSize=0.0389 RelImp=74.73%\n",
            "Epoch[1/2] Step[6172] Loss=0.4116 GradNorm=4.8877 StepSize=0.0489 RelImp=82.08%\n",
            "Epoch[1/2] Step[6173] Loss=0.6220 GradNorm=6.2854 StepSize=0.0629 RelImp=72.92%\n",
            "Epoch[1/2] Step[6174] Loss=0.3521 GradNorm=3.4411 StepSize=0.0344 RelImp=84.67%\n",
            "Epoch[1/2] Step[6175] Loss=0.0762 GradNorm=1.4268 StepSize=0.0143 RelImp=96.68%\n",
            "Epoch[1/2] Step[6176] Loss=0.3991 GradNorm=5.2082 StepSize=0.0521 RelImp=82.62%\n",
            "Epoch[1/2] Step[6177] Loss=0.7821 GradNorm=7.3408 StepSize=0.0734 RelImp=65.95%\n",
            "Epoch[1/2] Step[6178] Loss=0.6145 GradNorm=3.2401 StepSize=0.0324 RelImp=73.24%\n",
            "Epoch[1/2] Step[6179] Loss=0.2680 GradNorm=2.8694 StepSize=0.0287 RelImp=88.33%\n",
            "Epoch[1/2] Step[6180] Loss=0.4067 GradNorm=3.6334 StepSize=0.0363 RelImp=82.29%\n",
            "Epoch[1/2] Step[6181] Loss=0.8359 GradNorm=3.6614 StepSize=0.0366 RelImp=63.60%\n",
            "Epoch[1/2] Step[6182] Loss=0.3888 GradNorm=3.1546 StepSize=0.0315 RelImp=83.07%\n",
            "Epoch[1/2] Step[6183] Loss=0.0765 GradNorm=0.9308 StepSize=0.0093 RelImp=96.67%\n",
            "Epoch[1/2] Step[6184] Loss=0.2410 GradNorm=2.1889 StepSize=0.0219 RelImp=89.51%\n",
            "Epoch[1/2] Step[6185] Loss=0.2290 GradNorm=3.4019 StepSize=0.0340 RelImp=90.03%\n",
            "Epoch[1/2] Step[6186] Loss=0.6441 GradNorm=4.7236 StepSize=0.0472 RelImp=71.95%\n",
            "Epoch[1/2] Step[6187] Loss=0.6185 GradNorm=5.4650 StepSize=0.0546 RelImp=73.07%\n",
            "Epoch[1/2] Step[6188] Loss=0.3718 GradNorm=3.4378 StepSize=0.0344 RelImp=83.81%\n",
            "Epoch[1/2] Step[6189] Loss=0.4474 GradNorm=3.7528 StepSize=0.0375 RelImp=80.52%\n",
            "Epoch[1/2] Step[6190] Loss=0.0635 GradNorm=0.9411 StepSize=0.0094 RelImp=97.24%\n",
            "Epoch[1/2] Step[6191] Loss=0.3257 GradNorm=2.7320 StepSize=0.0273 RelImp=85.82%\n",
            "Epoch[1/2] Step[6192] Loss=0.1628 GradNorm=2.2633 StepSize=0.0226 RelImp=92.91%\n",
            "Epoch[1/2] Step[6193] Loss=0.4716 GradNorm=4.6762 StepSize=0.0468 RelImp=79.47%\n",
            "Epoch[1/2] Step[6194] Loss=0.7866 GradNorm=5.6507 StepSize=0.0565 RelImp=65.75%\n",
            "Epoch[1/2] Step[6195] Loss=0.1078 GradNorm=1.9750 StepSize=0.0197 RelImp=95.31%\n",
            "Epoch[1/2] Step[6196] Loss=0.4101 GradNorm=2.8124 StepSize=0.0281 RelImp=82.14%\n",
            "Epoch[1/2] Step[6197] Loss=0.2705 GradNorm=2.9602 StepSize=0.0296 RelImp=88.22%\n",
            "Epoch[1/2] Step[6198] Loss=0.4828 GradNorm=2.7480 StepSize=0.0275 RelImp=78.98%\n",
            "Epoch[1/2] Step[6199] Loss=0.3520 GradNorm=4.4681 StepSize=0.0447 RelImp=84.67%\n",
            "Epoch[1/2] Step[6200] Loss=0.6717 GradNorm=4.4486 StepSize=0.0445 RelImp=70.75%\n",
            "Epoch[1/2] Step[6201] Loss=0.3121 GradNorm=2.7677 StepSize=0.0277 RelImp=86.41%\n",
            "Epoch[1/2] Step[6202] Loss=0.5208 GradNorm=4.9791 StepSize=0.0498 RelImp=77.32%\n",
            "Epoch[1/2] Step[6203] Loss=0.2181 GradNorm=3.3774 StepSize=0.0338 RelImp=90.50%\n",
            "Epoch[1/2] Step[6204] Loss=0.1338 GradNorm=1.7866 StepSize=0.0179 RelImp=94.17%\n",
            "Epoch[1/2] Step[6205] Loss=0.1718 GradNorm=2.6823 StepSize=0.0268 RelImp=92.52%\n",
            "Epoch[1/2] Step[6206] Loss=0.9726 GradNorm=2.8640 StepSize=0.0286 RelImp=57.65%\n",
            "Epoch[1/2] Step[6207] Loss=0.1376 GradNorm=1.8368 StepSize=0.0184 RelImp=94.01%\n",
            "Epoch[1/2] Step[6208] Loss=0.1669 GradNorm=2.4939 StepSize=0.0249 RelImp=92.73%\n",
            "Epoch[1/2] Step[6209] Loss=0.2582 GradNorm=2.3949 StepSize=0.0239 RelImp=88.76%\n",
            "Epoch[1/2] Step[6210] Loss=0.6297 GradNorm=2.7665 StepSize=0.0277 RelImp=72.58%\n",
            "Epoch[1/2] Step[6211] Loss=0.1369 GradNorm=1.8933 StepSize=0.0189 RelImp=94.04%\n",
            "Epoch[1/2] Step[6212] Loss=0.6728 GradNorm=4.3141 StepSize=0.0431 RelImp=70.70%\n",
            "Epoch[1/2] Step[6213] Loss=0.3082 GradNorm=4.6127 StepSize=0.0461 RelImp=86.58%\n",
            "Epoch[1/2] Step[6214] Loss=0.1360 GradNorm=2.1845 StepSize=0.0218 RelImp=94.08%\n",
            "Epoch[1/2] Step[6215] Loss=0.1331 GradNorm=2.1179 StepSize=0.0212 RelImp=94.20%\n",
            "Epoch[1/2] Step[6216] Loss=0.1003 GradNorm=1.0194 StepSize=0.0102 RelImp=95.63%\n",
            "Epoch[1/2] Step[6217] Loss=0.1178 GradNorm=1.2150 StepSize=0.0122 RelImp=94.87%\n",
            "Epoch[1/2] Step[6218] Loss=0.9751 GradNorm=4.6586 StepSize=0.0466 RelImp=57.54%\n",
            "Epoch[1/2] Step[6219] Loss=0.0733 GradNorm=0.9995 StepSize=0.0100 RelImp=96.81%\n",
            "Epoch[1/2] Step[6220] Loss=0.2049 GradNorm=2.1667 StepSize=0.0217 RelImp=91.08%\n",
            "Epoch[1/2] Step[6221] Loss=0.1901 GradNorm=2.4584 StepSize=0.0246 RelImp=91.72%\n",
            "Epoch[1/2] Step[6222] Loss=0.2831 GradNorm=3.2616 StepSize=0.0326 RelImp=87.67%\n",
            "Epoch[1/2] Step[6223] Loss=0.2815 GradNorm=3.8267 StepSize=0.0383 RelImp=87.74%\n",
            "Epoch[1/2] Step[6224] Loss=0.2855 GradNorm=2.6639 StepSize=0.0266 RelImp=87.57%\n",
            "Epoch[1/2] Step[6225] Loss=0.2554 GradNorm=2.7943 StepSize=0.0279 RelImp=88.88%\n",
            "Epoch[1/2] Step[6226] Loss=0.1075 GradNorm=1.3833 StepSize=0.0138 RelImp=95.32%\n",
            "Epoch[1/2] Step[6227] Loss=0.3519 GradNorm=4.3678 StepSize=0.0437 RelImp=84.68%\n",
            "Epoch[1/2] Step[6228] Loss=0.3626 GradNorm=3.6949 StepSize=0.0369 RelImp=84.21%\n",
            "Epoch[1/2] Step[6229] Loss=0.2489 GradNorm=3.2277 StepSize=0.0323 RelImp=89.16%\n",
            "Epoch[1/2] Step[6230] Loss=0.5426 GradNorm=4.1577 StepSize=0.0416 RelImp=76.37%\n",
            "Epoch[1/2] Step[6231] Loss=0.5188 GradNorm=5.0283 StepSize=0.0503 RelImp=77.41%\n",
            "Epoch[1/2] Step[6232] Loss=0.6769 GradNorm=3.7731 StepSize=0.0377 RelImp=70.53%\n",
            "Epoch[1/2] Step[6233] Loss=0.1223 GradNorm=1.3146 StepSize=0.0131 RelImp=94.68%\n",
            "Epoch[1/2] Step[6234] Loss=0.0393 GradNorm=0.6036 StepSize=0.0060 RelImp=98.29%\n",
            "Epoch[1/2] Step[6235] Loss=0.0957 GradNorm=1.7583 StepSize=0.0176 RelImp=95.83%\n",
            "Epoch[1/2] Step[6236] Loss=0.1796 GradNorm=2.5254 StepSize=0.0253 RelImp=92.18%\n",
            "Epoch[1/2] Step[6237] Loss=0.1391 GradNorm=2.2412 StepSize=0.0224 RelImp=93.94%\n",
            "Epoch[1/2] Step[6238] Loss=0.1790 GradNorm=2.2317 StepSize=0.0223 RelImp=92.20%\n",
            "Epoch[1/2] Step[6239] Loss=0.4099 GradNorm=3.0927 StepSize=0.0309 RelImp=82.15%\n",
            "Epoch[1/2] Step[6240] Loss=0.4110 GradNorm=4.3344 StepSize=0.0433 RelImp=82.11%\n",
            "Epoch[1/2] Step[6241] Loss=0.3187 GradNorm=3.2018 StepSize=0.0320 RelImp=86.12%\n",
            "Epoch[1/2] Step[6242] Loss=0.4033 GradNorm=2.4719 StepSize=0.0247 RelImp=82.44%\n",
            "Epoch[1/2] Step[6243] Loss=0.4876 GradNorm=4.5574 StepSize=0.0456 RelImp=78.77%\n",
            "Epoch[1/2] Step[6244] Loss=0.2116 GradNorm=3.0191 StepSize=0.0302 RelImp=90.79%\n",
            "Epoch[1/2] Step[6245] Loss=0.0846 GradNorm=0.9474 StepSize=0.0095 RelImp=96.31%\n",
            "Epoch[1/2] Step[6246] Loss=0.2052 GradNorm=2.7092 StepSize=0.0271 RelImp=91.07%\n",
            "Epoch[1/2] Step[6247] Loss=0.0764 GradNorm=1.1492 StepSize=0.0115 RelImp=96.67%\n",
            "Epoch[1/2] Step[6248] Loss=0.2160 GradNorm=2.9825 StepSize=0.0298 RelImp=90.59%\n",
            "Epoch[1/2] Step[6249] Loss=0.5062 GradNorm=3.0378 StepSize=0.0304 RelImp=77.96%\n",
            "Epoch[1/2] Step[6250] Loss=0.3039 GradNorm=2.2026 StepSize=0.0220 RelImp=86.77%\n",
            "Epoch[1/2] Step[6251] Loss=0.2924 GradNorm=2.8562 StepSize=0.0286 RelImp=87.27%\n",
            "Epoch[1/2] Step[6252] Loss=0.1740 GradNorm=2.7251 StepSize=0.0273 RelImp=92.43%\n",
            "Epoch[1/2] Step[6253] Loss=0.2088 GradNorm=2.7200 StepSize=0.0272 RelImp=90.91%\n",
            "Epoch[1/2] Step[6254] Loss=0.2575 GradNorm=3.7684 StepSize=0.0377 RelImp=88.79%\n",
            "Epoch[1/2] Step[6255] Loss=0.4706 GradNorm=3.5981 StepSize=0.0360 RelImp=79.51%\n",
            "Epoch[1/2] Step[6256] Loss=0.4221 GradNorm=4.0333 StepSize=0.0403 RelImp=81.62%\n",
            "Epoch[1/2] Step[6257] Loss=0.4951 GradNorm=4.0067 StepSize=0.0401 RelImp=78.44%\n",
            "Epoch[1/2] Step[6258] Loss=0.3980 GradNorm=3.3270 StepSize=0.0333 RelImp=82.67%\n",
            "Epoch[1/2] Step[6259] Loss=0.5236 GradNorm=3.0773 StepSize=0.0308 RelImp=77.20%\n",
            "Epoch[1/2] Step[6260] Loss=0.5474 GradNorm=3.8577 StepSize=0.0386 RelImp=76.16%\n",
            "Epoch[1/2] Step[6261] Loss=0.1924 GradNorm=2.3371 StepSize=0.0234 RelImp=91.62%\n",
            "Epoch[1/2] Step[6262] Loss=0.2375 GradNorm=2.3299 StepSize=0.0233 RelImp=89.66%\n",
            "Epoch[1/2] Step[6263] Loss=0.2645 GradNorm=3.0819 StepSize=0.0308 RelImp=88.48%\n",
            "Epoch[1/2] Step[6264] Loss=0.1600 GradNorm=2.5590 StepSize=0.0256 RelImp=93.03%\n",
            "Epoch[1/2] Step[6265] Loss=0.2369 GradNorm=2.2626 StepSize=0.0226 RelImp=89.68%\n",
            "Epoch[1/2] Step[6266] Loss=0.9278 GradNorm=4.4733 StepSize=0.0447 RelImp=59.60%\n",
            "Epoch[1/2] Step[6267] Loss=0.1430 GradNorm=2.1275 StepSize=0.0213 RelImp=93.77%\n",
            "Epoch[1/2] Step[6268] Loss=0.5336 GradNorm=5.6648 StepSize=0.0566 RelImp=76.77%\n",
            "Epoch[1/2] Step[6269] Loss=0.9796 GradNorm=4.7944 StepSize=0.0479 RelImp=57.35%\n",
            "Epoch[1/2] Step[6270] Loss=0.1983 GradNorm=2.6072 StepSize=0.0261 RelImp=91.36%\n",
            "Epoch[1/2] Step[6271] Loss=0.2018 GradNorm=2.4303 StepSize=0.0243 RelImp=91.21%\n",
            "Epoch[1/2] Step[6272] Loss=0.3135 GradNorm=2.8463 StepSize=0.0285 RelImp=86.35%\n",
            "Epoch[1/2] Step[6273] Loss=0.0263 GradNorm=0.3493 StepSize=0.0035 RelImp=98.85%\n",
            "Epoch[1/2] Step[6274] Loss=0.1239 GradNorm=1.7386 StepSize=0.0174 RelImp=94.60%\n",
            "Epoch[1/2] Step[6275] Loss=0.4737 GradNorm=3.5623 StepSize=0.0356 RelImp=79.37%\n",
            "Epoch[1/2] Step[6276] Loss=0.1742 GradNorm=2.2743 StepSize=0.0227 RelImp=92.42%\n",
            "Epoch[1/2] Step[6277] Loss=0.1870 GradNorm=2.8781 StepSize=0.0288 RelImp=91.86%\n",
            "Epoch[1/2] Step[6278] Loss=0.3709 GradNorm=4.6224 StepSize=0.0462 RelImp=83.85%\n",
            "Epoch[1/2] Step[6279] Loss=0.4624 GradNorm=3.9072 StepSize=0.0391 RelImp=79.87%\n",
            "Epoch[1/2] Step[6280] Loss=0.2820 GradNorm=2.3561 StepSize=0.0236 RelImp=87.72%\n",
            "Epoch[1/2] Step[6281] Loss=0.5046 GradNorm=3.5881 StepSize=0.0359 RelImp=78.03%\n",
            "Epoch[1/2] Step[6282] Loss=0.6614 GradNorm=3.5139 StepSize=0.0351 RelImp=71.20%\n",
            "Epoch[1/2] Step[6283] Loss=0.3537 GradNorm=3.6146 StepSize=0.0361 RelImp=84.60%\n",
            "Epoch[1/2] Step[6284] Loss=0.2946 GradNorm=4.2531 StepSize=0.0425 RelImp=87.17%\n",
            "Epoch[1/2] Step[6285] Loss=0.0555 GradNorm=0.7513 StepSize=0.0075 RelImp=97.58%\n",
            "Epoch[1/2] Step[6286] Loss=0.3080 GradNorm=5.1863 StepSize=0.0519 RelImp=86.59%\n",
            "Epoch[1/2] Step[6287] Loss=0.2345 GradNorm=3.2141 StepSize=0.0321 RelImp=89.79%\n",
            "Epoch[1/2] Step[6288] Loss=0.5081 GradNorm=3.8140 StepSize=0.0381 RelImp=77.88%\n",
            "Epoch[1/2] Step[6289] Loss=0.7210 GradNorm=5.2496 StepSize=0.0525 RelImp=68.61%\n",
            "Epoch[1/2] Step[6290] Loss=0.2715 GradNorm=2.4919 StepSize=0.0249 RelImp=88.18%\n",
            "Epoch[1/2] Step[6291] Loss=0.3286 GradNorm=3.1110 StepSize=0.0311 RelImp=85.69%\n",
            "Epoch[1/2] Step[6292] Loss=0.1249 GradNorm=1.5403 StepSize=0.0154 RelImp=94.56%\n",
            "Epoch[1/2] Step[6293] Loss=0.0509 GradNorm=0.7715 StepSize=0.0077 RelImp=97.78%\n",
            "Epoch[1/2] Step[6294] Loss=0.5180 GradNorm=3.9726 StepSize=0.0397 RelImp=77.44%\n",
            "Epoch[1/2] Step[6295] Loss=0.2401 GradNorm=3.0882 StepSize=0.0309 RelImp=89.54%\n",
            "Epoch[1/2] Step[6296] Loss=0.1151 GradNorm=1.6080 StepSize=0.0161 RelImp=94.99%\n",
            "Epoch[1/2] Step[6297] Loss=0.3850 GradNorm=4.3426 StepSize=0.0434 RelImp=83.23%\n",
            "Epoch[1/2] Step[6298] Loss=0.4675 GradNorm=4.8914 StepSize=0.0489 RelImp=79.65%\n",
            "Epoch[1/2] Step[6299] Loss=0.1662 GradNorm=2.0083 StepSize=0.0201 RelImp=92.77%\n",
            "Epoch[1/2] Step[6300] Loss=0.6248 GradNorm=2.8228 StepSize=0.0282 RelImp=72.79%\n",
            "Epoch[1/2] Step[6301] Loss=0.6984 GradNorm=6.8774 StepSize=0.0688 RelImp=69.59%\n",
            "Epoch[1/2] Step[6302] Loss=0.7675 GradNorm=3.9322 StepSize=0.0393 RelImp=66.58%\n",
            "Epoch[1/2] Step[6303] Loss=0.4673 GradNorm=2.4874 StepSize=0.0249 RelImp=79.65%\n",
            "Epoch[1/2] Step[6304] Loss=0.1104 GradNorm=1.5971 StepSize=0.0160 RelImp=95.19%\n",
            "Epoch[1/2] Step[6305] Loss=0.2212 GradNorm=3.1082 StepSize=0.0311 RelImp=90.37%\n",
            "Epoch[1/2] Step[6306] Loss=0.2887 GradNorm=3.5634 StepSize=0.0356 RelImp=87.43%\n",
            "Epoch[1/2] Step[6307] Loss=0.1868 GradNorm=2.8343 StepSize=0.0283 RelImp=91.87%\n",
            "Epoch[1/2] Step[6308] Loss=0.5728 GradNorm=3.4171 StepSize=0.0342 RelImp=75.06%\n",
            "Epoch[1/2] Step[6309] Loss=0.4549 GradNorm=2.3799 StepSize=0.0238 RelImp=80.19%\n",
            "Epoch[1/2] Step[6310] Loss=0.3474 GradNorm=3.0051 StepSize=0.0301 RelImp=84.87%\n",
            "Epoch[1/2] Step[6311] Loss=0.4457 GradNorm=3.6342 StepSize=0.0363 RelImp=80.60%\n",
            "Epoch[1/2] Step[6312] Loss=0.3574 GradNorm=3.0599 StepSize=0.0306 RelImp=84.44%\n",
            "Epoch[1/2] Step[6313] Loss=0.5980 GradNorm=4.3099 StepSize=0.0431 RelImp=73.96%\n",
            "Epoch[1/2] Step[6314] Loss=0.2465 GradNorm=3.7529 StepSize=0.0375 RelImp=89.27%\n",
            "Epoch[1/2] Step[6315] Loss=0.4507 GradNorm=4.9613 StepSize=0.0496 RelImp=80.37%\n",
            "Epoch[1/2] Step[6316] Loss=0.2011 GradNorm=3.7214 StepSize=0.0372 RelImp=91.25%\n",
            "Epoch[1/2] Step[6317] Loss=0.1119 GradNorm=1.4421 StepSize=0.0144 RelImp=95.13%\n",
            "Epoch[1/2] Step[6318] Loss=0.3546 GradNorm=3.5623 StepSize=0.0356 RelImp=84.56%\n",
            "Epoch[1/2] Step[6319] Loss=0.2339 GradNorm=2.3419 StepSize=0.0234 RelImp=89.82%\n",
            "Epoch[1/2] Step[6320] Loss=0.2937 GradNorm=3.3797 StepSize=0.0338 RelImp=87.21%\n",
            "Epoch[1/2] Step[6321] Loss=0.4516 GradNorm=3.1227 StepSize=0.0312 RelImp=80.34%\n",
            "Epoch[1/2] Step[6322] Loss=0.1984 GradNorm=2.9428 StepSize=0.0294 RelImp=91.36%\n",
            "Epoch[1/2] Step[6323] Loss=0.2451 GradNorm=2.4286 StepSize=0.0243 RelImp=89.33%\n",
            "Epoch[1/2] Step[6324] Loss=0.1150 GradNorm=1.1711 StepSize=0.0117 RelImp=94.99%\n",
            "Epoch[1/2] Step[6325] Loss=0.2040 GradNorm=2.3283 StepSize=0.0233 RelImp=91.12%\n",
            "Epoch[1/2] Step[6326] Loss=0.6110 GradNorm=3.9927 StepSize=0.0399 RelImp=73.39%\n",
            "Epoch[1/2] Step[6327] Loss=0.1755 GradNorm=1.7674 StepSize=0.0177 RelImp=92.36%\n",
            "Epoch[1/2] Step[6328] Loss=0.7425 GradNorm=1.8686 StepSize=0.0187 RelImp=67.67%\n",
            "Epoch[1/2] Step[6329] Loss=0.3078 GradNorm=4.0946 StepSize=0.0409 RelImp=86.60%\n",
            "Epoch[1/2] Step[6330] Loss=0.0961 GradNorm=1.2683 StepSize=0.0127 RelImp=95.81%\n",
            "Epoch[1/2] Step[6331] Loss=0.6631 GradNorm=3.4207 StepSize=0.0342 RelImp=71.13%\n",
            "Epoch[1/2] Step[6332] Loss=0.2699 GradNorm=3.4428 StepSize=0.0344 RelImp=88.25%\n",
            "Epoch[1/2] Step[6333] Loss=0.2389 GradNorm=2.7565 StepSize=0.0276 RelImp=89.60%\n",
            "Epoch[1/2] Step[6334] Loss=0.4855 GradNorm=4.1192 StepSize=0.0412 RelImp=78.86%\n",
            "Epoch[1/2] Step[6335] Loss=0.4353 GradNorm=3.4898 StepSize=0.0349 RelImp=81.05%\n",
            "Epoch[1/2] Step[6336] Loss=0.3625 GradNorm=2.8981 StepSize=0.0290 RelImp=84.22%\n",
            "Epoch[1/2] Step[6337] Loss=0.1849 GradNorm=2.5572 StepSize=0.0256 RelImp=91.95%\n",
            "Epoch[1/2] Step[6338] Loss=0.1110 GradNorm=1.1386 StepSize=0.0114 RelImp=95.17%\n",
            "Epoch[1/2] Step[6339] Loss=0.7396 GradNorm=4.8145 StepSize=0.0481 RelImp=67.79%\n",
            "Epoch[1/2] Step[6340] Loss=0.6748 GradNorm=4.4762 StepSize=0.0448 RelImp=70.62%\n",
            "Epoch[1/2] Step[6341] Loss=0.3375 GradNorm=3.3981 StepSize=0.0340 RelImp=85.30%\n",
            "Epoch[1/2] Step[6342] Loss=0.2854 GradNorm=3.2572 StepSize=0.0326 RelImp=87.57%\n",
            "Epoch[1/2] Step[6343] Loss=0.2234 GradNorm=2.3801 StepSize=0.0238 RelImp=90.27%\n",
            "Epoch[1/2] Step[6344] Loss=0.6834 GradNorm=3.7410 StepSize=0.0374 RelImp=70.24%\n",
            "Epoch[1/2] Step[6345] Loss=0.1290 GradNorm=1.1250 StepSize=0.0112 RelImp=94.38%\n",
            "Epoch[1/2] Step[6346] Loss=0.0872 GradNorm=0.9903 StepSize=0.0099 RelImp=96.20%\n",
            "Epoch[1/2] Step[6347] Loss=0.2578 GradNorm=3.0504 StepSize=0.0305 RelImp=88.78%\n",
            "Epoch[1/2] Step[6348] Loss=0.2539 GradNorm=2.7089 StepSize=0.0271 RelImp=88.95%\n",
            "Epoch[1/2] Step[6349] Loss=0.3667 GradNorm=4.7611 StepSize=0.0476 RelImp=84.03%\n",
            "Epoch[1/2] Step[6350] Loss=0.2823 GradNorm=3.8426 StepSize=0.0384 RelImp=87.71%\n",
            "Epoch[1/2] Step[6351] Loss=0.1883 GradNorm=2.7342 StepSize=0.0273 RelImp=91.80%\n",
            "Epoch[1/2] Step[6352] Loss=0.6583 GradNorm=4.5987 StepSize=0.0460 RelImp=71.34%\n",
            "Epoch[1/2] Step[6353] Loss=0.2645 GradNorm=2.0807 StepSize=0.0208 RelImp=88.48%\n",
            "Epoch[1/2] Step[6354] Loss=0.0739 GradNorm=1.2496 StepSize=0.0125 RelImp=96.78%\n",
            "Epoch[1/2] Step[6355] Loss=0.2050 GradNorm=2.3935 StepSize=0.0239 RelImp=91.07%\n",
            "Epoch[1/2] Step[6356] Loss=0.1525 GradNorm=1.0372 StepSize=0.0104 RelImp=93.36%\n",
            "Epoch[1/2] Step[6357] Loss=0.4393 GradNorm=2.9580 StepSize=0.0296 RelImp=80.87%\n",
            "Epoch[1/2] Step[6358] Loss=0.2165 GradNorm=2.2177 StepSize=0.0222 RelImp=90.57%\n",
            "Epoch[1/2] Step[6359] Loss=0.1824 GradNorm=1.8909 StepSize=0.0189 RelImp=92.06%\n",
            "Epoch[1/2] Step[6360] Loss=0.1647 GradNorm=1.6755 StepSize=0.0168 RelImp=92.83%\n",
            "Epoch[1/2] Step[6361] Loss=0.0924 GradNorm=1.1283 StepSize=0.0113 RelImp=95.98%\n",
            "Epoch[1/2] Step[6362] Loss=0.1881 GradNorm=2.0664 StepSize=0.0207 RelImp=91.81%\n",
            "Epoch[1/2] Step[6363] Loss=0.0561 GradNorm=0.6150 StepSize=0.0062 RelImp=97.56%\n",
            "Epoch[1/2] Step[6364] Loss=0.1579 GradNorm=2.5805 StepSize=0.0258 RelImp=93.12%\n",
            "Epoch[1/2] Step[6365] Loss=0.3210 GradNorm=3.3546 StepSize=0.0335 RelImp=86.02%\n",
            "Epoch[1/2] Step[6366] Loss=0.1785 GradNorm=2.4709 StepSize=0.0247 RelImp=92.23%\n",
            "Epoch[1/2] Step[6367] Loss=0.6937 GradNorm=3.8685 StepSize=0.0387 RelImp=69.80%\n",
            "Epoch[1/2] Step[6368] Loss=0.7446 GradNorm=4.0871 StepSize=0.0409 RelImp=67.58%\n",
            "Epoch[1/2] Step[6369] Loss=0.4006 GradNorm=3.5837 StepSize=0.0358 RelImp=82.56%\n",
            "Epoch[1/2] Step[6370] Loss=0.0686 GradNorm=0.9655 StepSize=0.0097 RelImp=97.01%\n",
            "Epoch[1/2] Step[6371] Loss=0.2674 GradNorm=2.0714 StepSize=0.0207 RelImp=88.35%\n",
            "Epoch[1/2] Step[6372] Loss=0.2652 GradNorm=2.7628 StepSize=0.0276 RelImp=88.45%\n",
            "Epoch[1/2] Step[6373] Loss=0.3509 GradNorm=3.2570 StepSize=0.0326 RelImp=84.72%\n",
            "Epoch[1/2] Step[6374] Loss=0.4531 GradNorm=4.5033 StepSize=0.0450 RelImp=80.27%\n",
            "Epoch[1/2] Step[6375] Loss=0.5049 GradNorm=3.0066 StepSize=0.0301 RelImp=78.01%\n",
            "Epoch[1/2] Step[6376] Loss=0.0508 GradNorm=0.6810 StepSize=0.0068 RelImp=97.79%\n",
            "Epoch[1/2] Step[6377] Loss=0.6569 GradNorm=4.2013 StepSize=0.0420 RelImp=71.40%\n",
            "Epoch[1/2] Step[6378] Loss=0.5422 GradNorm=4.2640 StepSize=0.0426 RelImp=76.39%\n",
            "Epoch[1/2] Step[6379] Loss=0.1098 GradNorm=2.0592 StepSize=0.0206 RelImp=95.22%\n",
            "Epoch[1/2] Step[6380] Loss=0.1028 GradNorm=1.4394 StepSize=0.0144 RelImp=95.52%\n",
            "Epoch[1/2] Step[6381] Loss=0.2552 GradNorm=3.0873 StepSize=0.0309 RelImp=88.89%\n",
            "Epoch[1/2] Step[6382] Loss=0.4708 GradNorm=4.0596 StepSize=0.0406 RelImp=79.50%\n",
            "Epoch[1/2] Step[6383] Loss=0.2730 GradNorm=3.2773 StepSize=0.0328 RelImp=88.11%\n",
            "Epoch[1/2] Step[6384] Loss=0.2228 GradNorm=2.5582 StepSize=0.0256 RelImp=90.30%\n",
            "Epoch[1/2] Step[6385] Loss=0.8011 GradNorm=3.5202 StepSize=0.0352 RelImp=65.12%\n",
            "Epoch[1/2] Step[6386] Loss=0.4554 GradNorm=3.1804 StepSize=0.0318 RelImp=80.17%\n",
            "Epoch[1/2] Step[6387] Loss=0.5927 GradNorm=3.6505 StepSize=0.0365 RelImp=74.19%\n",
            "Epoch[1/2] Step[6388] Loss=0.1084 GradNorm=0.6896 StepSize=0.0069 RelImp=95.28%\n",
            "Epoch[1/2] Step[6389] Loss=0.2769 GradNorm=3.0504 StepSize=0.0305 RelImp=87.94%\n",
            "Epoch[1/2] Step[6390] Loss=0.0604 GradNorm=1.1795 StepSize=0.0118 RelImp=97.37%\n",
            "Epoch[1/2] Step[6391] Loss=0.2117 GradNorm=2.9899 StepSize=0.0299 RelImp=90.78%\n",
            "Epoch[1/2] Step[6392] Loss=0.1022 GradNorm=1.1126 StepSize=0.0111 RelImp=95.55%\n",
            "Epoch[1/2] Step[6393] Loss=0.0760 GradNorm=1.1041 StepSize=0.0110 RelImp=96.69%\n",
            "Epoch[1/2] Step[6394] Loss=0.0611 GradNorm=0.9628 StepSize=0.0096 RelImp=97.34%\n",
            "Epoch[1/2] Step[6395] Loss=0.2139 GradNorm=2.7945 StepSize=0.0279 RelImp=90.69%\n",
            "Epoch[1/2] Step[6396] Loss=0.7694 GradNorm=2.8665 StepSize=0.0287 RelImp=66.50%\n",
            "Epoch[1/2] Step[6397] Loss=0.0642 GradNorm=1.1919 StepSize=0.0119 RelImp=97.21%\n",
            "Epoch[1/2] Step[6398] Loss=1.2142 GradNorm=3.7513 StepSize=0.0375 RelImp=47.13%\n",
            "Epoch[1/2] Step[6399] Loss=0.2978 GradNorm=2.7876 StepSize=0.0279 RelImp=87.03%\n",
            "Epoch[1/2] Step[6400] Loss=0.4227 GradNorm=2.9714 StepSize=0.0297 RelImp=81.59%\n",
            "Epoch[1/2] Step[6401] Loss=0.3383 GradNorm=3.8242 StepSize=0.0382 RelImp=85.27%\n",
            "Epoch[1/2] Step[6402] Loss=0.2486 GradNorm=2.8165 StepSize=0.0282 RelImp=89.17%\n",
            "Epoch[1/2] Step[6403] Loss=0.2368 GradNorm=3.0638 StepSize=0.0306 RelImp=89.69%\n",
            "Epoch[1/2] Step[6404] Loss=0.0826 GradNorm=1.4413 StepSize=0.0144 RelImp=96.40%\n",
            "Epoch[1/2] Step[6405] Loss=0.3678 GradNorm=4.7752 StepSize=0.0478 RelImp=83.98%\n",
            "Epoch[1/2] Step[6406] Loss=0.2571 GradNorm=4.0129 StepSize=0.0401 RelImp=88.80%\n",
            "Epoch[1/2] Step[6407] Loss=0.2052 GradNorm=2.3244 StepSize=0.0232 RelImp=91.06%\n",
            "Epoch[1/2] Step[6408] Loss=0.2995 GradNorm=2.6437 StepSize=0.0264 RelImp=86.96%\n",
            "Epoch[1/2] Step[6409] Loss=0.0774 GradNorm=0.8624 StepSize=0.0086 RelImp=96.63%\n",
            "Epoch[1/2] Step[6410] Loss=0.7787 GradNorm=4.9579 StepSize=0.0496 RelImp=66.10%\n",
            "Epoch[1/2] Step[6411] Loss=0.6758 GradNorm=3.1302 StepSize=0.0313 RelImp=70.58%\n",
            "Epoch[1/2] Step[6412] Loss=0.1571 GradNorm=1.8395 StepSize=0.0184 RelImp=93.16%\n",
            "Epoch[1/2] Step[6413] Loss=0.0396 GradNorm=0.3689 StepSize=0.0037 RelImp=98.28%\n",
            "Epoch[1/2] Step[6414] Loss=0.9453 GradNorm=5.5187 StepSize=0.0552 RelImp=58.84%\n",
            "Epoch[1/2] Step[6415] Loss=0.1271 GradNorm=1.6162 StepSize=0.0162 RelImp=94.47%\n",
            "Epoch[1/2] Step[6416] Loss=0.6133 GradNorm=3.3654 StepSize=0.0337 RelImp=73.30%\n",
            "Epoch[1/2] Step[6417] Loss=0.2794 GradNorm=3.4006 StepSize=0.0340 RelImp=87.83%\n",
            "Epoch[1/2] Step[6418] Loss=0.2835 GradNorm=3.6364 StepSize=0.0364 RelImp=87.65%\n",
            "Epoch[1/2] Step[6419] Loss=0.2181 GradNorm=3.0829 StepSize=0.0308 RelImp=90.50%\n",
            "Epoch[1/2] Step[6420] Loss=0.1215 GradNorm=1.7298 StepSize=0.0173 RelImp=94.71%\n",
            "Epoch[1/2] Step[6421] Loss=0.4355 GradNorm=3.3067 StepSize=0.0331 RelImp=81.04%\n",
            "Epoch[1/2] Step[6422] Loss=0.1873 GradNorm=1.7654 StepSize=0.0177 RelImp=91.84%\n",
            "Epoch[1/2] Step[6423] Loss=0.2698 GradNorm=2.9398 StepSize=0.0294 RelImp=88.25%\n",
            "Epoch[1/2] Step[6424] Loss=0.4692 GradNorm=5.1070 StepSize=0.0511 RelImp=79.57%\n",
            "Epoch[1/2] Step[6425] Loss=0.6989 GradNorm=3.2589 StepSize=0.0326 RelImp=69.57%\n",
            "Epoch[1/2] Step[6426] Loss=0.3738 GradNorm=3.8393 StepSize=0.0384 RelImp=83.72%\n",
            "Epoch[1/2] Step[6427] Loss=0.5662 GradNorm=4.1054 StepSize=0.0411 RelImp=75.35%\n",
            "Epoch[1/2] Step[6428] Loss=0.0745 GradNorm=1.2825 StepSize=0.0128 RelImp=96.75%\n",
            "Epoch[1/2] Step[6429] Loss=0.1588 GradNorm=1.4042 StepSize=0.0140 RelImp=93.08%\n",
            "Epoch[1/2] Step[6430] Loss=0.0989 GradNorm=1.3435 StepSize=0.0134 RelImp=95.69%\n",
            "Epoch[1/2] Step[6431] Loss=0.1617 GradNorm=2.6401 StepSize=0.0264 RelImp=92.96%\n",
            "Epoch[1/2] Step[6432] Loss=1.2017 GradNorm=5.0383 StepSize=0.0504 RelImp=47.68%\n",
            "Epoch[1/2] Step[6433] Loss=0.1570 GradNorm=1.8643 StepSize=0.0186 RelImp=93.16%\n",
            "Epoch[1/2] Step[6434] Loss=0.6115 GradNorm=4.6341 StepSize=0.0463 RelImp=73.37%\n",
            "Epoch[1/2] Step[6435] Loss=0.0871 GradNorm=1.3113 StepSize=0.0131 RelImp=96.21%\n",
            "Epoch[1/2] Step[6436] Loss=0.1554 GradNorm=2.4144 StepSize=0.0241 RelImp=93.23%\n",
            "Epoch[1/2] Step[6437] Loss=0.1210 GradNorm=1.9595 StepSize=0.0196 RelImp=94.73%\n",
            "Epoch[1/2] Step[6438] Loss=0.6957 GradNorm=5.1783 StepSize=0.0518 RelImp=69.71%\n",
            "Epoch[1/2] Step[6439] Loss=0.1729 GradNorm=2.3711 StepSize=0.0237 RelImp=92.47%\n",
            "Epoch[1/2] Step[6440] Loss=0.1383 GradNorm=2.0284 StepSize=0.0203 RelImp=93.98%\n",
            "Epoch[1/2] Step[6441] Loss=0.1757 GradNorm=1.9044 StepSize=0.0190 RelImp=92.35%\n",
            "Epoch[1/2] Step[6442] Loss=0.1232 GradNorm=1.0572 StepSize=0.0106 RelImp=94.64%\n",
            "Epoch[1/2] Step[6443] Loss=0.1189 GradNorm=2.1351 StepSize=0.0214 RelImp=94.82%\n",
            "Epoch[1/2] Step[6444] Loss=0.3018 GradNorm=3.9541 StepSize=0.0395 RelImp=86.86%\n",
            "Epoch[1/2] Step[6445] Loss=0.2783 GradNorm=2.6062 StepSize=0.0261 RelImp=87.88%\n",
            "Epoch[1/2] Step[6446] Loss=0.2983 GradNorm=4.5141 StepSize=0.0451 RelImp=87.01%\n",
            "Epoch[1/2] Step[6447] Loss=0.1315 GradNorm=2.5402 StepSize=0.0254 RelImp=94.27%\n",
            "Epoch[1/2] Step[6448] Loss=0.1872 GradNorm=3.0028 StepSize=0.0300 RelImp=91.85%\n",
            "Epoch[1/2] Step[6449] Loss=0.3899 GradNorm=4.4949 StepSize=0.0449 RelImp=83.02%\n",
            "Epoch[1/2] Step[6450] Loss=0.2345 GradNorm=2.3763 StepSize=0.0238 RelImp=89.79%\n",
            "Epoch[1/2] Step[6451] Loss=0.0285 GradNorm=0.5255 StepSize=0.0053 RelImp=98.76%\n",
            "Epoch[1/2] Step[6452] Loss=0.1826 GradNorm=2.7275 StepSize=0.0273 RelImp=92.05%\n",
            "Epoch[1/2] Step[6453] Loss=0.1485 GradNorm=1.8864 StepSize=0.0189 RelImp=93.53%\n",
            "Epoch[1/2] Step[6454] Loss=0.0522 GradNorm=1.0987 StepSize=0.0110 RelImp=97.73%\n",
            "Epoch[1/2] Step[6455] Loss=0.5137 GradNorm=3.6123 StepSize=0.0361 RelImp=77.63%\n",
            "Epoch[1/2] Step[6456] Loss=0.4923 GradNorm=3.7569 StepSize=0.0376 RelImp=78.57%\n",
            "Epoch[1/2] Step[6457] Loss=0.8336 GradNorm=7.3449 StepSize=0.0734 RelImp=63.70%\n",
            "Epoch[1/2] Step[6458] Loss=0.0780 GradNorm=1.3265 StepSize=0.0133 RelImp=96.60%\n",
            "Epoch[1/2] Step[6459] Loss=0.3359 GradNorm=2.5419 StepSize=0.0254 RelImp=85.37%\n",
            "Epoch[1/2] Step[6460] Loss=0.5740 GradNorm=4.3436 StepSize=0.0434 RelImp=75.00%\n",
            "Epoch[1/2] Step[6461] Loss=0.4546 GradNorm=5.1342 StepSize=0.0513 RelImp=80.20%\n",
            "Epoch[1/2] Step[6462] Loss=0.1246 GradNorm=2.0987 StepSize=0.0210 RelImp=94.57%\n",
            "Epoch[1/2] Step[6463] Loss=0.1161 GradNorm=2.2520 StepSize=0.0225 RelImp=94.94%\n",
            "Epoch[1/2] Step[6464] Loss=0.3958 GradNorm=4.7065 StepSize=0.0471 RelImp=82.76%\n",
            "Epoch[1/2] Step[6465] Loss=0.2680 GradNorm=2.9772 StepSize=0.0298 RelImp=88.33%\n",
            "Epoch[1/2] Step[6466] Loss=0.2653 GradNorm=2.1658 StepSize=0.0217 RelImp=88.45%\n",
            "Epoch[1/2] Step[6467] Loss=0.1330 GradNorm=1.8215 StepSize=0.0182 RelImp=94.21%\n",
            "Epoch[1/2] Step[6468] Loss=0.0884 GradNorm=0.8709 StepSize=0.0087 RelImp=96.15%\n",
            "Epoch[1/2] Step[6469] Loss=0.2531 GradNorm=3.5661 StepSize=0.0357 RelImp=88.98%\n",
            "Epoch[1/2] Step[6470] Loss=0.4993 GradNorm=4.4737 StepSize=0.0447 RelImp=78.26%\n",
            "Epoch[1/2] Step[6471] Loss=0.1650 GradNorm=1.9081 StepSize=0.0191 RelImp=92.81%\n",
            "Epoch[1/2] Step[6472] Loss=0.3952 GradNorm=3.4312 StepSize=0.0343 RelImp=82.79%\n",
            "Epoch[1/2] Step[6473] Loss=0.2494 GradNorm=2.5331 StepSize=0.0253 RelImp=89.14%\n",
            "Epoch[1/2] Step[6474] Loss=0.9852 GradNorm=7.1463 StepSize=0.0715 RelImp=57.10%\n",
            "Epoch[1/2] Step[6475] Loss=0.2259 GradNorm=2.7166 StepSize=0.0272 RelImp=90.16%\n",
            "Epoch[1/2] Step[6476] Loss=0.3727 GradNorm=4.5422 StepSize=0.0454 RelImp=83.77%\n",
            "Epoch[1/2] Step[6477] Loss=0.4274 GradNorm=3.7679 StepSize=0.0377 RelImp=81.39%\n",
            "Epoch[1/2] Step[6478] Loss=0.4052 GradNorm=3.4298 StepSize=0.0343 RelImp=82.36%\n",
            "Epoch[1/2] Step[6479] Loss=0.1902 GradNorm=2.9292 StepSize=0.0293 RelImp=91.72%\n",
            "Epoch[1/2] Step[6480] Loss=0.7392 GradNorm=4.9369 StepSize=0.0494 RelImp=67.82%\n",
            "Epoch[1/2] Step[6481] Loss=0.2088 GradNorm=3.1740 StepSize=0.0317 RelImp=90.91%\n",
            "Epoch[1/2] Step[6482] Loss=0.5096 GradNorm=4.0452 StepSize=0.0405 RelImp=77.81%\n",
            "Epoch[1/2] Step[6483] Loss=0.1804 GradNorm=3.3658 StepSize=0.0337 RelImp=92.14%\n",
            "Epoch[1/2] Step[6484] Loss=0.1200 GradNorm=1.5833 StepSize=0.0158 RelImp=94.77%\n",
            "Epoch[1/2] Step[6485] Loss=0.3504 GradNorm=2.3993 StepSize=0.0240 RelImp=84.74%\n",
            "Epoch[1/2] Step[6486] Loss=0.4131 GradNorm=3.9436 StepSize=0.0394 RelImp=82.01%\n",
            "Epoch[1/2] Step[6487] Loss=0.3487 GradNorm=3.0569 StepSize=0.0306 RelImp=84.81%\n",
            "Epoch[1/2] Step[6488] Loss=0.2651 GradNorm=2.6343 StepSize=0.0263 RelImp=88.46%\n",
            "Epoch[1/2] Step[6489] Loss=0.5560 GradNorm=2.6726 StepSize=0.0267 RelImp=75.79%\n",
            "Epoch[1/2] Step[6490] Loss=0.7497 GradNorm=4.3704 StepSize=0.0437 RelImp=67.36%\n",
            "Epoch[1/2] Step[6491] Loss=0.4547 GradNorm=3.4687 StepSize=0.0347 RelImp=80.20%\n",
            "Epoch[1/2] Step[6492] Loss=0.3896 GradNorm=4.0525 StepSize=0.0405 RelImp=83.04%\n",
            "Epoch[1/2] Step[6493] Loss=0.1025 GradNorm=1.9033 StepSize=0.0190 RelImp=95.54%\n",
            "Epoch[1/2] Step[6494] Loss=0.4009 GradNorm=2.3738 StepSize=0.0237 RelImp=82.55%\n",
            "Epoch[1/2] Step[6495] Loss=0.1146 GradNorm=1.0313 StepSize=0.0103 RelImp=95.01%\n",
            "Epoch[1/2] Step[6496] Loss=0.1639 GradNorm=2.2530 StepSize=0.0225 RelImp=92.86%\n",
            "Epoch[1/2] Step[6497] Loss=0.0834 GradNorm=0.9729 StepSize=0.0097 RelImp=96.37%\n",
            "Epoch[1/2] Step[6498] Loss=0.1671 GradNorm=2.4901 StepSize=0.0249 RelImp=92.73%\n",
            "Epoch[1/2] Step[6499] Loss=0.4515 GradNorm=4.7344 StepSize=0.0473 RelImp=80.34%\n",
            "Epoch[1/2] Step[6500] Loss=0.1365 GradNorm=2.0378 StepSize=0.0204 RelImp=94.06%\n",
            "Epoch[1/2] Step[6501] Loss=0.2309 GradNorm=2.0937 StepSize=0.0209 RelImp=89.95%\n",
            "Epoch[1/2] Step[6502] Loss=0.2047 GradNorm=2.8405 StepSize=0.0284 RelImp=91.09%\n",
            "Epoch[1/2] Step[6503] Loss=0.2272 GradNorm=3.0360 StepSize=0.0304 RelImp=90.11%\n",
            "Epoch[1/2] Step[6504] Loss=0.7180 GradNorm=5.6839 StepSize=0.0568 RelImp=68.74%\n",
            "Epoch[1/2] Step[6505] Loss=0.4567 GradNorm=5.4503 StepSize=0.0545 RelImp=80.11%\n",
            "Epoch[1/2] Step[6506] Loss=0.2400 GradNorm=1.7374 StepSize=0.0174 RelImp=89.55%\n",
            "Epoch[1/2] Step[6507] Loss=0.3240 GradNorm=3.3478 StepSize=0.0335 RelImp=85.89%\n",
            "Epoch[1/2] Step[6508] Loss=0.3698 GradNorm=3.3830 StepSize=0.0338 RelImp=83.90%\n",
            "Epoch[1/2] Step[6509] Loss=0.0494 GradNorm=0.9677 StepSize=0.0097 RelImp=97.85%\n",
            "Epoch[1/2] Step[6510] Loss=0.7933 GradNorm=4.1607 StepSize=0.0416 RelImp=65.46%\n",
            "Epoch[1/2] Step[6511] Loss=0.0893 GradNorm=0.9787 StepSize=0.0098 RelImp=96.11%\n",
            "Epoch[1/2] Step[6512] Loss=0.8464 GradNorm=4.5078 StepSize=0.0451 RelImp=63.15%\n",
            "Epoch[1/2] Step[6513] Loss=0.2067 GradNorm=2.4624 StepSize=0.0246 RelImp=91.00%\n",
            "Epoch[1/2] Step[6514] Loss=0.5457 GradNorm=4.7147 StepSize=0.0471 RelImp=76.24%\n",
            "Epoch[1/2] Step[6515] Loss=0.5608 GradNorm=3.2382 StepSize=0.0324 RelImp=75.58%\n",
            "Epoch[1/2] Step[6516] Loss=0.2947 GradNorm=3.2386 StepSize=0.0324 RelImp=87.17%\n",
            "Epoch[1/2] Step[6517] Loss=0.1488 GradNorm=2.3539 StepSize=0.0235 RelImp=93.52%\n",
            "Epoch[1/2] Step[6518] Loss=0.1051 GradNorm=1.3437 StepSize=0.0134 RelImp=95.43%\n",
            "Epoch[1/2] Step[6519] Loss=0.4498 GradNorm=3.8227 StepSize=0.0382 RelImp=80.41%\n",
            "Epoch[1/2] Step[6520] Loss=0.1920 GradNorm=2.6613 StepSize=0.0266 RelImp=91.64%\n",
            "Epoch[1/2] Step[6521] Loss=0.0470 GradNorm=0.7367 StepSize=0.0074 RelImp=97.95%\n",
            "Epoch[1/2] Step[6522] Loss=0.0660 GradNorm=0.7337 StepSize=0.0073 RelImp=97.13%\n",
            "Epoch[1/2] Step[6523] Loss=0.9506 GradNorm=6.2369 StepSize=0.0624 RelImp=58.61%\n",
            "Epoch[1/2] Step[6524] Loss=0.2003 GradNorm=2.7173 StepSize=0.0272 RelImp=91.28%\n",
            "Epoch[1/2] Step[6525] Loss=0.2611 GradNorm=3.4729 StepSize=0.0347 RelImp=88.63%\n",
            "Epoch[1/2] Step[6526] Loss=0.2660 GradNorm=3.4006 StepSize=0.0340 RelImp=88.42%\n",
            "Epoch[1/2] Step[6527] Loss=0.1380 GradNorm=2.9683 StepSize=0.0297 RelImp=93.99%\n",
            "Epoch[1/2] Step[6528] Loss=0.3437 GradNorm=3.6497 StepSize=0.0365 RelImp=85.04%\n",
            "Epoch[1/2] Step[6529] Loss=0.1947 GradNorm=3.2996 StepSize=0.0330 RelImp=91.52%\n",
            "Epoch[1/2] Step[6530] Loss=0.0916 GradNorm=1.3162 StepSize=0.0132 RelImp=96.01%\n",
            "Epoch[1/2] Step[6531] Loss=0.0919 GradNorm=1.3557 StepSize=0.0136 RelImp=96.00%\n",
            "Epoch[1/2] Step[6532] Loss=0.2086 GradNorm=3.1533 StepSize=0.0315 RelImp=90.92%\n",
            "Epoch[1/2] Step[6533] Loss=0.4473 GradNorm=4.0581 StepSize=0.0406 RelImp=80.52%\n",
            "Epoch[1/2] Step[6534] Loss=0.7858 GradNorm=3.5872 StepSize=0.0359 RelImp=65.78%\n",
            "Epoch[1/2] Step[6535] Loss=0.2017 GradNorm=2.2982 StepSize=0.0230 RelImp=91.22%\n",
            "Epoch[1/2] Step[6536] Loss=0.6755 GradNorm=5.4777 StepSize=0.0548 RelImp=70.59%\n",
            "Epoch[1/2] Step[6537] Loss=0.3101 GradNorm=2.0994 StepSize=0.0210 RelImp=86.50%\n",
            "Epoch[1/2] Step[6538] Loss=0.3590 GradNorm=3.9049 StepSize=0.0390 RelImp=84.37%\n",
            "Epoch[1/2] Step[6539] Loss=0.2697 GradNorm=2.3335 StepSize=0.0233 RelImp=88.25%\n",
            "Epoch[1/2] Step[6540] Loss=0.6344 GradNorm=4.5770 StepSize=0.0458 RelImp=72.38%\n",
            "Epoch[1/2] Step[6541] Loss=0.3169 GradNorm=2.8210 StepSize=0.0282 RelImp=86.20%\n",
            "Epoch[1/2] Step[6542] Loss=0.3674 GradNorm=4.5136 StepSize=0.0451 RelImp=84.00%\n",
            "Epoch[1/2] Step[6543] Loss=0.6085 GradNorm=3.8205 StepSize=0.0382 RelImp=73.50%\n",
            "Epoch[1/2] Step[6544] Loss=0.2424 GradNorm=2.7561 StepSize=0.0276 RelImp=89.44%\n",
            "Epoch[1/2] Step[6545] Loss=0.5556 GradNorm=4.6627 StepSize=0.0466 RelImp=75.81%\n",
            "Epoch[1/2] Step[6546] Loss=0.9237 GradNorm=6.3280 StepSize=0.0633 RelImp=59.78%\n",
            "Epoch[1/2] Step[6547] Loss=0.2915 GradNorm=3.6947 StepSize=0.0369 RelImp=87.31%\n",
            "Epoch[1/2] Step[6548] Loss=0.0974 GradNorm=1.7435 StepSize=0.0174 RelImp=95.76%\n",
            "Epoch[1/2] Step[6549] Loss=0.1621 GradNorm=2.2644 StepSize=0.0226 RelImp=92.94%\n",
            "Epoch[1/2] Step[6550] Loss=0.2936 GradNorm=2.3947 StepSize=0.0239 RelImp=87.22%\n",
            "Epoch[1/2] Step[6551] Loss=0.4336 GradNorm=4.3745 StepSize=0.0437 RelImp=81.12%\n",
            "Epoch[1/2] Step[6552] Loss=0.1324 GradNorm=2.4085 StepSize=0.0241 RelImp=94.24%\n",
            "Epoch[1/2] Step[6553] Loss=0.1908 GradNorm=1.6264 StepSize=0.0163 RelImp=91.69%\n",
            "Epoch[1/2] Step[6554] Loss=0.0749 GradNorm=1.1991 StepSize=0.0120 RelImp=96.74%\n",
            "Epoch[1/2] Step[6555] Loss=0.2719 GradNorm=2.4579 StepSize=0.0246 RelImp=88.16%\n",
            "Epoch[1/2] Step[6556] Loss=0.3732 GradNorm=4.1780 StepSize=0.0418 RelImp=83.75%\n",
            "Epoch[1/2] Step[6557] Loss=0.3139 GradNorm=4.1140 StepSize=0.0411 RelImp=86.33%\n",
            "Epoch[1/2] Step[6558] Loss=0.2702 GradNorm=2.7314 StepSize=0.0273 RelImp=88.24%\n",
            "Epoch[1/2] Step[6559] Loss=0.2640 GradNorm=2.9226 StepSize=0.0292 RelImp=88.51%\n",
            "Epoch[1/2] Step[6560] Loss=0.3737 GradNorm=4.4878 StepSize=0.0449 RelImp=83.73%\n",
            "Epoch[1/2] Step[6561] Loss=0.2141 GradNorm=3.0206 StepSize=0.0302 RelImp=90.68%\n",
            "Epoch[1/2] Step[6562] Loss=0.2055 GradNorm=2.4591 StepSize=0.0246 RelImp=91.05%\n",
            "Epoch[1/2] Step[6563] Loss=0.2988 GradNorm=3.1463 StepSize=0.0315 RelImp=86.99%\n",
            "Epoch[1/2] Step[6564] Loss=0.2049 GradNorm=1.7715 StepSize=0.0177 RelImp=91.08%\n",
            "Epoch[1/2] Step[6565] Loss=0.1777 GradNorm=1.6944 StepSize=0.0169 RelImp=92.26%\n",
            "Epoch[1/2] Step[6566] Loss=0.5034 GradNorm=4.0155 StepSize=0.0402 RelImp=78.08%\n",
            "Epoch[1/2] Step[6567] Loss=0.6665 GradNorm=4.1656 StepSize=0.0417 RelImp=70.98%\n",
            "Epoch[1/2] Step[6568] Loss=0.2584 GradNorm=2.6008 StepSize=0.0260 RelImp=88.75%\n",
            "Epoch[1/2] Step[6569] Loss=0.1998 GradNorm=2.2803 StepSize=0.0228 RelImp=91.30%\n",
            "Epoch[1/2] Step[6570] Loss=0.1602 GradNorm=2.3103 StepSize=0.0231 RelImp=93.02%\n",
            "Epoch[1/2] Step[6571] Loss=0.1913 GradNorm=2.2353 StepSize=0.0224 RelImp=91.67%\n",
            "Epoch[1/2] Step[6572] Loss=0.1862 GradNorm=1.8198 StepSize=0.0182 RelImp=91.89%\n",
            "Epoch[1/2] Step[6573] Loss=0.3526 GradNorm=3.7867 StepSize=0.0379 RelImp=84.65%\n",
            "Epoch[1/2] Step[6574] Loss=0.1336 GradNorm=1.5382 StepSize=0.0154 RelImp=94.18%\n",
            "Epoch[1/2] Step[6575] Loss=0.1543 GradNorm=1.7468 StepSize=0.0175 RelImp=93.28%\n",
            "Epoch[1/2] Step[6576] Loss=0.2047 GradNorm=2.2593 StepSize=0.0226 RelImp=91.09%\n",
            "Epoch[1/2] Step[6577] Loss=0.4508 GradNorm=2.4840 StepSize=0.0248 RelImp=80.37%\n",
            "Epoch[1/2] Step[6578] Loss=0.4021 GradNorm=4.1962 StepSize=0.0420 RelImp=82.49%\n",
            "Epoch[1/2] Step[6579] Loss=0.3486 GradNorm=2.9635 StepSize=0.0296 RelImp=84.82%\n",
            "Epoch[1/2] Step[6580] Loss=0.0820 GradNorm=1.1793 StepSize=0.0118 RelImp=96.43%\n",
            "Epoch[1/2] Step[6581] Loss=0.1733 GradNorm=2.4090 StepSize=0.0241 RelImp=92.45%\n",
            "Epoch[1/2] Step[6582] Loss=0.6423 GradNorm=4.2779 StepSize=0.0428 RelImp=72.03%\n",
            "Epoch[1/2] Step[6583] Loss=0.3804 GradNorm=2.0156 StepSize=0.0202 RelImp=83.44%\n",
            "Epoch[1/2] Step[6584] Loss=0.2923 GradNorm=3.1450 StepSize=0.0315 RelImp=87.27%\n",
            "Epoch[1/2] Step[6585] Loss=0.2741 GradNorm=3.1459 StepSize=0.0315 RelImp=88.06%\n",
            "Epoch[1/2] Step[6586] Loss=0.2264 GradNorm=2.8431 StepSize=0.0284 RelImp=90.14%\n",
            "Epoch[1/2] Step[6587] Loss=0.3074 GradNorm=2.8420 StepSize=0.0284 RelImp=86.61%\n",
            "Epoch[1/2] Step[6588] Loss=0.3603 GradNorm=3.3415 StepSize=0.0334 RelImp=84.31%\n",
            "Epoch[1/2] Step[6589] Loss=0.4993 GradNorm=2.6143 StepSize=0.0261 RelImp=78.26%\n",
            "Epoch[1/2] Step[6590] Loss=0.2261 GradNorm=2.1219 StepSize=0.0212 RelImp=90.16%\n",
            "Epoch[1/2] Step[6591] Loss=0.0945 GradNorm=1.4229 StepSize=0.0142 RelImp=95.89%\n",
            "Epoch[1/2] Step[6592] Loss=0.0865 GradNorm=1.3470 StepSize=0.0135 RelImp=96.23%\n",
            "Epoch[1/2] Step[6593] Loss=0.6048 GradNorm=4.4274 StepSize=0.0443 RelImp=73.66%\n",
            "Epoch[1/2] Step[6594] Loss=0.6134 GradNorm=4.5678 StepSize=0.0457 RelImp=73.29%\n",
            "Epoch[1/2] Step[6595] Loss=0.3509 GradNorm=2.8495 StepSize=0.0285 RelImp=84.72%\n",
            "Epoch[1/2] Step[6596] Loss=0.0981 GradNorm=1.0586 StepSize=0.0106 RelImp=95.73%\n",
            "Epoch[1/2] Step[6597] Loss=0.3006 GradNorm=3.7269 StepSize=0.0373 RelImp=86.91%\n",
            "Epoch[1/2] Step[6598] Loss=0.4707 GradNorm=4.8367 StepSize=0.0484 RelImp=79.50%\n",
            "Epoch[1/2] Step[6599] Loss=0.3951 GradNorm=3.5287 StepSize=0.0353 RelImp=82.80%\n",
            "Epoch[1/2] Step[6600] Loss=0.4391 GradNorm=5.8054 StepSize=0.0581 RelImp=80.88%\n",
            "Epoch[1/2] Step[6601] Loss=0.1432 GradNorm=2.4211 StepSize=0.0242 RelImp=93.77%\n",
            "Epoch[1/2] Step[6602] Loss=0.0480 GradNorm=0.7122 StepSize=0.0071 RelImp=97.91%\n",
            "Epoch[1/2] Step[6603] Loss=0.1328 GradNorm=1.5546 StepSize=0.0155 RelImp=94.22%\n",
            "Epoch[1/2] Step[6604] Loss=0.1483 GradNorm=2.2462 StepSize=0.0225 RelImp=93.54%\n",
            "Epoch[1/2] Step[6605] Loss=0.3524 GradNorm=2.2871 StepSize=0.0229 RelImp=84.65%\n",
            "Epoch[1/2] Step[6606] Loss=0.2776 GradNorm=4.5914 StepSize=0.0459 RelImp=87.91%\n",
            "Epoch[1/2] Step[6607] Loss=1.0072 GradNorm=4.8718 StepSize=0.0487 RelImp=56.14%\n",
            "Epoch[1/2] Step[6608] Loss=0.1254 GradNorm=1.3731 StepSize=0.0137 RelImp=94.54%\n",
            "Epoch[1/2] Step[6609] Loss=0.3187 GradNorm=2.4687 StepSize=0.0247 RelImp=86.12%\n",
            "Epoch[1/2] Step[6610] Loss=0.4883 GradNorm=6.1459 StepSize=0.0615 RelImp=78.74%\n",
            "Epoch[1/2] Step[6611] Loss=0.5889 GradNorm=6.6902 StepSize=0.0669 RelImp=74.36%\n",
            "Epoch[1/2] Step[6612] Loss=0.1483 GradNorm=1.1644 StepSize=0.0116 RelImp=93.54%\n",
            "Epoch[1/2] Step[6613] Loss=0.3170 GradNorm=2.3754 StepSize=0.0238 RelImp=86.20%\n",
            "Epoch[1/2] Step[6614] Loss=0.3409 GradNorm=3.1555 StepSize=0.0316 RelImp=85.16%\n",
            "Epoch[1/2] Step[6615] Loss=0.3588 GradNorm=2.3574 StepSize=0.0236 RelImp=84.38%\n",
            "Epoch[1/2] Step[6616] Loss=0.3862 GradNorm=3.0459 StepSize=0.0305 RelImp=83.18%\n",
            "Epoch[1/2] Step[6617] Loss=0.2910 GradNorm=2.6337 StepSize=0.0263 RelImp=87.33%\n",
            "Epoch[1/2] Step[6618] Loss=0.1787 GradNorm=2.6288 StepSize=0.0263 RelImp=92.22%\n",
            "Epoch[1/2] Step[6619] Loss=0.3310 GradNorm=3.2842 StepSize=0.0328 RelImp=85.59%\n",
            "Epoch[1/2] Step[6620] Loss=0.2881 GradNorm=2.9763 StepSize=0.0298 RelImp=87.45%\n",
            "Epoch[1/2] Step[6621] Loss=0.5225 GradNorm=3.4586 StepSize=0.0346 RelImp=77.25%\n",
            "Epoch[1/2] Step[6622] Loss=0.4665 GradNorm=5.6214 StepSize=0.0562 RelImp=79.69%\n",
            "Epoch[1/2] Step[6623] Loss=0.1684 GradNorm=1.5861 StepSize=0.0159 RelImp=92.67%\n",
            "Epoch[1/2] Step[6624] Loss=0.0527 GradNorm=0.7078 StepSize=0.0071 RelImp=97.70%\n",
            "Epoch[1/2] Step[6625] Loss=0.1604 GradNorm=2.1643 StepSize=0.0216 RelImp=93.02%\n",
            "Epoch[1/2] Step[6626] Loss=0.2049 GradNorm=2.1848 StepSize=0.0218 RelImp=91.08%\n",
            "Epoch[1/2] Step[6627] Loss=0.1090 GradNorm=1.1235 StepSize=0.0112 RelImp=95.25%\n",
            "Epoch[1/2] Step[6628] Loss=0.3458 GradNorm=3.3695 StepSize=0.0337 RelImp=84.94%\n",
            "Epoch[1/2] Step[6629] Loss=0.1866 GradNorm=1.7758 StepSize=0.0178 RelImp=91.87%\n",
            "Epoch[1/2] Step[6630] Loss=0.3486 GradNorm=4.1520 StepSize=0.0415 RelImp=84.82%\n",
            "Epoch[1/2] Step[6631] Loss=0.9077 GradNorm=6.2788 StepSize=0.0628 RelImp=60.48%\n",
            "Epoch[1/2] Step[6632] Loss=0.2158 GradNorm=3.0398 StepSize=0.0304 RelImp=90.60%\n",
            "Epoch[1/2] Step[6633] Loss=0.2440 GradNorm=2.1069 StepSize=0.0211 RelImp=89.38%\n",
            "Epoch[1/2] Step[6634] Loss=0.1655 GradNorm=2.0270 StepSize=0.0203 RelImp=92.79%\n",
            "Epoch[1/2] Step[6635] Loss=0.2896 GradNorm=2.7121 StepSize=0.0271 RelImp=87.39%\n",
            "Epoch[1/2] Step[6636] Loss=0.3254 GradNorm=3.0186 StepSize=0.0302 RelImp=85.83%\n",
            "Epoch[1/2] Step[6637] Loss=0.1116 GradNorm=1.0713 StepSize=0.0107 RelImp=95.14%\n",
            "Epoch[1/2] Step[6638] Loss=0.1430 GradNorm=2.2633 StepSize=0.0226 RelImp=93.77%\n",
            "Epoch[1/2] Step[6639] Loss=0.1514 GradNorm=2.1772 StepSize=0.0218 RelImp=93.41%\n",
            "Epoch[1/2] Step[6640] Loss=0.3085 GradNorm=3.3744 StepSize=0.0337 RelImp=86.57%\n",
            "Epoch[1/2] Step[6641] Loss=0.0792 GradNorm=1.0666 StepSize=0.0107 RelImp=96.55%\n",
            "Epoch[1/2] Step[6642] Loss=0.2720 GradNorm=4.2707 StepSize=0.0427 RelImp=88.16%\n",
            "Epoch[1/2] Step[6643] Loss=1.0385 GradNorm=3.6757 StepSize=0.0368 RelImp=54.78%\n",
            "Epoch[1/2] Step[6644] Loss=0.4495 GradNorm=3.6692 StepSize=0.0367 RelImp=80.43%\n",
            "Epoch[1/2] Step[6645] Loss=0.2976 GradNorm=4.1508 StepSize=0.0415 RelImp=87.04%\n",
            "Epoch[1/2] Step[6646] Loss=0.3369 GradNorm=2.5656 StepSize=0.0257 RelImp=85.33%\n",
            "Epoch[1/2] Step[6647] Loss=0.1573 GradNorm=2.5045 StepSize=0.0250 RelImp=93.15%\n",
            "Epoch[1/2] Step[6648] Loss=0.4088 GradNorm=4.9915 StepSize=0.0499 RelImp=82.20%\n",
            "Epoch[1/2] Step[6649] Loss=0.3249 GradNorm=3.1865 StepSize=0.0319 RelImp=85.85%\n",
            "Epoch[1/2] Step[6650] Loss=0.1893 GradNorm=2.3522 StepSize=0.0235 RelImp=91.76%\n",
            "Epoch[1/2] Step[6651] Loss=0.3309 GradNorm=2.4859 StepSize=0.0249 RelImp=85.59%\n",
            "Epoch[1/2] Step[6652] Loss=0.2873 GradNorm=2.1643 StepSize=0.0216 RelImp=87.49%\n",
            "Epoch[1/2] Step[6653] Loss=0.2394 GradNorm=2.6159 StepSize=0.0262 RelImp=89.58%\n",
            "Epoch[1/2] Step[6654] Loss=0.3127 GradNorm=2.7003 StepSize=0.0270 RelImp=86.39%\n",
            "Epoch[1/2] Step[6655] Loss=0.6609 GradNorm=3.9047 StepSize=0.0390 RelImp=71.22%\n",
            "Epoch[1/2] Step[6656] Loss=0.7482 GradNorm=4.7561 StepSize=0.0476 RelImp=67.42%\n",
            "Epoch[1/2] Step[6657] Loss=0.2787 GradNorm=2.4050 StepSize=0.0240 RelImp=87.86%\n",
            "Epoch[1/2] Step[6658] Loss=0.1708 GradNorm=2.2841 StepSize=0.0228 RelImp=92.56%\n",
            "Epoch[1/2] Step[6659] Loss=0.1034 GradNorm=1.5612 StepSize=0.0156 RelImp=95.50%\n",
            "Epoch[1/2] Step[6660] Loss=0.6815 GradNorm=4.0106 StepSize=0.0401 RelImp=70.32%\n",
            "Epoch[1/2] Step[6661] Loss=0.4564 GradNorm=4.7743 StepSize=0.0477 RelImp=80.13%\n",
            "Epoch[1/2] Step[6662] Loss=0.1881 GradNorm=2.3559 StepSize=0.0236 RelImp=91.81%\n",
            "Epoch[1/2] Step[6663] Loss=0.2468 GradNorm=3.3136 StepSize=0.0331 RelImp=89.25%\n",
            "Epoch[1/2] Step[6664] Loss=0.0674 GradNorm=1.2386 StepSize=0.0124 RelImp=97.07%\n",
            "Epoch[1/2] Step[6665] Loss=0.1591 GradNorm=1.8835 StepSize=0.0188 RelImp=93.07%\n",
            "Epoch[1/2] Step[6666] Loss=0.1860 GradNorm=2.7703 StepSize=0.0277 RelImp=91.90%\n",
            "Epoch[1/2] Step[6667] Loss=0.4047 GradNorm=3.0251 StepSize=0.0303 RelImp=82.38%\n",
            "Epoch[1/2] Step[6668] Loss=0.3432 GradNorm=4.2658 StepSize=0.0427 RelImp=85.05%\n",
            "Epoch[1/2] Step[6669] Loss=0.1735 GradNorm=2.7761 StepSize=0.0278 RelImp=92.44%\n",
            "Epoch[1/2] Step[6670] Loss=0.0417 GradNorm=0.8745 StepSize=0.0087 RelImp=98.18%\n",
            "Epoch[1/2] Step[6671] Loss=0.0714 GradNorm=1.4164 StepSize=0.0142 RelImp=96.89%\n",
            "Epoch[1/2] Step[6672] Loss=0.6353 GradNorm=3.8393 StepSize=0.0384 RelImp=72.34%\n",
            "Epoch[1/2] Step[6673] Loss=0.2585 GradNorm=3.1969 StepSize=0.0320 RelImp=88.74%\n",
            "Epoch[1/2] Step[6674] Loss=0.2848 GradNorm=2.6871 StepSize=0.0269 RelImp=87.60%\n",
            "Epoch[1/2] Step[6675] Loss=0.2040 GradNorm=2.1605 StepSize=0.0216 RelImp=91.12%\n",
            "Epoch[1/2] Step[6676] Loss=0.3662 GradNorm=3.3880 StepSize=0.0339 RelImp=84.05%\n",
            "Epoch[1/2] Step[6677] Loss=0.2377 GradNorm=2.7328 StepSize=0.0273 RelImp=89.65%\n",
            "Epoch[1/2] Step[6678] Loss=0.3107 GradNorm=4.1173 StepSize=0.0412 RelImp=86.47%\n",
            "Epoch[1/2] Step[6679] Loss=0.1795 GradNorm=2.6861 StepSize=0.0269 RelImp=92.18%\n",
            "Epoch[1/2] Step[6680] Loss=0.5278 GradNorm=3.6107 StepSize=0.0361 RelImp=77.02%\n",
            "Epoch[1/2] Step[6681] Loss=0.1485 GradNorm=1.9213 StepSize=0.0192 RelImp=93.54%\n",
            "Epoch[1/2] Step[6682] Loss=0.1633 GradNorm=2.1919 StepSize=0.0219 RelImp=92.89%\n",
            "Epoch[1/2] Step[6683] Loss=0.0709 GradNorm=1.2081 StepSize=0.0121 RelImp=96.91%\n",
            "Epoch[1/2] Step[6684] Loss=0.1179 GradNorm=1.6440 StepSize=0.0164 RelImp=94.87%\n",
            "Epoch[1/2] Step[6685] Loss=0.3016 GradNorm=3.3253 StepSize=0.0333 RelImp=86.87%\n",
            "Epoch[1/2] Step[6686] Loss=0.3110 GradNorm=2.9504 StepSize=0.0295 RelImp=86.46%\n",
            "Epoch[1/2] Step[6687] Loss=0.1892 GradNorm=2.4738 StepSize=0.0247 RelImp=91.76%\n",
            "Epoch[1/2] Step[6688] Loss=0.2800 GradNorm=2.6335 StepSize=0.0263 RelImp=87.81%\n",
            "Epoch[1/2] Step[6689] Loss=0.2884 GradNorm=1.9663 StepSize=0.0197 RelImp=87.44%\n",
            "Epoch[1/2] Step[6690] Loss=0.2408 GradNorm=2.6676 StepSize=0.0267 RelImp=89.51%\n",
            "Epoch[1/2] Step[6691] Loss=1.1125 GradNorm=7.4010 StepSize=0.0740 RelImp=51.56%\n",
            "Epoch[1/2] Step[6692] Loss=0.6044 GradNorm=4.2298 StepSize=0.0423 RelImp=73.68%\n",
            "Epoch[1/2] Step[6693] Loss=0.6383 GradNorm=4.4707 StepSize=0.0447 RelImp=72.21%\n",
            "Epoch[1/2] Step[6694] Loss=0.1702 GradNorm=2.4037 StepSize=0.0240 RelImp=92.59%\n",
            "Epoch[1/2] Step[6695] Loss=0.3916 GradNorm=4.0344 StepSize=0.0403 RelImp=82.95%\n",
            "Epoch[1/2] Step[6696] Loss=0.3001 GradNorm=2.8531 StepSize=0.0285 RelImp=86.93%\n",
            "Epoch[1/2] Step[6697] Loss=0.0324 GradNorm=0.5041 StepSize=0.0050 RelImp=98.59%\n",
            "Epoch[1/2] Step[6698] Loss=0.4515 GradNorm=3.2587 StepSize=0.0326 RelImp=80.34%\n",
            "Epoch[1/2] Step[6699] Loss=0.3870 GradNorm=2.8649 StepSize=0.0286 RelImp=83.15%\n",
            "Epoch[1/2] Step[6700] Loss=0.3142 GradNorm=2.3372 StepSize=0.0234 RelImp=86.32%\n",
            "Epoch[1/2] Step[6701] Loss=0.4847 GradNorm=3.9276 StepSize=0.0393 RelImp=78.90%\n",
            "Epoch[1/2] Step[6702] Loss=0.1858 GradNorm=2.2577 StepSize=0.0226 RelImp=91.91%\n",
            "Epoch[1/2] Step[6703] Loss=0.1383 GradNorm=1.7203 StepSize=0.0172 RelImp=93.98%\n",
            "Epoch[1/2] Step[6704] Loss=0.3753 GradNorm=3.8481 StepSize=0.0385 RelImp=83.66%\n",
            "Epoch[1/2] Step[6705] Loss=0.3942 GradNorm=3.8233 StepSize=0.0382 RelImp=82.83%\n",
            "Epoch[1/2] Step[6706] Loss=0.3149 GradNorm=2.9457 StepSize=0.0295 RelImp=86.29%\n",
            "Epoch[1/2] Step[6707] Loss=0.3704 GradNorm=3.8352 StepSize=0.0384 RelImp=83.87%\n",
            "Epoch[1/2] Step[6708] Loss=0.3583 GradNorm=5.6334 StepSize=0.0563 RelImp=84.40%\n",
            "Epoch[1/2] Step[6709] Loss=0.2143 GradNorm=2.6012 StepSize=0.0260 RelImp=90.67%\n",
            "Epoch[1/2] Step[6710] Loss=0.4425 GradNorm=3.4784 StepSize=0.0348 RelImp=80.73%\n",
            "Epoch[1/2] Step[6711] Loss=0.2872 GradNorm=2.4329 StepSize=0.0243 RelImp=87.49%\n",
            "Epoch[1/2] Step[6712] Loss=0.2756 GradNorm=3.2899 StepSize=0.0329 RelImp=88.00%\n",
            "Epoch[1/2] Step[6713] Loss=0.0629 GradNorm=1.1813 StepSize=0.0118 RelImp=97.26%\n",
            "Epoch[1/2] Step[6714] Loss=0.3771 GradNorm=2.7201 StepSize=0.0272 RelImp=83.58%\n",
            "Epoch[1/2] Step[6715] Loss=0.4132 GradNorm=4.0748 StepSize=0.0407 RelImp=82.01%\n",
            "Epoch[1/2] Step[6716] Loss=0.5943 GradNorm=2.0826 StepSize=0.0208 RelImp=74.12%\n",
            "Epoch[1/2] Step[6717] Loss=0.3191 GradNorm=3.6133 StepSize=0.0361 RelImp=86.11%\n",
            "Epoch[1/2] Step[6718] Loss=0.2794 GradNorm=3.3829 StepSize=0.0338 RelImp=87.84%\n",
            "Epoch[1/2] Step[6719] Loss=0.2419 GradNorm=2.9997 StepSize=0.0300 RelImp=89.47%\n",
            "Epoch[1/2] Step[6720] Loss=0.4051 GradNorm=4.7016 StepSize=0.0470 RelImp=82.36%\n",
            "Epoch[1/2] Step[6721] Loss=0.9444 GradNorm=4.4182 StepSize=0.0442 RelImp=58.88%\n",
            "Epoch[1/2] Step[6722] Loss=0.1966 GradNorm=3.4413 StepSize=0.0344 RelImp=91.44%\n",
            "Epoch[1/2] Step[6723] Loss=0.1318 GradNorm=1.4914 StepSize=0.0149 RelImp=94.26%\n",
            "Epoch[1/2] Step[6724] Loss=0.4004 GradNorm=4.0802 StepSize=0.0408 RelImp=82.57%\n",
            "Epoch[1/2] Step[6725] Loss=0.4079 GradNorm=3.6386 StepSize=0.0364 RelImp=82.24%\n",
            "Epoch[1/2] Step[6726] Loss=0.2003 GradNorm=1.8131 StepSize=0.0181 RelImp=91.28%\n",
            "Epoch[1/2] Step[6727] Loss=0.1617 GradNorm=2.0992 StepSize=0.0210 RelImp=92.96%\n",
            "Epoch[1/2] Step[6728] Loss=0.2381 GradNorm=2.8399 StepSize=0.0284 RelImp=89.63%\n",
            "Epoch[1/2] Step[6729] Loss=0.1365 GradNorm=2.0185 StepSize=0.0202 RelImp=94.06%\n",
            "Epoch[1/2] Step[6730] Loss=0.3924 GradNorm=5.2623 StepSize=0.0526 RelImp=82.92%\n",
            "Epoch[1/2] Step[6731] Loss=0.0947 GradNorm=1.7025 StepSize=0.0170 RelImp=95.88%\n",
            "Epoch[1/2] Step[6732] Loss=0.0658 GradNorm=1.1337 StepSize=0.0113 RelImp=97.14%\n",
            "Epoch[1/2] Step[6733] Loss=0.1553 GradNorm=2.2941 StepSize=0.0229 RelImp=93.24%\n",
            "Epoch[1/2] Step[6734] Loss=0.0848 GradNorm=1.2105 StepSize=0.0121 RelImp=96.31%\n",
            "Epoch[1/2] Step[6735] Loss=0.1220 GradNorm=1.3968 StepSize=0.0140 RelImp=94.69%\n",
            "Epoch[1/2] Step[6736] Loss=0.3392 GradNorm=3.8388 StepSize=0.0384 RelImp=85.23%\n",
            "Epoch[1/2] Step[6737] Loss=0.1492 GradNorm=1.4308 StepSize=0.0143 RelImp=93.50%\n",
            "Epoch[1/2] Step[6738] Loss=0.2272 GradNorm=4.6466 StepSize=0.0465 RelImp=90.11%\n",
            "Epoch[1/2] Step[6739] Loss=0.6130 GradNorm=4.0859 StepSize=0.0409 RelImp=73.31%\n",
            "Epoch[1/2] Step[6740] Loss=0.4637 GradNorm=3.2036 StepSize=0.0320 RelImp=79.81%\n",
            "Epoch[1/2] Step[6741] Loss=0.6461 GradNorm=3.5824 StepSize=0.0358 RelImp=71.87%\n",
            "Epoch[1/2] Step[6742] Loss=0.0402 GradNorm=0.5259 StepSize=0.0053 RelImp=98.25%\n",
            "Epoch[1/2] Step[6743] Loss=0.0981 GradNorm=1.1820 StepSize=0.0118 RelImp=95.73%\n",
            "Epoch[1/2] Step[6744] Loss=0.3972 GradNorm=4.4763 StepSize=0.0448 RelImp=82.70%\n",
            "Epoch[1/2] Step[6745] Loss=0.4623 GradNorm=3.0151 StepSize=0.0302 RelImp=79.87%\n",
            "Epoch[1/2] Step[6746] Loss=0.1800 GradNorm=2.3557 StepSize=0.0236 RelImp=92.16%\n",
            "Epoch[1/2] Step[6747] Loss=0.4447 GradNorm=4.1420 StepSize=0.0414 RelImp=80.64%\n",
            "Epoch[1/2] Step[6748] Loss=0.2915 GradNorm=3.1382 StepSize=0.0314 RelImp=87.31%\n",
            "Epoch[1/2] Step[6749] Loss=0.6242 GradNorm=4.1048 StepSize=0.0410 RelImp=72.82%\n",
            "Epoch[1/2] Step[6750] Loss=0.3660 GradNorm=3.5771 StepSize=0.0358 RelImp=84.07%\n",
            "Epoch[1/2] Step[6751] Loss=0.2022 GradNorm=2.3647 StepSize=0.0236 RelImp=91.19%\n",
            "Epoch[1/2] Step[6752] Loss=0.1081 GradNorm=1.6131 StepSize=0.0161 RelImp=95.29%\n",
            "Epoch[1/2] Step[6753] Loss=0.3156 GradNorm=3.9337 StepSize=0.0393 RelImp=86.26%\n",
            "Epoch[1/2] Step[6754] Loss=0.1943 GradNorm=3.2634 StepSize=0.0326 RelImp=91.54%\n",
            "Epoch[1/2] Step[6755] Loss=0.5747 GradNorm=4.4917 StepSize=0.0449 RelImp=74.97%\n",
            "Epoch[1/2] Step[6756] Loss=0.0735 GradNorm=1.4618 StepSize=0.0146 RelImp=96.80%\n",
            "Epoch[1/2] Step[6757] Loss=0.2177 GradNorm=2.1829 StepSize=0.0218 RelImp=90.52%\n",
            "Epoch[1/2] Step[6758] Loss=0.2585 GradNorm=2.7265 StepSize=0.0273 RelImp=88.75%\n",
            "Epoch[1/2] Step[6759] Loss=0.1262 GradNorm=1.6599 StepSize=0.0166 RelImp=94.50%\n",
            "Epoch[1/2] Step[6760] Loss=0.8211 GradNorm=5.4042 StepSize=0.0540 RelImp=64.25%\n",
            "Epoch[1/2] Step[6761] Loss=0.0652 GradNorm=0.9690 StepSize=0.0097 RelImp=97.16%\n",
            "Epoch[1/2] Step[6762] Loss=0.3039 GradNorm=3.7977 StepSize=0.0380 RelImp=86.77%\n",
            "Epoch[1/2] Step[6763] Loss=0.1356 GradNorm=2.1380 StepSize=0.0214 RelImp=94.10%\n",
            "Epoch[1/2] Step[6764] Loss=0.2582 GradNorm=3.7588 StepSize=0.0376 RelImp=88.76%\n",
            "Epoch[1/2] Step[6765] Loss=0.3282 GradNorm=2.1247 StepSize=0.0212 RelImp=85.71%\n",
            "Epoch[1/2] Step[6766] Loss=0.3336 GradNorm=3.8550 StepSize=0.0386 RelImp=85.48%\n",
            "Epoch[1/2] Step[6767] Loss=0.2538 GradNorm=2.0661 StepSize=0.0207 RelImp=88.95%\n",
            "Epoch[1/2] Step[6768] Loss=0.4256 GradNorm=4.0665 StepSize=0.0407 RelImp=81.47%\n",
            "Epoch[1/2] Step[6769] Loss=0.4901 GradNorm=2.7474 StepSize=0.0275 RelImp=78.66%\n",
            "Epoch[1/2] Step[6770] Loss=0.2527 GradNorm=3.3532 StepSize=0.0335 RelImp=89.00%\n",
            "Epoch[1/2] Step[6771] Loss=0.2074 GradNorm=2.3810 StepSize=0.0238 RelImp=90.97%\n",
            "Epoch[1/2] Step[6772] Loss=0.5532 GradNorm=4.3694 StepSize=0.0437 RelImp=75.91%\n",
            "Epoch[1/2] Step[6773] Loss=0.1014 GradNorm=1.0101 StepSize=0.0101 RelImp=95.58%\n",
            "Epoch[1/2] Step[6774] Loss=0.5761 GradNorm=4.2145 StepSize=0.0421 RelImp=74.91%\n",
            "Epoch[1/2] Step[6775] Loss=0.3590 GradNorm=2.9484 StepSize=0.0295 RelImp=84.37%\n",
            "Epoch[1/2] Step[6776] Loss=0.0732 GradNorm=1.3031 StepSize=0.0130 RelImp=96.81%\n",
            "Epoch[1/2] Step[6777] Loss=0.3078 GradNorm=3.2345 StepSize=0.0323 RelImp=86.60%\n",
            "Epoch[1/2] Step[6778] Loss=0.6887 GradNorm=4.8974 StepSize=0.0490 RelImp=70.01%\n",
            "Epoch[1/2] Step[6779] Loss=0.5901 GradNorm=3.6208 StepSize=0.0362 RelImp=74.31%\n",
            "Epoch[1/2] Step[6780] Loss=0.1620 GradNorm=2.3013 StepSize=0.0230 RelImp=92.95%\n",
            "Epoch[1/2] Step[6781] Loss=0.3868 GradNorm=3.5455 StepSize=0.0355 RelImp=83.16%\n",
            "Epoch[1/2] Step[6782] Loss=0.2579 GradNorm=3.4978 StepSize=0.0350 RelImp=88.77%\n",
            "Epoch[1/2] Step[6783] Loss=0.0625 GradNorm=0.6353 StepSize=0.0064 RelImp=97.28%\n",
            "Epoch[1/2] Step[6784] Loss=0.1310 GradNorm=1.8521 StepSize=0.0185 RelImp=94.30%\n",
            "Epoch[1/2] Step[6785] Loss=0.7993 GradNorm=5.4454 StepSize=0.0545 RelImp=65.20%\n",
            "Epoch[1/2] Step[6786] Loss=0.1522 GradNorm=1.9530 StepSize=0.0195 RelImp=93.37%\n",
            "Epoch[1/2] Step[6787] Loss=0.2467 GradNorm=3.5395 StepSize=0.0354 RelImp=89.26%\n",
            "Epoch[1/2] Step[6788] Loss=0.2623 GradNorm=2.3860 StepSize=0.0239 RelImp=88.58%\n",
            "Epoch[1/2] Step[6789] Loss=0.1353 GradNorm=1.8081 StepSize=0.0181 RelImp=94.11%\n",
            "Epoch[1/2] Step[6790] Loss=0.3027 GradNorm=3.0317 StepSize=0.0303 RelImp=86.82%\n",
            "Epoch[1/2] Step[6791] Loss=0.1551 GradNorm=2.3368 StepSize=0.0234 RelImp=93.25%\n",
            "Epoch[1/2] Step[6792] Loss=0.0244 GradNorm=0.4860 StepSize=0.0049 RelImp=98.94%\n",
            "Epoch[1/2] Step[6793] Loss=0.3129 GradNorm=2.2005 StepSize=0.0220 RelImp=86.38%\n",
            "Epoch[1/2] Step[6794] Loss=0.1024 GradNorm=1.2089 StepSize=0.0121 RelImp=95.54%\n",
            "Epoch[1/2] Step[6795] Loss=0.6646 GradNorm=3.5732 StepSize=0.0357 RelImp=71.06%\n",
            "Epoch[1/2] Step[6796] Loss=0.2008 GradNorm=2.1816 StepSize=0.0218 RelImp=91.26%\n",
            "Epoch[1/2] Step[6797] Loss=0.1732 GradNorm=2.4451 StepSize=0.0245 RelImp=92.46%\n",
            "Epoch[1/2] Step[6798] Loss=0.0750 GradNorm=1.2396 StepSize=0.0124 RelImp=96.74%\n",
            "Epoch[1/2] Step[6799] Loss=0.1738 GradNorm=2.4799 StepSize=0.0248 RelImp=92.43%\n",
            "Epoch[1/2] Step[6800] Loss=0.6794 GradNorm=4.0010 StepSize=0.0400 RelImp=70.42%\n",
            "Epoch[1/2] Step[6801] Loss=0.3563 GradNorm=2.1766 StepSize=0.0218 RelImp=84.48%\n",
            "Epoch[1/2] Step[6802] Loss=0.5349 GradNorm=3.7494 StepSize=0.0375 RelImp=76.71%\n",
            "Epoch[1/2] Step[6803] Loss=0.3931 GradNorm=2.9587 StepSize=0.0296 RelImp=82.88%\n",
            "Epoch[1/2] Step[6804] Loss=0.1235 GradNorm=2.1224 StepSize=0.0212 RelImp=94.62%\n",
            "Epoch[1/2] Step[6805] Loss=0.4620 GradNorm=3.5198 StepSize=0.0352 RelImp=79.88%\n",
            "Epoch[1/2] Step[6806] Loss=0.0469 GradNorm=0.5168 StepSize=0.0052 RelImp=97.96%\n",
            "Epoch[1/2] Step[6807] Loss=0.1934 GradNorm=2.9855 StepSize=0.0299 RelImp=91.58%\n",
            "Epoch[1/2] Step[6808] Loss=0.0591 GradNorm=0.8513 StepSize=0.0085 RelImp=97.43%\n",
            "Epoch[1/2] Step[6809] Loss=0.6134 GradNorm=6.1675 StepSize=0.0617 RelImp=73.29%\n",
            "Epoch[1/2] Step[6810] Loss=0.2179 GradNorm=3.0157 StepSize=0.0302 RelImp=90.51%\n",
            "Epoch[1/2] Step[6811] Loss=0.4364 GradNorm=3.6391 StepSize=0.0364 RelImp=81.00%\n",
            "Epoch[1/2] Step[6812] Loss=0.3123 GradNorm=3.7831 StepSize=0.0378 RelImp=86.40%\n",
            "Epoch[1/2] Step[6813] Loss=0.0898 GradNorm=1.5184 StepSize=0.0152 RelImp=96.09%\n",
            "Epoch[1/2] Step[6814] Loss=0.2588 GradNorm=2.9637 StepSize=0.0296 RelImp=88.73%\n",
            "Epoch[1/2] Step[6815] Loss=0.3241 GradNorm=3.2736 StepSize=0.0327 RelImp=85.89%\n",
            "Epoch[1/2] Step[6816] Loss=0.3528 GradNorm=1.8830 StepSize=0.0188 RelImp=84.64%\n",
            "Epoch[1/2] Step[6817] Loss=0.5245 GradNorm=4.1515 StepSize=0.0415 RelImp=77.16%\n",
            "Epoch[1/2] Step[6818] Loss=0.6153 GradNorm=4.3782 StepSize=0.0438 RelImp=73.21%\n",
            "Epoch[1/2] Step[6819] Loss=0.7883 GradNorm=3.3584 StepSize=0.0336 RelImp=65.68%\n",
            "Epoch[1/2] Step[6820] Loss=0.1632 GradNorm=2.5178 StepSize=0.0252 RelImp=92.89%\n",
            "Epoch[1/2] Step[6821] Loss=0.8448 GradNorm=5.1185 StepSize=0.0512 RelImp=63.21%\n",
            "Epoch[1/2] Step[6822] Loss=0.5075 GradNorm=3.3739 StepSize=0.0337 RelImp=77.90%\n",
            "Epoch[1/2] Step[6823] Loss=0.1064 GradNorm=1.2280 StepSize=0.0123 RelImp=95.37%\n",
            "Epoch[1/2] Step[6824] Loss=0.6587 GradNorm=4.4425 StepSize=0.0444 RelImp=71.32%\n",
            "Epoch[1/2] Step[6825] Loss=0.2317 GradNorm=3.5724 StepSize=0.0357 RelImp=89.91%\n",
            "Epoch[1/2] Step[6826] Loss=0.2332 GradNorm=2.4985 StepSize=0.0250 RelImp=89.84%\n",
            "Epoch[1/2] Step[6827] Loss=0.2771 GradNorm=3.0157 StepSize=0.0302 RelImp=87.93%\n",
            "Epoch[1/2] Step[6828] Loss=0.1386 GradNorm=1.9200 StepSize=0.0192 RelImp=93.97%\n",
            "Epoch[1/2] Step[6829] Loss=0.1981 GradNorm=2.6844 StepSize=0.0268 RelImp=91.37%\n",
            "Epoch[1/2] Step[6830] Loss=0.1396 GradNorm=1.6103 StepSize=0.0161 RelImp=93.92%\n",
            "Epoch[1/2] Step[6831] Loss=0.6985 GradNorm=3.8626 StepSize=0.0386 RelImp=69.58%\n",
            "Epoch[1/2] Step[6832] Loss=0.7533 GradNorm=5.2143 StepSize=0.0521 RelImp=67.20%\n",
            "Epoch[1/2] Step[6833] Loss=0.2774 GradNorm=2.7035 StepSize=0.0270 RelImp=87.92%\n",
            "Epoch[1/2] Step[6834] Loss=0.2304 GradNorm=3.0949 StepSize=0.0309 RelImp=89.97%\n",
            "Epoch[1/2] Step[6835] Loss=0.3056 GradNorm=3.2838 StepSize=0.0328 RelImp=86.70%\n",
            "Epoch[1/2] Step[6836] Loss=0.4250 GradNorm=3.6636 StepSize=0.0366 RelImp=81.49%\n",
            "Epoch[1/2] Step[6837] Loss=0.4013 GradNorm=4.1551 StepSize=0.0416 RelImp=82.52%\n",
            "Epoch[1/2] Step[6838] Loss=0.2259 GradNorm=3.3989 StepSize=0.0340 RelImp=90.16%\n",
            "Epoch[1/2] Step[6839] Loss=0.6005 GradNorm=4.6978 StepSize=0.0470 RelImp=73.85%\n",
            "Epoch[1/2] Step[6840] Loss=0.5702 GradNorm=3.7453 StepSize=0.0375 RelImp=75.17%\n",
            "Epoch[1/2] Step[6841] Loss=0.1354 GradNorm=2.0130 StepSize=0.0201 RelImp=94.10%\n",
            "Epoch[1/2] Step[6842] Loss=0.5635 GradNorm=4.0743 StepSize=0.0407 RelImp=75.46%\n",
            "Epoch[1/2] Step[6843] Loss=0.8476 GradNorm=3.1341 StepSize=0.0313 RelImp=63.09%\n",
            "Epoch[1/2] Step[6844] Loss=0.3850 GradNorm=3.4482 StepSize=0.0345 RelImp=83.24%\n",
            "Epoch[1/2] Step[6845] Loss=0.5340 GradNorm=2.6789 StepSize=0.0268 RelImp=76.75%\n",
            "Epoch[1/2] Step[6846] Loss=1.1215 GradNorm=7.1484 StepSize=0.0715 RelImp=51.17%\n",
            "Epoch[1/2] Step[6847] Loss=0.2932 GradNorm=3.5843 StepSize=0.0358 RelImp=87.23%\n",
            "Epoch[1/2] Step[6848] Loss=0.2925 GradNorm=3.4971 StepSize=0.0350 RelImp=87.26%\n",
            "Epoch[1/2] Step[6849] Loss=0.4849 GradNorm=3.1948 StepSize=0.0319 RelImp=78.89%\n",
            "Epoch[1/2] Step[6850] Loss=0.5892 GradNorm=3.9099 StepSize=0.0391 RelImp=74.34%\n",
            "Epoch[1/2] Step[6851] Loss=0.2033 GradNorm=2.2669 StepSize=0.0227 RelImp=91.15%\n",
            "Epoch[1/2] Step[6852] Loss=0.1108 GradNorm=1.7491 StepSize=0.0175 RelImp=95.17%\n",
            "Epoch[1/2] Step[6853] Loss=0.0584 GradNorm=1.1068 StepSize=0.0111 RelImp=97.46%\n",
            "Epoch[1/2] Step[6854] Loss=0.5083 GradNorm=4.4942 StepSize=0.0449 RelImp=77.87%\n",
            "Epoch[1/2] Step[6855] Loss=0.2837 GradNorm=2.8909 StepSize=0.0289 RelImp=87.65%\n",
            "Epoch[1/2] Step[6856] Loss=0.4660 GradNorm=4.7751 StepSize=0.0478 RelImp=79.71%\n",
            "Epoch[1/2] Step[6857] Loss=0.1328 GradNorm=1.5503 StepSize=0.0155 RelImp=94.22%\n",
            "Epoch[1/2] Step[6858] Loss=0.6792 GradNorm=5.1983 StepSize=0.0520 RelImp=70.43%\n",
            "Epoch[1/2] Step[6859] Loss=0.4293 GradNorm=2.7029 StepSize=0.0270 RelImp=81.31%\n",
            "Epoch[1/2] Step[6860] Loss=0.1347 GradNorm=1.9151 StepSize=0.0192 RelImp=94.14%\n",
            "Epoch[1/2] Step[6861] Loss=0.0455 GradNorm=0.5301 StepSize=0.0053 RelImp=98.02%\n",
            "Epoch[1/2] Step[6862] Loss=0.2006 GradNorm=2.6125 StepSize=0.0261 RelImp=91.26%\n",
            "Epoch[1/2] Step[6863] Loss=0.1669 GradNorm=2.2781 StepSize=0.0228 RelImp=92.73%\n",
            "Epoch[1/2] Step[6864] Loss=0.2490 GradNorm=2.9979 StepSize=0.0300 RelImp=89.16%\n",
            "Epoch[1/2] Step[6865] Loss=0.5755 GradNorm=3.9234 StepSize=0.0392 RelImp=74.94%\n",
            "Epoch[1/2] Step[6866] Loss=0.2283 GradNorm=2.5395 StepSize=0.0254 RelImp=90.06%\n",
            "Epoch[1/2] Step[6867] Loss=0.1579 GradNorm=1.8653 StepSize=0.0187 RelImp=93.13%\n",
            "Epoch[1/2] Step[6868] Loss=0.4545 GradNorm=4.9537 StepSize=0.0495 RelImp=80.21%\n",
            "Epoch[1/2] Step[6869] Loss=0.2027 GradNorm=2.1901 StepSize=0.0219 RelImp=91.17%\n",
            "Epoch[1/2] Step[6870] Loss=0.1080 GradNorm=2.2888 StepSize=0.0229 RelImp=95.30%\n",
            "Epoch[1/2] Step[6871] Loss=0.2992 GradNorm=2.9822 StepSize=0.0298 RelImp=86.97%\n",
            "Epoch[1/2] Step[6872] Loss=0.2138 GradNorm=3.5441 StepSize=0.0354 RelImp=90.69%\n",
            "Epoch[1/2] Step[6873] Loss=0.2090 GradNorm=2.3079 StepSize=0.0231 RelImp=90.90%\n",
            "Epoch[1/2] Step[6874] Loss=0.1078 GradNorm=1.9394 StepSize=0.0194 RelImp=95.31%\n",
            "Epoch[1/2] Step[6875] Loss=0.3020 GradNorm=4.3814 StepSize=0.0438 RelImp=86.85%\n",
            "Epoch[1/2] Step[6876] Loss=0.2474 GradNorm=3.2024 StepSize=0.0320 RelImp=89.23%\n",
            "Epoch[1/2] Step[6877] Loss=0.1492 GradNorm=1.8662 StepSize=0.0187 RelImp=93.50%\n",
            "Epoch[1/2] Step[6878] Loss=0.2764 GradNorm=1.9335 StepSize=0.0193 RelImp=87.97%\n",
            "Epoch[1/2] Step[6879] Loss=0.1007 GradNorm=1.5272 StepSize=0.0153 RelImp=95.62%\n",
            "Epoch[1/2] Step[6880] Loss=0.0794 GradNorm=1.4084 StepSize=0.0141 RelImp=96.54%\n",
            "Epoch[1/2] Step[6881] Loss=0.2196 GradNorm=3.2264 StepSize=0.0323 RelImp=90.44%\n",
            "Epoch[1/2] Step[6882] Loss=0.6198 GradNorm=2.7548 StepSize=0.0275 RelImp=73.01%\n",
            "Epoch[1/2] Step[6883] Loss=0.4359 GradNorm=3.8389 StepSize=0.0384 RelImp=81.02%\n",
            "Epoch[1/2] Step[6884] Loss=0.1720 GradNorm=2.6396 StepSize=0.0264 RelImp=92.51%\n",
            "Epoch[1/2] Step[6885] Loss=0.4057 GradNorm=3.1597 StepSize=0.0316 RelImp=82.34%\n",
            "Epoch[1/2] Step[6886] Loss=0.1062 GradNorm=1.8768 StepSize=0.0188 RelImp=95.38%\n",
            "Epoch[1/2] Step[6887] Loss=0.2134 GradNorm=1.9267 StepSize=0.0193 RelImp=90.71%\n",
            "Epoch[1/2] Step[6888] Loss=0.2410 GradNorm=2.2627 StepSize=0.0226 RelImp=89.51%\n",
            "Epoch[1/2] Step[6889] Loss=0.2382 GradNorm=2.6873 StepSize=0.0269 RelImp=89.63%\n",
            "Epoch[1/2] Step[6890] Loss=0.1859 GradNorm=2.5648 StepSize=0.0256 RelImp=91.91%\n",
            "Epoch[1/2] Step[6891] Loss=0.1669 GradNorm=2.4876 StepSize=0.0249 RelImp=92.73%\n",
            "Epoch[1/2] Step[6892] Loss=0.2780 GradNorm=3.6031 StepSize=0.0360 RelImp=87.89%\n",
            "Epoch[1/2] Step[6893] Loss=0.0802 GradNorm=1.1801 StepSize=0.0118 RelImp=96.51%\n",
            "Epoch[1/2] Step[6894] Loss=0.4471 GradNorm=3.9839 StepSize=0.0398 RelImp=80.53%\n",
            "Epoch[1/2] Step[6895] Loss=0.6486 GradNorm=3.8577 StepSize=0.0386 RelImp=71.76%\n",
            "Epoch[1/2] Step[6896] Loss=0.4192 GradNorm=3.0315 StepSize=0.0303 RelImp=81.75%\n",
            "Epoch[1/2] Step[6897] Loss=0.3037 GradNorm=3.8563 StepSize=0.0386 RelImp=86.78%\n",
            "Epoch[1/2] Step[6898] Loss=0.2654 GradNorm=2.2144 StepSize=0.0221 RelImp=88.45%\n",
            "Epoch[1/2] Step[6899] Loss=0.4672 GradNorm=4.6070 StepSize=0.0461 RelImp=79.66%\n",
            "Epoch[1/2] Step[6900] Loss=0.3766 GradNorm=2.8794 StepSize=0.0288 RelImp=83.60%\n",
            "Epoch[1/2] Step[6901] Loss=0.1812 GradNorm=1.5977 StepSize=0.0160 RelImp=92.11%\n",
            "Epoch[1/2] Step[6902] Loss=0.3736 GradNorm=3.1138 StepSize=0.0311 RelImp=83.73%\n",
            "Epoch[1/2] Step[6903] Loss=0.1267 GradNorm=1.8797 StepSize=0.0188 RelImp=94.48%\n",
            "Epoch[1/2] Step[6904] Loss=0.5953 GradNorm=4.4511 StepSize=0.0445 RelImp=74.08%\n",
            "Epoch[1/2] Step[6905] Loss=0.7754 GradNorm=4.4855 StepSize=0.0449 RelImp=66.24%\n",
            "Epoch[1/2] Step[6906] Loss=0.1303 GradNorm=1.9099 StepSize=0.0191 RelImp=94.33%\n",
            "Epoch[1/2] Step[6907] Loss=0.4905 GradNorm=4.1698 StepSize=0.0417 RelImp=78.64%\n",
            "Epoch[1/2] Step[6908] Loss=1.0617 GradNorm=7.2564 StepSize=0.0726 RelImp=53.77%\n",
            "Epoch[1/2] Step[6909] Loss=0.5634 GradNorm=4.5481 StepSize=0.0455 RelImp=75.47%\n",
            "Epoch[1/2] Step[6910] Loss=0.0648 GradNorm=1.3625 StepSize=0.0136 RelImp=97.18%\n",
            "Epoch[1/2] Step[6911] Loss=0.2579 GradNorm=2.5764 StepSize=0.0258 RelImp=88.77%\n",
            "Epoch[1/2] Step[6912] Loss=0.2567 GradNorm=3.2042 StepSize=0.0320 RelImp=88.82%\n",
            "Epoch[1/2] Step[6913] Loss=0.6008 GradNorm=3.9911 StepSize=0.0399 RelImp=73.84%\n",
            "Epoch[1/2] Step[6914] Loss=0.1454 GradNorm=2.0592 StepSize=0.0206 RelImp=93.67%\n",
            "Epoch[1/2] Step[6915] Loss=0.1917 GradNorm=2.4448 StepSize=0.0244 RelImp=91.65%\n",
            "Epoch[1/2] Step[6916] Loss=0.3516 GradNorm=2.7653 StepSize=0.0277 RelImp=84.69%\n",
            "Epoch[1/2] Step[6917] Loss=0.3298 GradNorm=3.0784 StepSize=0.0308 RelImp=85.64%\n",
            "Epoch[1/2] Step[6918] Loss=0.2453 GradNorm=3.0482 StepSize=0.0305 RelImp=89.32%\n",
            "Epoch[1/2] Step[6919] Loss=0.0932 GradNorm=1.3290 StepSize=0.0133 RelImp=95.94%\n",
            "Epoch[1/2] Step[6920] Loss=0.5264 GradNorm=6.0381 StepSize=0.0604 RelImp=77.08%\n",
            "Epoch[1/2] Step[6921] Loss=0.2107 GradNorm=3.4774 StepSize=0.0348 RelImp=90.83%\n",
            "Epoch[1/2] Step[6922] Loss=0.3771 GradNorm=2.7055 StepSize=0.0271 RelImp=83.58%\n",
            "Epoch[1/2] Step[6923] Loss=0.2965 GradNorm=2.7492 StepSize=0.0275 RelImp=87.09%\n",
            "Epoch[1/2] Step[6924] Loss=0.6811 GradNorm=5.4621 StepSize=0.0546 RelImp=70.34%\n",
            "Epoch[1/2] Step[6925] Loss=0.1378 GradNorm=1.6806 StepSize=0.0168 RelImp=94.00%\n",
            "Epoch[1/2] Step[6926] Loss=0.5982 GradNorm=4.9421 StepSize=0.0494 RelImp=73.95%\n",
            "Epoch[1/2] Step[6927] Loss=0.5853 GradNorm=4.0120 StepSize=0.0401 RelImp=74.51%\n",
            "Epoch[1/2] Step[6928] Loss=0.2781 GradNorm=3.0133 StepSize=0.0301 RelImp=87.89%\n",
            "Epoch[1/2] Step[6929] Loss=0.3818 GradNorm=3.4089 StepSize=0.0341 RelImp=83.38%\n",
            "Epoch[1/2] Step[6930] Loss=0.1116 GradNorm=1.1593 StepSize=0.0116 RelImp=95.14%\n",
            "Epoch[1/2] Step[6931] Loss=0.2297 GradNorm=2.1941 StepSize=0.0219 RelImp=90.00%\n",
            "Epoch[1/2] Step[6932] Loss=0.2543 GradNorm=2.4990 StepSize=0.0250 RelImp=88.93%\n",
            "Epoch[1/2] Step[6933] Loss=0.2582 GradNorm=2.5714 StepSize=0.0257 RelImp=88.76%\n",
            "Epoch[1/2] Step[6934] Loss=0.3936 GradNorm=4.1003 StepSize=0.0410 RelImp=82.86%\n",
            "Epoch[1/2] Step[6935] Loss=0.4299 GradNorm=4.1658 StepSize=0.0417 RelImp=81.28%\n",
            "Epoch[1/2] Step[6936] Loss=0.2722 GradNorm=3.4204 StepSize=0.0342 RelImp=88.15%\n",
            "Epoch[1/2] Step[6937] Loss=0.1695 GradNorm=2.0076 StepSize=0.0201 RelImp=92.62%\n",
            "Epoch[1/2] Step[6938] Loss=0.2984 GradNorm=4.0617 StepSize=0.0406 RelImp=87.01%\n",
            "Epoch[1/2] Step[6939] Loss=0.7252 GradNorm=5.1821 StepSize=0.0518 RelImp=68.42%\n",
            "Epoch[1/2] Step[6940] Loss=0.6407 GradNorm=2.0690 StepSize=0.0207 RelImp=72.10%\n",
            "Epoch[1/2] Step[6941] Loss=0.1918 GradNorm=2.3272 StepSize=0.0233 RelImp=91.65%\n",
            "Epoch[1/2] Step[6942] Loss=0.2441 GradNorm=3.0252 StepSize=0.0303 RelImp=89.37%\n",
            "Epoch[1/2] Step[6943] Loss=0.0840 GradNorm=1.4449 StepSize=0.0144 RelImp=96.34%\n",
            "Epoch[1/2] Step[6944] Loss=0.1490 GradNorm=1.8365 StepSize=0.0184 RelImp=93.51%\n",
            "Epoch[1/2] Step[6945] Loss=0.3522 GradNorm=3.8005 StepSize=0.0380 RelImp=84.66%\n",
            "Epoch[1/2] Step[6946] Loss=0.2352 GradNorm=1.7370 StepSize=0.0174 RelImp=89.76%\n",
            "Epoch[1/2] Step[6947] Loss=0.2617 GradNorm=3.7793 StepSize=0.0378 RelImp=88.61%\n",
            "Epoch[1/2] Step[6948] Loss=0.3127 GradNorm=3.6508 StepSize=0.0365 RelImp=86.38%\n",
            "Epoch[1/2] Step[6949] Loss=0.0552 GradNorm=1.1446 StepSize=0.0114 RelImp=97.59%\n",
            "Epoch[1/2] Step[6950] Loss=0.1113 GradNorm=1.4779 StepSize=0.0148 RelImp=95.15%\n",
            "Epoch[1/2] Step[6951] Loss=0.3538 GradNorm=4.0970 StepSize=0.0410 RelImp=84.60%\n",
            "Epoch[1/2] Step[6952] Loss=0.1651 GradNorm=2.2797 StepSize=0.0228 RelImp=92.81%\n",
            "Epoch[1/2] Step[6953] Loss=0.4820 GradNorm=4.0744 StepSize=0.0407 RelImp=79.01%\n",
            "Epoch[1/2] Step[6954] Loss=0.4551 GradNorm=4.0955 StepSize=0.0410 RelImp=80.18%\n",
            "Epoch[1/2] Step[6955] Loss=0.1092 GradNorm=1.6339 StepSize=0.0163 RelImp=95.24%\n",
            "Epoch[1/2] Step[6956] Loss=0.8438 GradNorm=5.7278 StepSize=0.0573 RelImp=63.26%\n",
            "Epoch[1/2] Step[6957] Loss=0.0729 GradNorm=0.8263 StepSize=0.0083 RelImp=96.83%\n",
            "Epoch[1/2] Step[6958] Loss=0.9394 GradNorm=3.9696 StepSize=0.0397 RelImp=59.10%\n",
            "Epoch[1/2] Step[6959] Loss=0.5324 GradNorm=3.3667 StepSize=0.0337 RelImp=76.82%\n",
            "Epoch[1/2] Step[6960] Loss=0.3856 GradNorm=3.0587 StepSize=0.0306 RelImp=83.21%\n",
            "Epoch[1/2] Step[6961] Loss=0.1433 GradNorm=1.7117 StepSize=0.0171 RelImp=93.76%\n",
            "Epoch[1/2] Step[6962] Loss=0.6344 GradNorm=3.9319 StepSize=0.0393 RelImp=72.38%\n",
            "Epoch[1/2] Step[6963] Loss=0.2946 GradNorm=2.6067 StepSize=0.0261 RelImp=87.17%\n",
            "Epoch[1/2] Step[6964] Loss=0.2859 GradNorm=3.2310 StepSize=0.0323 RelImp=87.55%\n",
            "Epoch[1/2] Step[6965] Loss=0.1820 GradNorm=2.1129 StepSize=0.0211 RelImp=92.08%\n",
            "Epoch[1/2] Step[6966] Loss=0.2972 GradNorm=3.1442 StepSize=0.0314 RelImp=87.06%\n",
            "Epoch[1/2] Step[6967] Loss=0.2035 GradNorm=3.6965 StepSize=0.0370 RelImp=91.14%\n",
            "Epoch[1/2] Step[6968] Loss=0.0537 GradNorm=0.7667 StepSize=0.0077 RelImp=97.66%\n",
            "Epoch[1/2] Step[6969] Loss=0.0401 GradNorm=0.9168 StepSize=0.0092 RelImp=98.25%\n",
            "Epoch[1/2] Step[6970] Loss=0.2059 GradNorm=3.2805 StepSize=0.0328 RelImp=91.04%\n",
            "Epoch[1/2] Step[6971] Loss=0.4805 GradNorm=2.5993 StepSize=0.0260 RelImp=79.08%\n",
            "Epoch[1/2] Step[6972] Loss=0.1929 GradNorm=2.1627 StepSize=0.0216 RelImp=91.60%\n",
            "Epoch[1/2] Step[6973] Loss=0.1362 GradNorm=1.9530 StepSize=0.0195 RelImp=94.07%\n",
            "Epoch[1/2] Step[6974] Loss=0.1605 GradNorm=2.2733 StepSize=0.0227 RelImp=93.01%\n",
            "Epoch[1/2] Step[6975] Loss=0.1052 GradNorm=1.7638 StepSize=0.0176 RelImp=95.42%\n",
            "Epoch[1/2] Step[6976] Loss=0.1871 GradNorm=1.6781 StepSize=0.0168 RelImp=91.85%\n",
            "Epoch[1/2] Step[6977] Loss=0.2004 GradNorm=2.0368 StepSize=0.0204 RelImp=91.28%\n",
            "Epoch[1/2] Step[6978] Loss=0.3317 GradNorm=3.2709 StepSize=0.0327 RelImp=85.56%\n",
            "Epoch[1/2] Step[6979] Loss=0.1937 GradNorm=2.2986 StepSize=0.0230 RelImp=91.56%\n",
            "Epoch[1/2] Step[6980] Loss=0.1790 GradNorm=2.6983 StepSize=0.0270 RelImp=92.20%\n",
            "Epoch[1/2] Step[6981] Loss=0.0277 GradNorm=0.3523 StepSize=0.0035 RelImp=98.79%\n",
            "Epoch[1/2] Step[6982] Loss=0.2301 GradNorm=2.2657 StepSize=0.0227 RelImp=89.98%\n",
            "Epoch[1/2] Step[6983] Loss=0.5390 GradNorm=3.1594 StepSize=0.0316 RelImp=76.53%\n",
            "Epoch[1/2] Step[6984] Loss=0.2014 GradNorm=2.4992 StepSize=0.0250 RelImp=91.23%\n",
            "Epoch[1/2] Step[6985] Loss=0.1895 GradNorm=3.3396 StepSize=0.0334 RelImp=91.75%\n",
            "Epoch[1/2] Step[6986] Loss=0.5804 GradNorm=4.5709 StepSize=0.0457 RelImp=74.73%\n",
            "Epoch[1/2] Step[6987] Loss=0.2044 GradNorm=2.8189 StepSize=0.0282 RelImp=91.10%\n",
            "Epoch[1/2] Step[6988] Loss=0.1304 GradNorm=2.5289 StepSize=0.0253 RelImp=94.32%\n",
            "Epoch[1/2] Step[6989] Loss=1.1629 GradNorm=4.4989 StepSize=0.0450 RelImp=49.36%\n",
            "Epoch[1/2] Step[6990] Loss=0.3678 GradNorm=2.7058 StepSize=0.0271 RelImp=83.98%\n",
            "Epoch[1/2] Step[6991] Loss=0.0743 GradNorm=1.2298 StepSize=0.0123 RelImp=96.76%\n",
            "Epoch[1/2] Step[6992] Loss=0.0608 GradNorm=0.9502 StepSize=0.0095 RelImp=97.35%\n",
            "Epoch[1/2] Step[6993] Loss=0.1280 GradNorm=2.1302 StepSize=0.0213 RelImp=94.43%\n",
            "Epoch[1/2] Step[6994] Loss=0.0336 GradNorm=0.6082 StepSize=0.0061 RelImp=98.54%\n",
            "Epoch[1/2] Step[6995] Loss=0.1282 GradNorm=2.0033 StepSize=0.0200 RelImp=94.42%\n",
            "Epoch[1/2] Step[6996] Loss=0.6751 GradNorm=3.8955 StepSize=0.0390 RelImp=70.60%\n",
            "Epoch[1/2] Step[6997] Loss=0.1850 GradNorm=2.1188 StepSize=0.0212 RelImp=91.94%\n",
            "Epoch[1/2] Step[6998] Loss=0.2294 GradNorm=2.6338 StepSize=0.0263 RelImp=90.01%\n",
            "Epoch[1/2] Step[6999] Loss=0.2047 GradNorm=3.2762 StepSize=0.0328 RelImp=91.09%\n",
            "Epoch[1/2] Step[7000] Loss=0.1973 GradNorm=3.3315 StepSize=0.0333 RelImp=91.41%\n",
            "Epoch[1/2] Step[7001] Loss=0.0896 GradNorm=1.3349 StepSize=0.0133 RelImp=96.10%\n",
            "Epoch[1/2] Step[7002] Loss=0.1686 GradNorm=2.5989 StepSize=0.0260 RelImp=92.66%\n",
            "Epoch[1/2] Step[7003] Loss=0.1729 GradNorm=2.5084 StepSize=0.0251 RelImp=92.47%\n",
            "Epoch[1/2] Step[7004] Loss=0.2930 GradNorm=3.2256 StepSize=0.0323 RelImp=87.24%\n",
            "Epoch[1/2] Step[7005] Loss=0.4585 GradNorm=2.9042 StepSize=0.0290 RelImp=80.03%\n",
            "Epoch[1/2] Step[7006] Loss=0.4996 GradNorm=3.8751 StepSize=0.0388 RelImp=78.25%\n",
            "Epoch[1/2] Step[7007] Loss=0.6995 GradNorm=6.1024 StepSize=0.0610 RelImp=69.54%\n",
            "Epoch[1/2] Step[7008] Loss=0.2448 GradNorm=2.7349 StepSize=0.0273 RelImp=89.34%\n",
            "Epoch[1/2] Step[7009] Loss=0.7421 GradNorm=3.8365 StepSize=0.0384 RelImp=67.69%\n",
            "Epoch[1/2] Step[7010] Loss=0.4820 GradNorm=2.5264 StepSize=0.0253 RelImp=79.01%\n",
            "Epoch[1/2] Step[7011] Loss=0.5248 GradNorm=3.4741 StepSize=0.0347 RelImp=77.15%\n",
            "Epoch[1/2] Step[7012] Loss=0.1983 GradNorm=2.1247 StepSize=0.0212 RelImp=91.36%\n",
            "Epoch[1/2] Step[7013] Loss=0.0576 GradNorm=0.9571 StepSize=0.0096 RelImp=97.49%\n",
            "Epoch[1/2] Step[7014] Loss=0.8673 GradNorm=4.7320 StepSize=0.0473 RelImp=62.24%\n",
            "Epoch[1/2] Step[7015] Loss=0.2618 GradNorm=1.5534 StepSize=0.0155 RelImp=88.60%\n",
            "Epoch[1/2] Step[7016] Loss=0.2296 GradNorm=2.7394 StepSize=0.0274 RelImp=90.00%\n",
            "Epoch[1/2] Step[7017] Loss=0.1350 GradNorm=2.3436 StepSize=0.0234 RelImp=94.12%\n",
            "Epoch[1/2] Step[7018] Loss=0.2174 GradNorm=2.6568 StepSize=0.0266 RelImp=90.54%\n",
            "Epoch[1/2] Step[7019] Loss=0.5427 GradNorm=4.1065 StepSize=0.0411 RelImp=76.37%\n",
            "Epoch[1/2] Step[7020] Loss=0.0591 GradNorm=0.8809 StepSize=0.0088 RelImp=97.43%\n",
            "Epoch[1/2] Step[7021] Loss=0.0668 GradNorm=0.8990 StepSize=0.0090 RelImp=97.09%\n",
            "Epoch[1/2] Step[7022] Loss=0.2306 GradNorm=2.8748 StepSize=0.0287 RelImp=89.96%\n",
            "Epoch[1/2] Step[7023] Loss=0.3051 GradNorm=2.9707 StepSize=0.0297 RelImp=86.72%\n",
            "Epoch[1/2] Step[7024] Loss=0.1233 GradNorm=1.6860 StepSize=0.0169 RelImp=94.63%\n",
            "Epoch[1/2] Step[7025] Loss=0.2199 GradNorm=2.4975 StepSize=0.0250 RelImp=90.42%\n",
            "Epoch[1/2] Step[7026] Loss=0.1356 GradNorm=1.7561 StepSize=0.0176 RelImp=94.10%\n",
            "Epoch[1/2] Step[7027] Loss=0.2489 GradNorm=3.5123 StepSize=0.0351 RelImp=89.16%\n",
            "Epoch[1/2] Step[7028] Loss=0.2900 GradNorm=2.6561 StepSize=0.0266 RelImp=87.37%\n",
            "Epoch[1/2] Step[7029] Loss=0.6498 GradNorm=4.2676 StepSize=0.0427 RelImp=71.71%\n",
            "Epoch[1/2] Step[7030] Loss=0.3077 GradNorm=3.1635 StepSize=0.0316 RelImp=86.60%\n",
            "Epoch[1/2] Step[7031] Loss=0.0411 GradNorm=0.7915 StepSize=0.0079 RelImp=98.21%\n",
            "Epoch[1/2] Step[7032] Loss=0.3203 GradNorm=4.1027 StepSize=0.0410 RelImp=86.05%\n",
            "Epoch[1/2] Step[7033] Loss=0.3693 GradNorm=3.5624 StepSize=0.0356 RelImp=83.92%\n",
            "Epoch[1/2] Step[7034] Loss=0.5729 GradNorm=3.3572 StepSize=0.0336 RelImp=75.06%\n",
            "Epoch[1/2] Step[7035] Loss=0.1249 GradNorm=2.0294 StepSize=0.0203 RelImp=94.56%\n",
            "Epoch[1/2] Step[7036] Loss=0.1955 GradNorm=3.3814 StepSize=0.0338 RelImp=91.49%\n",
            "Epoch[1/2] Step[7037] Loss=0.1504 GradNorm=2.4037 StepSize=0.0240 RelImp=93.45%\n",
            "Epoch[1/2] Step[7038] Loss=0.7678 GradNorm=3.0621 StepSize=0.0306 RelImp=66.57%\n",
            "Epoch[1/2] Step[7039] Loss=0.1733 GradNorm=2.3855 StepSize=0.0239 RelImp=92.45%\n",
            "Epoch[1/2] Step[7040] Loss=0.4489 GradNorm=3.2446 StepSize=0.0324 RelImp=80.45%\n",
            "Epoch[1/2] Step[7041] Loss=0.1035 GradNorm=1.5707 StepSize=0.0157 RelImp=95.49%\n",
            "Epoch[1/2] Step[7042] Loss=0.4250 GradNorm=3.2827 StepSize=0.0328 RelImp=81.49%\n",
            "Epoch[1/2] Step[7043] Loss=0.1475 GradNorm=2.9740 StepSize=0.0297 RelImp=93.58%\n",
            "Epoch[1/2] Step[7044] Loss=0.3036 GradNorm=3.4003 StepSize=0.0340 RelImp=86.78%\n",
            "Epoch[1/2] Step[7045] Loss=0.2136 GradNorm=2.5843 StepSize=0.0258 RelImp=90.70%\n",
            "Epoch[1/2] Step[7046] Loss=0.2448 GradNorm=3.4542 StepSize=0.0345 RelImp=89.34%\n",
            "Epoch[1/2] Step[7047] Loss=0.1425 GradNorm=1.7588 StepSize=0.0176 RelImp=93.79%\n",
            "Epoch[1/2] Step[7048] Loss=0.2237 GradNorm=2.7918 StepSize=0.0279 RelImp=90.26%\n",
            "Epoch[1/2] Step[7049] Loss=0.2288 GradNorm=2.6387 StepSize=0.0264 RelImp=90.04%\n",
            "Epoch[1/2] Step[7050] Loss=0.2965 GradNorm=3.0222 StepSize=0.0302 RelImp=87.09%\n",
            "Epoch[1/2] Step[7051] Loss=0.1755 GradNorm=2.4823 StepSize=0.0248 RelImp=92.36%\n",
            "Epoch[1/2] Step[7052] Loss=0.2409 GradNorm=2.5763 StepSize=0.0258 RelImp=89.51%\n",
            "Epoch[1/2] Step[7053] Loss=0.8064 GradNorm=4.2644 StepSize=0.0426 RelImp=64.89%\n",
            "Epoch[1/2] Step[7054] Loss=0.1779 GradNorm=2.3891 StepSize=0.0239 RelImp=92.26%\n",
            "Epoch[1/2] Step[7055] Loss=0.1687 GradNorm=2.0050 StepSize=0.0200 RelImp=92.65%\n",
            "Epoch[1/2] Step[7056] Loss=0.2836 GradNorm=4.2607 StepSize=0.0426 RelImp=87.65%\n",
            "Epoch[1/2] Step[7057] Loss=0.3375 GradNorm=3.9062 StepSize=0.0391 RelImp=85.31%\n",
            "Epoch[1/2] Step[7058] Loss=0.1404 GradNorm=1.7865 StepSize=0.0179 RelImp=93.89%\n",
            "Epoch[1/2] Step[7059] Loss=0.1181 GradNorm=2.0702 StepSize=0.0207 RelImp=94.86%\n",
            "Epoch[1/2] Step[7060] Loss=0.2070 GradNorm=2.3278 StepSize=0.0233 RelImp=90.99%\n",
            "Epoch[1/2] Step[7061] Loss=0.4483 GradNorm=1.9798 StepSize=0.0198 RelImp=80.48%\n",
            "Epoch[1/2] Step[7062] Loss=0.3682 GradNorm=3.1039 StepSize=0.0310 RelImp=83.97%\n",
            "Epoch[1/2] Step[7063] Loss=0.2215 GradNorm=3.2180 StepSize=0.0322 RelImp=90.35%\n",
            "Epoch[1/2] Step[7064] Loss=0.3999 GradNorm=4.1104 StepSize=0.0411 RelImp=82.59%\n",
            "Epoch[1/2] Step[7065] Loss=0.2238 GradNorm=2.6007 StepSize=0.0260 RelImp=90.26%\n",
            "Epoch[1/2] Step[7066] Loss=0.5315 GradNorm=3.6991 StepSize=0.0370 RelImp=76.86%\n",
            "Epoch[1/2] Step[7067] Loss=0.6800 GradNorm=3.5456 StepSize=0.0355 RelImp=70.39%\n",
            "Epoch[1/2] Step[7068] Loss=0.0489 GradNorm=0.8733 StepSize=0.0087 RelImp=97.87%\n",
            "Epoch[1/2] Step[7069] Loss=0.3432 GradNorm=2.2598 StepSize=0.0226 RelImp=85.06%\n",
            "Epoch[1/2] Step[7070] Loss=0.5700 GradNorm=2.7542 StepSize=0.0275 RelImp=75.18%\n",
            "Epoch[1/2] Step[7071] Loss=0.2718 GradNorm=3.2881 StepSize=0.0329 RelImp=88.17%\n",
            "Epoch[1/2] Step[7072] Loss=0.3604 GradNorm=2.9071 StepSize=0.0291 RelImp=84.31%\n",
            "Epoch[1/2] Step[7073] Loss=0.0982 GradNorm=1.7077 StepSize=0.0171 RelImp=95.72%\n",
            "Epoch[1/2] Step[7074] Loss=0.4609 GradNorm=3.4906 StepSize=0.0349 RelImp=79.93%\n",
            "Epoch[1/2] Step[7075] Loss=0.1595 GradNorm=1.8826 StepSize=0.0188 RelImp=93.06%\n",
            "Epoch[1/2] Step[7076] Loss=0.1928 GradNorm=2.1912 StepSize=0.0219 RelImp=91.61%\n",
            "Epoch[1/2] Step[7077] Loss=0.1361 GradNorm=1.6486 StepSize=0.0165 RelImp=94.07%\n",
            "Epoch[1/2] Step[7078] Loss=0.1421 GradNorm=2.6560 StepSize=0.0266 RelImp=93.81%\n",
            "Epoch[1/2] Step[7079] Loss=0.5781 GradNorm=3.1483 StepSize=0.0315 RelImp=74.83%\n",
            "Epoch[1/2] Step[7080] Loss=0.3621 GradNorm=3.1804 StepSize=0.0318 RelImp=84.23%\n",
            "Epoch[1/2] Step[7081] Loss=0.4134 GradNorm=3.2884 StepSize=0.0329 RelImp=82.00%\n",
            "Epoch[1/2] Step[7082] Loss=0.2182 GradNorm=2.5678 StepSize=0.0257 RelImp=90.50%\n",
            "Epoch[1/2] Step[7083] Loss=0.2236 GradNorm=2.7582 StepSize=0.0276 RelImp=90.27%\n",
            "Epoch[1/2] Step[7084] Loss=0.2945 GradNorm=2.6184 StepSize=0.0262 RelImp=87.18%\n",
            "Epoch[1/2] Step[7085] Loss=0.1604 GradNorm=2.0529 StepSize=0.0205 RelImp=93.01%\n",
            "Epoch[1/2] Step[7086] Loss=0.3162 GradNorm=3.9397 StepSize=0.0394 RelImp=86.23%\n",
            "Epoch[1/2] Step[7087] Loss=0.1861 GradNorm=2.4987 StepSize=0.0250 RelImp=91.90%\n",
            "Epoch[1/2] Step[7088] Loss=0.3331 GradNorm=2.5601 StepSize=0.0256 RelImp=85.50%\n",
            "Epoch[1/2] Step[7089] Loss=0.1082 GradNorm=1.4871 StepSize=0.0149 RelImp=95.29%\n",
            "Epoch[1/2] Step[7090] Loss=0.2299 GradNorm=3.1629 StepSize=0.0316 RelImp=89.99%\n",
            "Epoch[1/2] Step[7091] Loss=0.3347 GradNorm=3.6920 StepSize=0.0369 RelImp=85.43%\n",
            "Epoch[1/2] Step[7092] Loss=0.4908 GradNorm=3.5833 StepSize=0.0358 RelImp=78.63%\n",
            "Epoch[1/2] Step[7093] Loss=0.3283 GradNorm=1.8852 StepSize=0.0189 RelImp=85.70%\n",
            "Epoch[1/2] Step[7094] Loss=0.5592 GradNorm=5.8540 StepSize=0.0585 RelImp=75.65%\n",
            "Epoch[1/2] Step[7095] Loss=0.2986 GradNorm=3.6731 StepSize=0.0367 RelImp=87.00%\n",
            "Epoch[1/2] Step[7096] Loss=0.1178 GradNorm=1.8242 StepSize=0.0182 RelImp=94.87%\n",
            "Epoch[1/2] Step[7097] Loss=0.4324 GradNorm=3.6771 StepSize=0.0368 RelImp=81.17%\n",
            "Epoch[1/2] Step[7098] Loss=0.1360 GradNorm=1.6909 StepSize=0.0169 RelImp=94.08%\n",
            "Epoch[1/2] Step[7099] Loss=0.1749 GradNorm=2.3300 StepSize=0.0233 RelImp=92.38%\n",
            "Epoch[1/2] Step[7100] Loss=0.6230 GradNorm=4.4952 StepSize=0.0450 RelImp=72.87%\n",
            "Epoch[1/2] Step[7101] Loss=0.8168 GradNorm=7.4374 StepSize=0.0744 RelImp=64.43%\n",
            "Epoch[1/2] Step[7102] Loss=0.1632 GradNorm=2.6409 StepSize=0.0264 RelImp=92.90%\n",
            "Epoch[1/2] Step[7103] Loss=0.0676 GradNorm=1.0013 StepSize=0.0100 RelImp=97.06%\n",
            "Epoch[1/2] Step[7104] Loss=0.2304 GradNorm=2.9513 StepSize=0.0295 RelImp=89.97%\n",
            "Epoch[1/2] Step[7105] Loss=0.5051 GradNorm=4.1248 StepSize=0.0412 RelImp=78.01%\n",
            "Epoch[1/2] Step[7106] Loss=0.4204 GradNorm=3.3445 StepSize=0.0334 RelImp=81.69%\n",
            "Epoch[1/2] Step[7107] Loss=0.0930 GradNorm=1.3306 StepSize=0.0133 RelImp=95.95%\n",
            "Epoch[1/2] Step[7108] Loss=1.0802 GradNorm=6.8618 StepSize=0.0686 RelImp=52.96%\n",
            "Epoch[1/2] Step[7109] Loss=0.7840 GradNorm=4.7699 StepSize=0.0477 RelImp=65.86%\n",
            "Epoch[1/2] Step[7110] Loss=0.1887 GradNorm=2.7118 StepSize=0.0271 RelImp=91.79%\n",
            "Epoch[1/2] Step[7111] Loss=0.0662 GradNorm=0.9020 StepSize=0.0090 RelImp=97.12%\n",
            "Epoch[1/2] Step[7112] Loss=0.2590 GradNorm=1.8324 StepSize=0.0183 RelImp=88.72%\n",
            "Epoch[1/2] Step[7113] Loss=0.1275 GradNorm=2.1945 StepSize=0.0219 RelImp=94.45%\n",
            "Epoch[1/2] Step[7114] Loss=0.2063 GradNorm=1.2869 StepSize=0.0129 RelImp=91.02%\n",
            "Epoch[1/2] Step[7115] Loss=0.1733 GradNorm=2.5485 StepSize=0.0255 RelImp=92.45%\n",
            "Epoch[1/2] Step[7116] Loss=0.1570 GradNorm=2.7916 StepSize=0.0279 RelImp=93.16%\n",
            "Epoch[1/2] Step[7117] Loss=0.1774 GradNorm=1.8349 StepSize=0.0183 RelImp=92.27%\n",
            "Epoch[1/2] Step[7118] Loss=0.1857 GradNorm=2.3982 StepSize=0.0240 RelImp=91.91%\n",
            "Epoch[1/2] Step[7119] Loss=0.5468 GradNorm=4.5987 StepSize=0.0460 RelImp=76.19%\n",
            "Epoch[1/2] Step[7120] Loss=0.1718 GradNorm=1.9897 StepSize=0.0199 RelImp=92.52%\n",
            "Epoch[1/2] Step[7121] Loss=0.5298 GradNorm=2.8291 StepSize=0.0283 RelImp=76.93%\n",
            "Epoch[1/2] Step[7122] Loss=0.6609 GradNorm=4.7976 StepSize=0.0480 RelImp=71.22%\n",
            "Epoch[1/2] Step[7123] Loss=0.3005 GradNorm=3.8362 StepSize=0.0384 RelImp=86.92%\n",
            "Epoch[1/2] Step[7124] Loss=0.4291 GradNorm=4.7786 StepSize=0.0478 RelImp=81.32%\n",
            "Epoch[1/2] Step[7125] Loss=0.4255 GradNorm=3.7924 StepSize=0.0379 RelImp=81.47%\n",
            "Epoch[1/2] Step[7126] Loss=0.2468 GradNorm=2.4553 StepSize=0.0246 RelImp=89.25%\n",
            "Epoch[1/2] Step[7127] Loss=0.2495 GradNorm=2.8829 StepSize=0.0288 RelImp=89.14%\n",
            "Epoch[1/2] Step[7128] Loss=0.0613 GradNorm=0.7142 StepSize=0.0071 RelImp=97.33%\n",
            "Epoch[1/2] Step[7129] Loss=0.2220 GradNorm=2.4480 StepSize=0.0245 RelImp=90.34%\n",
            "Epoch[1/2] Step[7130] Loss=0.7660 GradNorm=3.0360 StepSize=0.0304 RelImp=66.64%\n",
            "Epoch[1/2] Step[7131] Loss=0.3149 GradNorm=2.9701 StepSize=0.0297 RelImp=86.29%\n",
            "Epoch[1/2] Step[7132] Loss=0.0631 GradNorm=0.9204 StepSize=0.0092 RelImp=97.25%\n",
            "Epoch[1/2] Step[7133] Loss=0.5719 GradNorm=3.3722 StepSize=0.0337 RelImp=75.10%\n",
            "Epoch[1/2] Step[7134] Loss=0.0268 GradNorm=0.4566 StepSize=0.0046 RelImp=98.83%\n",
            "Epoch[1/2] Step[7135] Loss=0.3148 GradNorm=3.0480 StepSize=0.0305 RelImp=86.29%\n",
            "Epoch[1/2] Step[7136] Loss=0.3345 GradNorm=3.2957 StepSize=0.0330 RelImp=85.43%\n",
            "Epoch[1/2] Step[7137] Loss=0.4853 GradNorm=2.9819 StepSize=0.0298 RelImp=78.87%\n",
            "Epoch[1/2] Step[7138] Loss=0.0338 GradNorm=0.5015 StepSize=0.0050 RelImp=98.53%\n",
            "Epoch[1/2] Step[7139] Loss=0.2800 GradNorm=3.0940 StepSize=0.0309 RelImp=87.81%\n",
            "Epoch[1/2] Step[7140] Loss=0.3420 GradNorm=2.7483 StepSize=0.0275 RelImp=85.11%\n",
            "Epoch[1/2] Step[7141] Loss=0.0914 GradNorm=1.0702 StepSize=0.0107 RelImp=96.02%\n",
            "Epoch[1/2] Step[7142] Loss=0.6568 GradNorm=4.1389 StepSize=0.0414 RelImp=71.40%\n",
            "Epoch[1/2] Step[7143] Loss=0.2191 GradNorm=2.1563 StepSize=0.0216 RelImp=90.46%\n",
            "Epoch[1/2] Step[7144] Loss=0.1052 GradNorm=1.2406 StepSize=0.0124 RelImp=95.42%\n",
            "Epoch[1/2] Step[7145] Loss=0.0457 GradNorm=0.5784 StepSize=0.0058 RelImp=98.01%\n",
            "Epoch[1/2] Step[7146] Loss=0.4087 GradNorm=4.2029 StepSize=0.0420 RelImp=82.20%\n",
            "Epoch[1/2] Step[7147] Loss=0.2392 GradNorm=2.8331 StepSize=0.0283 RelImp=89.59%\n",
            "Epoch[1/2] Step[7148] Loss=0.5761 GradNorm=4.3719 StepSize=0.0437 RelImp=74.92%\n",
            "Epoch[1/2] Step[7149] Loss=1.1472 GradNorm=5.4334 StepSize=0.0543 RelImp=50.05%\n",
            "Epoch[1/2] Step[7150] Loss=0.2911 GradNorm=3.2385 StepSize=0.0324 RelImp=87.32%\n",
            "Epoch[1/2] Step[7151] Loss=0.0970 GradNorm=0.9647 StepSize=0.0096 RelImp=95.78%\n",
            "Epoch[1/2] Step[7152] Loss=0.0496 GradNorm=0.9106 StepSize=0.0091 RelImp=97.84%\n",
            "Epoch[1/2] Step[7153] Loss=0.4045 GradNorm=4.6325 StepSize=0.0463 RelImp=82.39%\n",
            "Epoch[1/2] Step[7154] Loss=0.2089 GradNorm=2.1682 StepSize=0.0217 RelImp=90.90%\n",
            "Epoch[1/2] Step[7155] Loss=0.1738 GradNorm=3.1007 StepSize=0.0310 RelImp=92.43%\n",
            "Epoch[1/2] Step[7156] Loss=0.3298 GradNorm=3.8221 StepSize=0.0382 RelImp=85.64%\n",
            "Epoch[1/2] Step[7157] Loss=0.0571 GradNorm=0.8684 StepSize=0.0087 RelImp=97.51%\n",
            "Epoch[1/2] Step[7158] Loss=0.1862 GradNorm=2.2716 StepSize=0.0227 RelImp=91.89%\n",
            "Epoch[1/2] Step[7159] Loss=0.1438 GradNorm=1.9636 StepSize=0.0196 RelImp=93.74%\n",
            "Epoch[1/2] Step[7160] Loss=0.2446 GradNorm=2.8774 StepSize=0.0288 RelImp=89.35%\n",
            "Epoch[1/2] Step[7161] Loss=0.2968 GradNorm=2.9899 StepSize=0.0299 RelImp=87.08%\n",
            "Epoch[1/2] Step[7162] Loss=0.2806 GradNorm=3.5097 StepSize=0.0351 RelImp=87.78%\n",
            "Epoch[1/2] Step[7163] Loss=0.2669 GradNorm=2.7087 StepSize=0.0271 RelImp=88.38%\n",
            "Epoch[1/2] Step[7164] Loss=0.2948 GradNorm=3.0279 StepSize=0.0303 RelImp=87.16%\n",
            "Epoch[1/2] Step[7165] Loss=0.2794 GradNorm=3.0204 StepSize=0.0302 RelImp=87.83%\n",
            "Epoch[1/2] Step[7166] Loss=0.5947 GradNorm=4.5666 StepSize=0.0457 RelImp=74.11%\n",
            "Epoch[1/2] Step[7167] Loss=0.2841 GradNorm=4.2531 StepSize=0.0425 RelImp=87.63%\n",
            "Epoch[1/2] Step[7168] Loss=0.7764 GradNorm=4.4954 StepSize=0.0450 RelImp=66.19%\n",
            "Epoch[1/2] Step[7169] Loss=0.2537 GradNorm=2.4739 StepSize=0.0247 RelImp=88.95%\n",
            "Epoch[1/2] Step[7170] Loss=0.3704 GradNorm=3.0167 StepSize=0.0302 RelImp=83.87%\n",
            "Epoch[1/2] Step[7171] Loss=0.0680 GradNorm=1.0278 StepSize=0.0103 RelImp=97.04%\n",
            "Epoch[1/2] Step[7172] Loss=0.4056 GradNorm=2.9278 StepSize=0.0293 RelImp=82.34%\n",
            "Epoch[1/2] Step[7173] Loss=0.2336 GradNorm=2.7364 StepSize=0.0274 RelImp=89.83%\n",
            "Epoch[1/2] Step[7174] Loss=0.1161 GradNorm=0.8422 StepSize=0.0084 RelImp=94.94%\n",
            "Epoch[1/2] Step[7175] Loss=0.3059 GradNorm=3.5569 StepSize=0.0356 RelImp=86.68%\n",
            "Epoch[1/2] Step[7176] Loss=0.4770 GradNorm=3.7790 StepSize=0.0378 RelImp=79.23%\n",
            "Epoch[1/2] Step[7177] Loss=0.7386 GradNorm=4.9451 StepSize=0.0495 RelImp=67.84%\n",
            "Epoch[1/2] Step[7178] Loss=0.1987 GradNorm=2.4998 StepSize=0.0250 RelImp=91.35%\n",
            "Epoch[1/2] Step[7179] Loss=0.1286 GradNorm=1.7786 StepSize=0.0178 RelImp=94.40%\n",
            "Epoch[1/2] Step[7180] Loss=0.3193 GradNorm=3.5845 StepSize=0.0358 RelImp=86.10%\n",
            "Epoch[1/2] Step[7181] Loss=0.2834 GradNorm=4.9036 StepSize=0.0490 RelImp=87.66%\n",
            "Epoch[1/2] Step[7182] Loss=0.2439 GradNorm=3.2778 StepSize=0.0328 RelImp=89.38%\n",
            "Epoch[1/2] Step[7183] Loss=0.2534 GradNorm=3.7060 StepSize=0.0371 RelImp=88.97%\n",
            "Epoch[1/2] Step[7184] Loss=0.1416 GradNorm=2.2460 StepSize=0.0225 RelImp=93.83%\n",
            "Epoch[1/2] Step[7185] Loss=0.1737 GradNorm=2.0182 StepSize=0.0202 RelImp=92.44%\n",
            "Epoch[1/2] Step[7186] Loss=0.1356 GradNorm=1.4871 StepSize=0.0149 RelImp=94.10%\n",
            "Epoch[1/2] Step[7187] Loss=0.2883 GradNorm=2.9343 StepSize=0.0293 RelImp=87.45%\n",
            "Epoch[1/2] Step[7188] Loss=0.4628 GradNorm=2.4452 StepSize=0.0245 RelImp=79.85%\n",
            "Epoch[1/2] Step[7189] Loss=0.3083 GradNorm=3.2558 StepSize=0.0326 RelImp=86.58%\n",
            "Epoch[1/2] Step[7190] Loss=0.1318 GradNorm=1.8423 StepSize=0.0184 RelImp=94.26%\n",
            "Epoch[1/2] Step[7191] Loss=0.3202 GradNorm=3.7536 StepSize=0.0375 RelImp=86.06%\n",
            "Epoch[1/2] Step[7192] Loss=0.2193 GradNorm=3.8668 StepSize=0.0387 RelImp=90.45%\n",
            "Epoch[1/2] Step[7193] Loss=0.3867 GradNorm=3.9407 StepSize=0.0394 RelImp=83.16%\n",
            "Epoch[1/2] Step[7194] Loss=0.2994 GradNorm=3.8570 StepSize=0.0386 RelImp=86.96%\n",
            "Epoch[1/2] Step[7195] Loss=0.4678 GradNorm=4.8458 StepSize=0.0485 RelImp=79.63%\n",
            "Epoch[1/2] Step[7196] Loss=0.2869 GradNorm=2.8084 StepSize=0.0281 RelImp=87.51%\n",
            "Epoch[1/2] Step[7197] Loss=0.1963 GradNorm=2.0285 StepSize=0.0203 RelImp=91.45%\n",
            "Epoch[1/2] Step[7198] Loss=0.2668 GradNorm=3.1438 StepSize=0.0314 RelImp=88.38%\n",
            "Epoch[1/2] Step[7199] Loss=0.2437 GradNorm=2.9673 StepSize=0.0297 RelImp=89.39%\n",
            "Epoch[1/2] Step[7200] Loss=0.1110 GradNorm=2.2535 StepSize=0.0225 RelImp=95.17%\n",
            "Epoch[1/2] Step[7201] Loss=0.1064 GradNorm=1.7099 StepSize=0.0171 RelImp=95.37%\n",
            "Epoch[1/2] Step[7202] Loss=0.2123 GradNorm=2.6074 StepSize=0.0261 RelImp=90.76%\n",
            "Epoch[1/2] Step[7203] Loss=0.1510 GradNorm=2.3446 StepSize=0.0234 RelImp=93.43%\n",
            "Epoch[1/2] Step[7204] Loss=0.0355 GradNorm=0.9004 StepSize=0.0090 RelImp=98.45%\n",
            "Epoch[1/2] Step[7205] Loss=0.3996 GradNorm=3.5572 StepSize=0.0356 RelImp=82.60%\n",
            "Epoch[1/2] Step[7206] Loss=0.0665 GradNorm=1.2032 StepSize=0.0120 RelImp=97.10%\n",
            "Epoch[1/2] Step[7207] Loss=0.3165 GradNorm=3.5999 StepSize=0.0360 RelImp=86.22%\n",
            "Epoch[1/2] Step[7208] Loss=0.1268 GradNorm=2.2054 StepSize=0.0221 RelImp=94.48%\n",
            "Epoch[1/2] Step[7209] Loss=0.2304 GradNorm=2.6372 StepSize=0.0264 RelImp=89.97%\n",
            "Epoch[1/2] Step[7210] Loss=0.2557 GradNorm=3.2926 StepSize=0.0329 RelImp=88.87%\n",
            "Epoch[1/2] Step[7211] Loss=0.0703 GradNorm=1.1267 StepSize=0.0113 RelImp=96.94%\n",
            "Epoch[1/2] Step[7212] Loss=0.2323 GradNorm=2.7486 StepSize=0.0275 RelImp=89.89%\n",
            "Epoch[1/2] Step[7213] Loss=0.2843 GradNorm=3.2457 StepSize=0.0325 RelImp=87.62%\n",
            "Epoch[1/2] Step[7214] Loss=0.1064 GradNorm=1.9046 StepSize=0.0190 RelImp=95.37%\n",
            "Epoch[1/2] Step[7215] Loss=0.0960 GradNorm=1.6251 StepSize=0.0163 RelImp=95.82%\n",
            "Epoch[1/2] Step[7216] Loss=0.5644 GradNorm=4.4027 StepSize=0.0440 RelImp=75.42%\n",
            "Epoch[1/2] Step[7217] Loss=0.4663 GradNorm=2.9438 StepSize=0.0294 RelImp=79.70%\n",
            "Epoch[1/2] Step[7218] Loss=0.5658 GradNorm=3.2914 StepSize=0.0329 RelImp=75.36%\n",
            "Epoch[1/2] Step[7219] Loss=0.1484 GradNorm=2.5371 StepSize=0.0254 RelImp=93.54%\n",
            "Epoch[1/2] Step[7220] Loss=0.1889 GradNorm=2.9030 StepSize=0.0290 RelImp=91.77%\n",
            "Epoch[1/2] Step[7221] Loss=0.2939 GradNorm=3.6004 StepSize=0.0360 RelImp=87.20%\n",
            "Epoch[1/2] Step[7222] Loss=0.4571 GradNorm=4.1448 StepSize=0.0414 RelImp=80.10%\n",
            "Epoch[1/2] Step[7223] Loss=1.1366 GradNorm=6.4248 StepSize=0.0642 RelImp=50.51%\n",
            "Epoch[1/2] Step[7224] Loss=0.3190 GradNorm=2.8796 StepSize=0.0288 RelImp=86.11%\n",
            "Epoch[1/2] Step[7225] Loss=0.0933 GradNorm=1.2843 StepSize=0.0128 RelImp=95.94%\n",
            "Epoch[1/2] Step[7226] Loss=0.4737 GradNorm=5.1980 StepSize=0.0520 RelImp=79.38%\n",
            "Epoch[1/2] Step[7227] Loss=0.5374 GradNorm=4.0803 StepSize=0.0408 RelImp=76.60%\n",
            "Epoch[1/2] Step[7228] Loss=0.1188 GradNorm=0.9175 StepSize=0.0092 RelImp=94.83%\n",
            "Epoch[1/2] Step[7229] Loss=0.0472 GradNorm=0.5145 StepSize=0.0051 RelImp=97.94%\n",
            "Epoch[1/2] Step[7230] Loss=0.4202 GradNorm=5.0489 StepSize=0.0505 RelImp=81.70%\n",
            "Epoch[1/2] Step[7231] Loss=1.0680 GradNorm=4.0258 StepSize=0.0403 RelImp=53.50%\n",
            "Epoch[1/2] Step[7232] Loss=0.4264 GradNorm=3.9328 StepSize=0.0393 RelImp=81.43%\n",
            "Epoch[1/2] Step[7233] Loss=0.8005 GradNorm=2.3354 StepSize=0.0234 RelImp=65.14%\n",
            "Epoch[1/2] Step[7234] Loss=0.8978 GradNorm=5.2593 StepSize=0.0526 RelImp=60.91%\n",
            "Epoch[1/2] Step[7235] Loss=0.3977 GradNorm=4.3914 StepSize=0.0439 RelImp=82.68%\n",
            "Epoch[1/2] Step[7236] Loss=0.1381 GradNorm=1.8480 StepSize=0.0185 RelImp=93.99%\n",
            "Epoch[1/2] Step[7237] Loss=0.1393 GradNorm=1.1575 StepSize=0.0116 RelImp=93.93%\n",
            "Epoch[1/2] Step[7238] Loss=0.4364 GradNorm=4.6094 StepSize=0.0461 RelImp=81.00%\n",
            "Epoch[1/2] Step[7239] Loss=0.1814 GradNorm=3.0889 StepSize=0.0309 RelImp=92.10%\n",
            "Epoch[1/2] Step[7240] Loss=0.1391 GradNorm=1.5968 StepSize=0.0160 RelImp=93.94%\n",
            "Epoch[1/2] Step[7241] Loss=0.1317 GradNorm=2.6272 StepSize=0.0263 RelImp=94.26%\n",
            "Epoch[1/2] Step[7242] Loss=0.2313 GradNorm=3.1946 StepSize=0.0319 RelImp=89.93%\n",
            "Epoch[1/2] Step[7243] Loss=0.0349 GradNorm=0.5161 StepSize=0.0052 RelImp=98.48%\n",
            "Epoch[1/2] Step[7244] Loss=0.3475 GradNorm=4.7333 StepSize=0.0473 RelImp=84.87%\n",
            "Epoch[1/2] Step[7245] Loss=0.3059 GradNorm=2.7808 StepSize=0.0278 RelImp=86.68%\n",
            "Epoch[1/2] Step[7246] Loss=0.0532 GradNorm=0.9175 StepSize=0.0092 RelImp=97.68%\n",
            "Epoch[1/2] Step[7247] Loss=0.1226 GradNorm=2.2307 StepSize=0.0223 RelImp=94.66%\n",
            "Epoch[1/2] Step[7248] Loss=0.3310 GradNorm=2.8721 StepSize=0.0287 RelImp=85.59%\n",
            "Epoch[1/2] Step[7249] Loss=0.3384 GradNorm=2.8033 StepSize=0.0280 RelImp=85.26%\n",
            "Epoch[1/2] Step[7250] Loss=0.4610 GradNorm=3.5945 StepSize=0.0359 RelImp=79.93%\n",
            "Epoch[1/2] Step[7251] Loss=0.3810 GradNorm=2.9310 StepSize=0.0293 RelImp=83.41%\n",
            "Epoch[1/2] Step[7252] Loss=0.2709 GradNorm=3.7202 StepSize=0.0372 RelImp=88.20%\n",
            "Epoch[1/2] Step[7253] Loss=0.2142 GradNorm=2.8593 StepSize=0.0286 RelImp=90.67%\n",
            "Epoch[1/2] Step[7254] Loss=0.5555 GradNorm=3.9048 StepSize=0.0390 RelImp=75.81%\n",
            "Epoch[1/2] Step[7255] Loss=0.5901 GradNorm=4.0132 StepSize=0.0401 RelImp=74.31%\n",
            "Epoch[1/2] Step[7256] Loss=0.1204 GradNorm=1.5530 StepSize=0.0155 RelImp=94.76%\n",
            "Epoch[1/2] Step[7257] Loss=0.0722 GradNorm=0.8375 StepSize=0.0084 RelImp=96.85%\n",
            "Epoch[1/2] Step[7258] Loss=0.1662 GradNorm=1.9074 StepSize=0.0191 RelImp=92.76%\n",
            "Epoch[1/2] Step[7259] Loss=0.0783 GradNorm=1.5223 StepSize=0.0152 RelImp=96.59%\n",
            "Epoch[1/2] Step[7260] Loss=0.1557 GradNorm=1.8109 StepSize=0.0181 RelImp=93.22%\n",
            "Epoch[1/2] Step[7261] Loss=0.2018 GradNorm=2.9245 StepSize=0.0292 RelImp=91.21%\n",
            "Epoch[1/2] Step[7262] Loss=0.1044 GradNorm=1.8673 StepSize=0.0187 RelImp=95.45%\n",
            "Epoch[1/2] Step[7263] Loss=0.4083 GradNorm=3.9186 StepSize=0.0392 RelImp=82.22%\n",
            "Epoch[1/2] Step[7264] Loss=0.0710 GradNorm=1.4154 StepSize=0.0142 RelImp=96.91%\n",
            "Epoch[1/2] Step[7265] Loss=0.4672 GradNorm=4.5368 StepSize=0.0454 RelImp=79.66%\n",
            "Epoch[1/2] Step[7266] Loss=0.3858 GradNorm=4.4223 StepSize=0.0442 RelImp=83.20%\n",
            "Epoch[1/2] Step[7267] Loss=0.4818 GradNorm=3.2879 StepSize=0.0329 RelImp=79.02%\n",
            "Epoch[1/2] Step[7268] Loss=0.5422 GradNorm=4.5389 StepSize=0.0454 RelImp=76.39%\n",
            "Epoch[1/2] Step[7269] Loss=0.1237 GradNorm=1.9879 StepSize=0.0199 RelImp=94.61%\n",
            "Epoch[1/2] Step[7270] Loss=0.2395 GradNorm=2.4019 StepSize=0.0240 RelImp=89.57%\n",
            "Epoch[1/2] Step[7271] Loss=0.3524 GradNorm=3.6641 StepSize=0.0366 RelImp=84.66%\n",
            "Epoch[1/2] Step[7272] Loss=0.0944 GradNorm=1.1217 StepSize=0.0112 RelImp=95.89%\n",
            "Epoch[1/2] Step[7273] Loss=0.0680 GradNorm=0.7954 StepSize=0.0080 RelImp=97.04%\n",
            "Epoch[1/2] Step[7274] Loss=0.3033 GradNorm=3.2787 StepSize=0.0328 RelImp=86.79%\n",
            "Epoch[1/2] Step[7275] Loss=0.2692 GradNorm=2.7012 StepSize=0.0270 RelImp=88.28%\n",
            "Epoch[1/2] Step[7276] Loss=0.2767 GradNorm=3.3624 StepSize=0.0336 RelImp=87.95%\n",
            "Epoch[1/2] Step[7277] Loss=0.1165 GradNorm=1.9064 StepSize=0.0191 RelImp=94.93%\n",
            "Epoch[1/2] Step[7278] Loss=0.1261 GradNorm=1.7713 StepSize=0.0177 RelImp=94.51%\n",
            "Epoch[1/2] Step[7279] Loss=0.1658 GradNorm=2.0818 StepSize=0.0208 RelImp=92.78%\n",
            "Epoch[1/2] Step[7280] Loss=0.1798 GradNorm=2.7691 StepSize=0.0277 RelImp=92.17%\n",
            "Epoch[1/2] Step[7281] Loss=0.9150 GradNorm=4.9769 StepSize=0.0498 RelImp=60.16%\n",
            "Epoch[1/2] Step[7282] Loss=0.0833 GradNorm=1.3770 StepSize=0.0138 RelImp=96.37%\n",
            "Epoch[1/2] Step[7283] Loss=0.1513 GradNorm=2.4649 StepSize=0.0246 RelImp=93.41%\n",
            "Epoch[1/2] Step[7284] Loss=0.1856 GradNorm=2.0682 StepSize=0.0207 RelImp=91.92%\n",
            "Epoch[1/2] Step[7285] Loss=0.4088 GradNorm=5.5167 StepSize=0.0552 RelImp=82.20%\n",
            "Epoch[1/2] Step[7286] Loss=0.2273 GradNorm=3.3721 StepSize=0.0337 RelImp=90.10%\n",
            "Epoch[1/2] Step[7287] Loss=0.6485 GradNorm=4.6840 StepSize=0.0468 RelImp=71.76%\n",
            "Epoch[1/2] Step[7288] Loss=0.5800 GradNorm=4.0360 StepSize=0.0404 RelImp=74.75%\n",
            "Epoch[1/2] Step[7289] Loss=0.0847 GradNorm=0.8783 StepSize=0.0088 RelImp=96.31%\n",
            "Epoch[1/2] Step[7290] Loss=0.2067 GradNorm=2.6615 StepSize=0.0266 RelImp=91.00%\n",
            "Epoch[1/2] Step[7291] Loss=1.0820 GradNorm=5.6738 StepSize=0.0567 RelImp=52.88%\n",
            "Epoch[1/2] Step[7292] Loss=0.2122 GradNorm=3.7654 StepSize=0.0377 RelImp=90.76%\n",
            "Epoch[1/2] Step[7293] Loss=0.1525 GradNorm=1.8783 StepSize=0.0188 RelImp=93.36%\n",
            "Epoch[1/2] Step[7294] Loss=0.1422 GradNorm=2.7347 StepSize=0.0273 RelImp=93.81%\n",
            "Epoch[1/2] Step[7295] Loss=0.3588 GradNorm=3.0078 StepSize=0.0301 RelImp=84.38%\n",
            "Epoch[1/2] Step[7296] Loss=0.2265 GradNorm=3.1373 StepSize=0.0314 RelImp=90.14%\n",
            "Epoch[1/2] Step[7297] Loss=0.3087 GradNorm=3.9913 StepSize=0.0399 RelImp=86.56%\n",
            "Epoch[1/2] Step[7298] Loss=0.7894 GradNorm=5.6377 StepSize=0.0564 RelImp=65.63%\n",
            "Epoch[1/2] Step[7299] Loss=0.5119 GradNorm=3.6858 StepSize=0.0369 RelImp=77.71%\n",
            "Epoch[1/2] Step[7300] Loss=0.2195 GradNorm=2.4063 StepSize=0.0241 RelImp=90.44%\n",
            "Epoch[1/2] Step[7301] Loss=0.1112 GradNorm=1.4572 StepSize=0.0146 RelImp=95.16%\n",
            "Epoch[1/2] Step[7302] Loss=0.4779 GradNorm=4.7148 StepSize=0.0471 RelImp=79.19%\n",
            "Epoch[1/2] Step[7303] Loss=0.4636 GradNorm=4.4162 StepSize=0.0442 RelImp=79.82%\n",
            "Epoch[1/2] Step[7304] Loss=0.0434 GradNorm=0.7123 StepSize=0.0071 RelImp=98.11%\n",
            "Epoch[1/2] Step[7305] Loss=0.3366 GradNorm=3.8779 StepSize=0.0388 RelImp=85.34%\n",
            "Epoch[1/2] Step[7306] Loss=0.3541 GradNorm=5.0430 StepSize=0.0504 RelImp=84.58%\n",
            "Epoch[1/2] Step[7307] Loss=0.4869 GradNorm=3.2713 StepSize=0.0327 RelImp=78.80%\n",
            "Epoch[1/2] Step[7308] Loss=0.5479 GradNorm=4.7828 StepSize=0.0478 RelImp=76.14%\n",
            "Epoch[1/2] Step[7309] Loss=0.1726 GradNorm=2.4786 StepSize=0.0248 RelImp=92.49%\n",
            "Epoch[1/2] Step[7310] Loss=0.2034 GradNorm=2.2435 StepSize=0.0224 RelImp=91.14%\n",
            "Epoch[1/2] Step[7311] Loss=0.3453 GradNorm=5.8410 StepSize=0.0584 RelImp=84.96%\n",
            "Epoch[1/2] Step[7312] Loss=0.6525 GradNorm=5.4439 StepSize=0.0544 RelImp=71.59%\n",
            "Epoch[1/2] Step[7313] Loss=0.1891 GradNorm=3.1728 StepSize=0.0317 RelImp=91.77%\n",
            "Epoch[1/2] Step[7314] Loss=0.1009 GradNorm=1.5257 StepSize=0.0153 RelImp=95.61%\n",
            "Epoch[1/2] Step[7315] Loss=0.3549 GradNorm=3.9629 StepSize=0.0396 RelImp=84.55%\n",
            "Epoch[1/2] Step[7316] Loss=0.3136 GradNorm=2.9447 StepSize=0.0294 RelImp=86.34%\n",
            "Epoch[1/2] Step[7317] Loss=0.3170 GradNorm=3.2534 StepSize=0.0325 RelImp=86.20%\n",
            "Epoch[1/2] Step[7318] Loss=0.4400 GradNorm=3.6513 StepSize=0.0365 RelImp=80.84%\n",
            "Epoch[1/2] Step[7319] Loss=0.1215 GradNorm=1.5797 StepSize=0.0158 RelImp=94.71%\n",
            "Epoch[1/2] Step[7320] Loss=0.7359 GradNorm=4.7818 StepSize=0.0478 RelImp=67.96%\n",
            "Epoch[1/2] Step[7321] Loss=0.7810 GradNorm=5.7895 StepSize=0.0579 RelImp=65.99%\n",
            "Epoch[1/2] Step[7322] Loss=0.1603 GradNorm=2.5133 StepSize=0.0251 RelImp=93.02%\n",
            "Epoch[1/2] Step[7323] Loss=0.3301 GradNorm=4.4464 StepSize=0.0445 RelImp=85.62%\n",
            "Epoch[1/2] Step[7324] Loss=0.3629 GradNorm=2.9302 StepSize=0.0293 RelImp=84.20%\n",
            "Epoch[1/2] Step[7325] Loss=0.0578 GradNorm=0.9174 StepSize=0.0092 RelImp=97.48%\n",
            "Epoch[1/2] Step[7326] Loss=0.3603 GradNorm=4.2315 StepSize=0.0423 RelImp=84.31%\n",
            "Epoch[1/2] Step[7327] Loss=0.1416 GradNorm=2.2479 StepSize=0.0225 RelImp=93.83%\n",
            "Epoch[1/2] Step[7328] Loss=0.2667 GradNorm=3.8918 StepSize=0.0389 RelImp=88.39%\n",
            "Epoch[1/2] Step[7329] Loss=0.1402 GradNorm=1.3459 StepSize=0.0135 RelImp=93.90%\n",
            "Epoch[1/2] Step[7330] Loss=0.0719 GradNorm=0.8602 StepSize=0.0086 RelImp=96.87%\n",
            "Epoch[1/2] Step[7331] Loss=0.1878 GradNorm=2.3135 StepSize=0.0231 RelImp=91.82%\n",
            "Epoch[1/2] Step[7332] Loss=0.7550 GradNorm=6.0544 StepSize=0.0605 RelImp=67.12%\n",
            "Epoch[1/2] Step[7333] Loss=0.2438 GradNorm=2.6725 StepSize=0.0267 RelImp=89.38%\n",
            "Epoch[1/2] Step[7334] Loss=0.2907 GradNorm=3.3084 StepSize=0.0331 RelImp=87.34%\n",
            "Epoch[1/2] Step[7335] Loss=0.3015 GradNorm=3.4036 StepSize=0.0340 RelImp=86.87%\n",
            "Epoch[1/2] Step[7336] Loss=0.1113 GradNorm=2.2504 StepSize=0.0225 RelImp=95.15%\n",
            "Epoch[1/2] Step[7337] Loss=0.1721 GradNorm=2.5156 StepSize=0.0252 RelImp=92.50%\n",
            "Epoch[1/2] Step[7338] Loss=0.0682 GradNorm=0.9963 StepSize=0.0100 RelImp=97.03%\n",
            "Epoch[1/2] Step[7339] Loss=0.2714 GradNorm=2.9427 StepSize=0.0294 RelImp=88.18%\n",
            "Epoch[1/2] Step[7340] Loss=0.0759 GradNorm=1.0586 StepSize=0.0106 RelImp=96.70%\n",
            "Epoch[1/2] Step[7341] Loss=0.3230 GradNorm=2.8756 StepSize=0.0288 RelImp=85.93%\n",
            "Epoch[1/2] Step[7342] Loss=0.2979 GradNorm=3.1246 StepSize=0.0312 RelImp=87.03%\n",
            "Epoch[1/2] Step[7343] Loss=0.3419 GradNorm=2.7227 StepSize=0.0272 RelImp=85.11%\n",
            "Epoch[1/2] Step[7344] Loss=0.4175 GradNorm=5.2059 StepSize=0.0521 RelImp=81.82%\n",
            "Epoch[1/2] Step[7345] Loss=0.6289 GradNorm=3.8737 StepSize=0.0387 RelImp=72.62%\n",
            "Epoch[1/2] Step[7346] Loss=0.2362 GradNorm=2.2975 StepSize=0.0230 RelImp=89.72%\n",
            "Epoch[1/2] Step[7347] Loss=0.3278 GradNorm=3.7143 StepSize=0.0371 RelImp=85.73%\n",
            "Epoch[1/2] Step[7348] Loss=0.1724 GradNorm=3.4801 StepSize=0.0348 RelImp=92.49%\n",
            "Epoch[1/2] Step[7349] Loss=0.4243 GradNorm=2.5136 StepSize=0.0251 RelImp=81.53%\n",
            "Epoch[1/2] Step[7350] Loss=0.2256 GradNorm=2.9750 StepSize=0.0297 RelImp=90.18%\n",
            "Epoch[1/2] Step[7351] Loss=0.3262 GradNorm=3.1121 StepSize=0.0311 RelImp=85.80%\n",
            "Epoch[1/2] Step[7352] Loss=0.2312 GradNorm=2.5606 StepSize=0.0256 RelImp=89.93%\n",
            "Epoch[1/2] Step[7353] Loss=0.2993 GradNorm=2.7843 StepSize=0.0278 RelImp=86.97%\n",
            "Epoch[1/2] Step[7354] Loss=0.7262 GradNorm=3.6156 StepSize=0.0362 RelImp=68.38%\n",
            "Epoch[1/2] Step[7355] Loss=0.7942 GradNorm=4.1738 StepSize=0.0417 RelImp=65.42%\n",
            "Epoch[1/2] Step[7356] Loss=0.2271 GradNorm=1.8487 StepSize=0.0185 RelImp=90.11%\n",
            "Epoch[1/2] Step[7357] Loss=0.3127 GradNorm=2.3620 StepSize=0.0236 RelImp=86.38%\n",
            "Epoch[1/2] Step[7358] Loss=0.2626 GradNorm=3.3181 StepSize=0.0332 RelImp=88.57%\n",
            "Epoch[1/2] Step[7359] Loss=0.5716 GradNorm=4.2719 StepSize=0.0427 RelImp=75.11%\n",
            "Epoch[1/2] Step[7360] Loss=0.1816 GradNorm=2.5066 StepSize=0.0251 RelImp=92.09%\n",
            "Epoch[1/2] Step[7361] Loss=0.1878 GradNorm=2.2069 StepSize=0.0221 RelImp=91.82%\n",
            "Epoch[1/2] Step[7362] Loss=0.0622 GradNorm=1.1020 StepSize=0.0110 RelImp=97.29%\n",
            "Epoch[1/2] Step[7363] Loss=0.0548 GradNorm=0.6953 StepSize=0.0070 RelImp=97.61%\n",
            "Epoch[1/2] Step[7364] Loss=0.2344 GradNorm=3.0681 StepSize=0.0307 RelImp=89.79%\n",
            "Epoch[1/2] Step[7365] Loss=0.1193 GradNorm=2.1978 StepSize=0.0220 RelImp=94.81%\n",
            "Epoch[1/2] Step[7366] Loss=0.3533 GradNorm=4.5191 StepSize=0.0452 RelImp=84.62%\n",
            "Epoch[1/2] Step[7367] Loss=0.1886 GradNorm=2.3480 StepSize=0.0235 RelImp=91.79%\n",
            "Epoch[1/2] Step[7368] Loss=0.4152 GradNorm=3.5828 StepSize=0.0358 RelImp=81.92%\n",
            "Epoch[1/2] Step[7369] Loss=0.1528 GradNorm=2.4085 StepSize=0.0241 RelImp=93.35%\n",
            "Epoch[1/2] Step[7370] Loss=0.1793 GradNorm=2.6500 StepSize=0.0265 RelImp=92.19%\n",
            "Epoch[1/2] Step[7371] Loss=0.2939 GradNorm=2.8510 StepSize=0.0285 RelImp=87.20%\n",
            "Epoch[1/2] Step[7372] Loss=0.1002 GradNorm=1.2628 StepSize=0.0126 RelImp=95.64%\n",
            "Epoch[1/2] Step[7373] Loss=0.0939 GradNorm=1.5940 StepSize=0.0159 RelImp=95.91%\n",
            "Epoch[1/2] Step[7374] Loss=0.3804 GradNorm=2.4989 StepSize=0.0250 RelImp=83.44%\n",
            "Epoch[1/2] Step[7375] Loss=0.5852 GradNorm=3.8535 StepSize=0.0385 RelImp=74.52%\n",
            "Epoch[1/2] Step[7376] Loss=0.1656 GradNorm=2.2416 StepSize=0.0224 RelImp=92.79%\n",
            "Epoch[1/2] Step[7377] Loss=0.1694 GradNorm=2.1250 StepSize=0.0212 RelImp=92.62%\n",
            "Epoch[1/2] Step[7378] Loss=0.1564 GradNorm=1.8944 StepSize=0.0189 RelImp=93.19%\n",
            "Epoch[1/2] Step[7379] Loss=0.1414 GradNorm=1.7342 StepSize=0.0173 RelImp=93.84%\n",
            "Epoch[1/2] Step[7380] Loss=0.2676 GradNorm=3.9207 StepSize=0.0392 RelImp=88.35%\n",
            "Epoch[1/2] Step[7381] Loss=0.1912 GradNorm=2.7145 StepSize=0.0271 RelImp=91.68%\n",
            "Epoch[1/2] Step[7382] Loss=0.2981 GradNorm=2.3967 StepSize=0.0240 RelImp=87.02%\n",
            "Epoch[1/2] Step[7383] Loss=0.1580 GradNorm=2.5076 StepSize=0.0251 RelImp=93.12%\n",
            "Epoch[1/2] Step[7384] Loss=0.2595 GradNorm=3.6344 StepSize=0.0363 RelImp=88.70%\n",
            "Epoch[1/2] Step[7385] Loss=0.1296 GradNorm=2.1833 StepSize=0.0218 RelImp=94.36%\n",
            "Epoch[1/2] Step[7386] Loss=0.0553 GradNorm=0.7074 StepSize=0.0071 RelImp=97.59%\n",
            "Epoch[1/2] Step[7387] Loss=0.1297 GradNorm=2.3525 StepSize=0.0235 RelImp=94.35%\n",
            "Epoch[1/2] Step[7388] Loss=0.3292 GradNorm=3.5446 StepSize=0.0354 RelImp=85.67%\n",
            "Epoch[1/2] Step[7389] Loss=0.2301 GradNorm=2.5722 StepSize=0.0257 RelImp=89.98%\n",
            "Epoch[1/2] Step[7390] Loss=0.3141 GradNorm=4.3654 StepSize=0.0437 RelImp=86.32%\n",
            "Epoch[1/2] Step[7391] Loss=0.2814 GradNorm=3.0982 StepSize=0.0310 RelImp=87.75%\n",
            "Epoch[1/2] Step[7392] Loss=0.1385 GradNorm=1.4503 StepSize=0.0145 RelImp=93.97%\n",
            "Epoch[1/2] Step[7393] Loss=0.1954 GradNorm=3.6292 StepSize=0.0363 RelImp=91.49%\n",
            "Epoch[1/2] Step[7394] Loss=0.2566 GradNorm=3.3383 StepSize=0.0334 RelImp=88.83%\n",
            "Epoch[1/2] Step[7395] Loss=0.5119 GradNorm=3.7241 StepSize=0.0372 RelImp=77.71%\n",
            "Epoch[1/2] Step[7396] Loss=0.3423 GradNorm=3.5827 StepSize=0.0358 RelImp=85.09%\n",
            "Epoch[1/2] Step[7397] Loss=0.3959 GradNorm=3.1161 StepSize=0.0312 RelImp=82.76%\n",
            "Epoch[1/2] Step[7398] Loss=0.2647 GradNorm=3.4350 StepSize=0.0344 RelImp=88.48%\n",
            "Epoch[1/2] Step[7399] Loss=0.3347 GradNorm=3.4368 StepSize=0.0344 RelImp=85.42%\n",
            "Epoch[1/2] Step[7400] Loss=0.0670 GradNorm=0.9956 StepSize=0.0100 RelImp=97.08%\n",
            "Epoch[1/2] Step[7401] Loss=0.0934 GradNorm=1.2715 StepSize=0.0127 RelImp=95.93%\n",
            "Epoch[1/2] Step[7402] Loss=0.3206 GradNorm=3.7555 StepSize=0.0376 RelImp=86.04%\n",
            "Epoch[1/2] Step[7403] Loss=0.1062 GradNorm=1.7660 StepSize=0.0177 RelImp=95.38%\n",
            "Epoch[1/2] Step[7404] Loss=0.4651 GradNorm=3.6165 StepSize=0.0362 RelImp=79.75%\n",
            "Epoch[1/2] Step[7405] Loss=0.0651 GradNorm=0.9316 StepSize=0.0093 RelImp=97.16%\n",
            "Epoch[1/2] Step[7406] Loss=0.1032 GradNorm=1.6703 StepSize=0.0167 RelImp=95.50%\n",
            "Epoch[1/2] Step[7407] Loss=0.2528 GradNorm=3.0477 StepSize=0.0305 RelImp=88.99%\n",
            "Epoch[1/2] Step[7408] Loss=0.1271 GradNorm=1.4856 StepSize=0.0149 RelImp=94.47%\n",
            "Epoch[1/2] Step[7409] Loss=0.3332 GradNorm=4.0891 StepSize=0.0409 RelImp=85.49%\n",
            "Epoch[1/2] Step[7410] Loss=0.2328 GradNorm=2.7093 StepSize=0.0271 RelImp=89.86%\n",
            "Epoch[1/2] Step[7411] Loss=0.6095 GradNorm=4.3129 StepSize=0.0431 RelImp=73.46%\n",
            "Epoch[1/2] Step[7412] Loss=0.1687 GradNorm=2.6145 StepSize=0.0261 RelImp=92.65%\n",
            "Epoch[1/2] Step[7413] Loss=0.0695 GradNorm=0.9583 StepSize=0.0096 RelImp=96.97%\n",
            "Epoch[1/2] Step[7414] Loss=0.5926 GradNorm=4.5587 StepSize=0.0456 RelImp=74.20%\n",
            "Epoch[1/2] Step[7415] Loss=0.1681 GradNorm=2.7145 StepSize=0.0271 RelImp=92.68%\n",
            "Epoch[1/2] Step[7416] Loss=0.7934 GradNorm=3.7771 StepSize=0.0378 RelImp=65.45%\n",
            "Epoch[1/2] Step[7417] Loss=0.1727 GradNorm=3.2592 StepSize=0.0326 RelImp=92.48%\n",
            "Epoch[1/2] Step[7418] Loss=0.2064 GradNorm=2.3938 StepSize=0.0239 RelImp=91.01%\n",
            "Epoch[1/2] Step[7419] Loss=0.0850 GradNorm=1.3082 StepSize=0.0131 RelImp=96.30%\n",
            "Epoch[1/2] Step[7420] Loss=0.3063 GradNorm=3.8401 StepSize=0.0384 RelImp=86.66%\n",
            "Epoch[1/2] Step[7421] Loss=0.2695 GradNorm=2.7537 StepSize=0.0275 RelImp=88.27%\n",
            "Epoch[1/2] Step[7422] Loss=0.2156 GradNorm=1.9283 StepSize=0.0193 RelImp=90.61%\n",
            "Epoch[1/2] Step[7423] Loss=0.2501 GradNorm=2.8030 StepSize=0.0280 RelImp=89.11%\n",
            "Epoch[1/2] Step[7424] Loss=0.3897 GradNorm=3.7607 StepSize=0.0376 RelImp=83.03%\n",
            "Epoch[1/2] Step[7425] Loss=0.2568 GradNorm=2.4996 StepSize=0.0250 RelImp=88.82%\n",
            "Epoch[1/2] Step[7426] Loss=0.0537 GradNorm=0.9120 StepSize=0.0091 RelImp=97.66%\n",
            "Epoch[1/2] Step[7427] Loss=0.2698 GradNorm=2.9120 StepSize=0.0291 RelImp=88.25%\n",
            "Epoch[1/2] Step[7428] Loss=0.2638 GradNorm=3.1162 StepSize=0.0312 RelImp=88.51%\n",
            "Epoch[1/2] Step[7429] Loss=0.2825 GradNorm=2.7449 StepSize=0.0274 RelImp=87.70%\n",
            "Epoch[1/2] Step[7430] Loss=0.1786 GradNorm=2.3837 StepSize=0.0238 RelImp=92.22%\n",
            "Epoch[1/2] Step[7431] Loss=0.1851 GradNorm=2.6171 StepSize=0.0262 RelImp=91.94%\n",
            "Epoch[1/2] Step[7432] Loss=0.0949 GradNorm=1.2367 StepSize=0.0124 RelImp=95.87%\n",
            "Epoch[1/2] Step[7433] Loss=0.5965 GradNorm=4.3126 StepSize=0.0431 RelImp=74.02%\n",
            "Epoch[1/2] Step[7434] Loss=0.5535 GradNorm=6.4084 StepSize=0.0641 RelImp=75.90%\n",
            "Epoch[1/2] Step[7435] Loss=0.2320 GradNorm=3.0306 StepSize=0.0303 RelImp=89.90%\n",
            "Epoch[1/2] Step[7436] Loss=0.1022 GradNorm=1.8239 StepSize=0.0182 RelImp=95.55%\n",
            "Epoch[1/2] Step[7437] Loss=0.1681 GradNorm=2.4681 StepSize=0.0247 RelImp=92.68%\n",
            "Epoch[1/2] Step[7438] Loss=0.6399 GradNorm=2.6377 StepSize=0.0264 RelImp=72.14%\n",
            "Epoch[1/2] Step[7439] Loss=0.0624 GradNorm=1.0124 StepSize=0.0101 RelImp=97.28%\n",
            "Epoch[1/2] Step[7440] Loss=0.4582 GradNorm=4.4287 StepSize=0.0443 RelImp=80.05%\n",
            "Epoch[1/2] Step[7441] Loss=0.2530 GradNorm=2.5288 StepSize=0.0253 RelImp=88.99%\n",
            "Epoch[1/2] Step[7442] Loss=0.2331 GradNorm=3.5584 StepSize=0.0356 RelImp=89.85%\n",
            "Epoch[1/2] Step[7443] Loss=0.2226 GradNorm=2.5166 StepSize=0.0252 RelImp=90.31%\n",
            "Epoch[1/2] Step[7444] Loss=0.0668 GradNorm=1.1092 StepSize=0.0111 RelImp=97.09%\n",
            "Epoch[1/2] Step[7445] Loss=0.0330 GradNorm=0.4645 StepSize=0.0046 RelImp=98.56%\n",
            "Epoch[1/2] Step[7446] Loss=0.4987 GradNorm=2.2862 StepSize=0.0229 RelImp=78.28%\n",
            "Epoch[1/2] Step[7447] Loss=0.1547 GradNorm=1.8596 StepSize=0.0186 RelImp=93.27%\n",
            "Epoch[1/2] Step[7448] Loss=0.3846 GradNorm=3.1021 StepSize=0.0310 RelImp=83.25%\n",
            "Epoch[1/2] Step[7449] Loss=0.2731 GradNorm=3.2900 StepSize=0.0329 RelImp=88.11%\n",
            "Epoch[1/2] Step[7450] Loss=0.3118 GradNorm=3.0627 StepSize=0.0306 RelImp=86.42%\n",
            "Epoch[1/2] Step[7451] Loss=0.5078 GradNorm=3.7987 StepSize=0.0380 RelImp=77.89%\n",
            "Epoch[1/2] Step[7452] Loss=0.1313 GradNorm=2.2132 StepSize=0.0221 RelImp=94.28%\n",
            "Epoch[1/2] Step[7453] Loss=0.8307 GradNorm=3.1940 StepSize=0.0319 RelImp=63.83%\n",
            "Epoch[1/2] Step[7454] Loss=0.3663 GradNorm=2.8297 StepSize=0.0283 RelImp=84.05%\n",
            "Epoch[1/2] Step[7455] Loss=0.4531 GradNorm=3.8918 StepSize=0.0389 RelImp=80.27%\n",
            "Epoch[1/2] Step[7456] Loss=0.0528 GradNorm=0.9336 StepSize=0.0093 RelImp=97.70%\n",
            "Epoch[1/2] Step[7457] Loss=1.0583 GradNorm=4.9861 StepSize=0.0499 RelImp=53.92%\n",
            "Epoch[1/2] Step[7458] Loss=0.1145 GradNorm=1.5992 StepSize=0.0160 RelImp=95.01%\n",
            "Epoch[1/2] Step[7459] Loss=0.1620 GradNorm=3.0815 StepSize=0.0308 RelImp=92.95%\n",
            "Epoch[1/2] Step[7460] Loss=0.1850 GradNorm=1.9808 StepSize=0.0198 RelImp=91.95%\n",
            "Epoch[1/2] Step[7461] Loss=0.1374 GradNorm=2.2592 StepSize=0.0226 RelImp=94.02%\n",
            "Epoch[1/2] Step[7462] Loss=0.1340 GradNorm=2.1169 StepSize=0.0212 RelImp=94.17%\n",
            "Epoch[1/2] Step[7463] Loss=0.4508 GradNorm=4.1453 StepSize=0.0415 RelImp=80.37%\n",
            "Epoch[1/2] Step[7464] Loss=0.0859 GradNorm=1.5573 StepSize=0.0156 RelImp=96.26%\n",
            "Epoch[1/2] Step[7465] Loss=0.0569 GradNorm=0.7007 StepSize=0.0070 RelImp=97.52%\n",
            "Epoch[1/2] Step[7466] Loss=0.2162 GradNorm=3.1170 StepSize=0.0312 RelImp=90.59%\n",
            "Epoch[1/2] Step[7467] Loss=0.2627 GradNorm=2.5694 StepSize=0.0257 RelImp=88.56%\n",
            "Epoch[1/2] Step[7468] Loss=0.2226 GradNorm=3.0814 StepSize=0.0308 RelImp=90.31%\n",
            "Epoch[1/2] Step[7469] Loss=0.1472 GradNorm=2.0605 StepSize=0.0206 RelImp=93.59%\n",
            "Epoch[1/2] Step[7470] Loss=0.4609 GradNorm=4.3870 StepSize=0.0439 RelImp=79.93%\n",
            "Epoch[1/2] Step[7471] Loss=0.3229 GradNorm=2.9467 StepSize=0.0295 RelImp=85.94%\n",
            "Epoch[1/2] Step[7472] Loss=0.2167 GradNorm=2.0543 StepSize=0.0205 RelImp=90.57%\n",
            "Epoch[1/2] Step[7473] Loss=0.1502 GradNorm=1.6929 StepSize=0.0169 RelImp=93.46%\n",
            "Epoch[1/2] Step[7474] Loss=0.5489 GradNorm=5.3505 StepSize=0.0535 RelImp=76.10%\n",
            "Epoch[1/2] Step[7475] Loss=0.4142 GradNorm=5.0603 StepSize=0.0506 RelImp=81.96%\n",
            "Epoch[1/2] Step[7476] Loss=0.3539 GradNorm=3.6598 StepSize=0.0366 RelImp=84.59%\n",
            "Epoch[1/2] Step[7477] Loss=0.2363 GradNorm=2.6888 StepSize=0.0269 RelImp=89.71%\n",
            "Epoch[1/2] Step[7478] Loss=0.4060 GradNorm=3.9253 StepSize=0.0393 RelImp=82.32%\n",
            "Epoch[1/2] Step[7479] Loss=0.3299 GradNorm=3.5103 StepSize=0.0351 RelImp=85.63%\n",
            "Epoch[1/2] Step[7480] Loss=0.6796 GradNorm=5.4425 StepSize=0.0544 RelImp=70.41%\n",
            "Epoch[1/2] Step[7481] Loss=0.1364 GradNorm=3.0334 StepSize=0.0303 RelImp=94.06%\n",
            "Epoch[1/2] Step[7482] Loss=0.2718 GradNorm=3.3906 StepSize=0.0339 RelImp=88.16%\n",
            "Epoch[1/2] Step[7483] Loss=0.4935 GradNorm=3.1588 StepSize=0.0316 RelImp=78.51%\n",
            "Epoch[1/2] Step[7484] Loss=0.2451 GradNorm=3.1016 StepSize=0.0310 RelImp=89.33%\n",
            "Epoch[1/2] Step[7485] Loss=0.3900 GradNorm=3.2294 StepSize=0.0323 RelImp=83.02%\n",
            "Epoch[1/2] Step[7486] Loss=0.1041 GradNorm=2.0199 StepSize=0.0202 RelImp=95.47%\n",
            "Epoch[1/2] Step[7487] Loss=0.3774 GradNorm=4.2799 StepSize=0.0428 RelImp=83.57%\n",
            "Epoch[1/2] Step[7488] Loss=0.4523 GradNorm=4.3988 StepSize=0.0440 RelImp=80.31%\n",
            "Epoch[1/2] Step[7489] Loss=0.4053 GradNorm=2.1278 StepSize=0.0213 RelImp=82.35%\n",
            "Epoch[1/2] Step[7490] Loss=0.6032 GradNorm=4.6134 StepSize=0.0461 RelImp=73.74%\n",
            "Epoch[1/2] Step[7491] Loss=0.2139 GradNorm=2.8739 StepSize=0.0287 RelImp=90.69%\n",
            "Epoch[1/2] Step[7492] Loss=0.5582 GradNorm=4.6929 StepSize=0.0469 RelImp=75.70%\n",
            "Epoch[1/2] Step[7493] Loss=0.0691 GradNorm=1.4351 StepSize=0.0144 RelImp=96.99%\n",
            "Epoch[1/2] Step[7494] Loss=0.3873 GradNorm=4.1970 StepSize=0.0420 RelImp=83.14%\n",
            "Epoch[1/2] Step[7495] Loss=0.4565 GradNorm=3.9488 StepSize=0.0395 RelImp=80.12%\n",
            "Epoch[1/2] Step[7496] Loss=0.9233 GradNorm=3.3648 StepSize=0.0336 RelImp=59.80%\n",
            "Epoch[1/2] Step[7497] Loss=0.3372 GradNorm=3.1120 StepSize=0.0311 RelImp=85.32%\n",
            "Epoch[1/2] Step[7498] Loss=0.1812 GradNorm=1.6896 StepSize=0.0169 RelImp=92.11%\n",
            "Epoch[1/2] Step[7499] Loss=0.8683 GradNorm=6.4613 StepSize=0.0646 RelImp=62.19%\n",
            "Epoch[1/2] Step[7500] Loss=0.1682 GradNorm=1.9444 StepSize=0.0194 RelImp=92.68%\n",
            "Epoch 2/2 => TrainLoss=0.3386 TestLoss=0.2952 Acc=91.64%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>grad_norm</td><td>▁▁▂▂▂▃▃▃▄▅█▄▆▅▄▆▃▇▆▄▃▄▄▆▂▅▃▅▆▅█▄█▃▂▂█▅▅▄</td></tr><tr><td>relative_improvement</td><td>▁▁▁▂▇█▆▅▇▇▇█▇▆█▆▆▇▇▇█▇▆▇█▆▇▆█▇█████▆█▆▇█</td></tr><tr><td>step_size</td><td>▂▄▆▆▄▆▄▄▅▅▃▅▅▆▄▂▅▄▇▁▃▆▄▅▃▄▄▃▇█▆▄▆▄▄▃▃█▄▃</td></tr><tr><td>test_accuracy</td><td>▁█</td></tr><tr><td>test_loss</td><td>█▁</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▃▃▃▃▃▂▂▁▄▂▂▂▂▄▃▂▂▃▂▂▂▃▂▂▂▁▃▂▃▂▂▁▁▁▂</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>grad_norm</td><td>1.9444</td></tr><tr><td>relative_improvement</td><td>0.92678</td></tr><tr><td>step_size</td><td>0.01944</td></tr><tr><td>test_accuracy</td><td>91.64</td></tr><tr><td>test_loss</td><td>0.29518</td></tr><tr><td>train_loss</td><td>0.16815</td></tr><tr><td>train_loss_epoch</td><td>0.33864</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earnest-bush-7</strong> at: <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__gradient-descent__/runs/gsalue4q' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/AAH-IA__gradient-descent__/runs/gsalue4q</a><br> View project at: <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__gradient-descent__' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/AAH-IA__gradient-descent__</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250115_142308-gsalue4q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done with main(). Returning model.\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "# 6) Main\n",
        "##################################################\n",
        "def main(\n",
        "    optimizer_name=\"gd\",\n",
        "    lr=1.0,\n",
        "    epochs=2,\n",
        "    batch_size=16,\n",
        "    wandb_project=\"AAH-IA__newton-rhapson__\",\n",
        "):\n",
        "    hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
        "    wandb_key = os.environ.get(\"wandb\", None)\n",
        "\n",
        "    print(\"HF token:\", bool(hf_token))\n",
        "    print(\"W&B key:\", bool(wandb_key))\n",
        "    print(f\"Using {optimizer_name} with ~9.6k param model. LR={lr}, epochs={epochs}, batch_size={batch_size}\")\n",
        "\n",
        "    model = train_model(\n",
        "        optimizer_name=optimizer_name,\n",
        "        learning_rate=lr,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        wandb_project=wandb_project\n",
        "    )\n",
        "    print(\"Done with main(). Returning model.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# Example if we want to run immediately:\n",
        "if __name__ == \"__main__\":\n",
        "    model = main(\n",
        "        optimizer_name=\"gd\",\n",
        "        lr=0.01, # 0.01 for gradient descent\n",
        "        epochs=2,\n",
        "        batch_size=16,\n",
        "        wandb_project=\"AAH-IA__gradient-descent__\"  # or AAH-IA__gradient-descent__\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "SHlkTdAyE9eJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "5033acb9-c20b-40c5-d2ff-7c46855a5a7f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▇██▇▆▇▅▅▅▄▄▄▄▃▂▂▂▂▂▂▂▂▂▁▁▂▃▂▂▂▂▂▁▁▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>4.85807</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">soft-thunder-14</strong> at: <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__/runs/9g5cwr7a' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__/runs/9g5cwr7a</a><br> View project at: <a href='https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__' target=\"_blank\">https://wandb.ai/marlborough-college-malaysia/AAH-IA__newton-rhapson__</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250115_140528-9g5cwr7a/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnhQT3onFx96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}